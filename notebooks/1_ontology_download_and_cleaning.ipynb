{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<img width='700' src=\"https://user-images.githubusercontent.com/8030363/108961534-b9a66980-7634-11eb-96e2-cc46589dcb8c.png\" style=\"vertical-align:middle\">\n",
    "\n",
    "## Pre-Knowledge Graph Build Ontology Download and Cleaning\n",
    "***\n",
    "***\n",
    "\n",
    "**Authors:** [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com), [ECavalleri](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=emanuele.cavalleri@unimi.it)    \n",
    "**GitHub Repositories:** [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki), [RNA-KG](https://github.com/AnacletoLAB/RNA-KG/)\n",
    "  \n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook serves as a script to help prepare ontologies prior to be ingested into the knowledge graph build algorithm. This script performs the following steps:  \n",
    "\n",
    "1. [Download Ontologies](#download-ontologies)  \n",
    "2. [Clean Ontologies](#clean-ontologies)  \n",
    "3. [Merge Ontologies](#merge-ontologies)\n",
    "4. [Create Merged Ontologies Graph](#graph-ontologies)\n",
    "\n",
    "**Dependencies:**   \n",
    "- <u>Scripts</u>: This notebook utilizes several helper functions from the following scripts:  \n",
    "  - [utility scripts](https://github.com/callahantiff/PheKnowLator/blob/master/pkt_kg/utils)  \n",
    "  - [ontology_cleaning.py](https://github.com/callahantiff/PheKnowLator/blob/master/builds/ontology_cleaning.py) \n",
    "- <u>Software</u>: [OWLTools](https://github.com/owlcollab/owltools)  \n",
    "- <u>Data</u>: All downloaded and generated data sources are provided through [10.5281/zenodo.10078876](https://zenodo.org/doi/10.5281/zenodo.10078876) dedicated repository. <u>This notebook will download everything that is needed for you</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-Up Environment\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import datetime\n",
    "import glob\n",
    "import itertools\n",
    "import networkx\n",
    "import numpy\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import gffpandas.gffpandas as gffpd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import re\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from reactome2py import content\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "from pkt_kg.utils import * \n",
    "from builds.ontology_cleaning import *\n",
    "\n",
    "from Bio import SeqIO, Entrez\n",
    "\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store resources\n",
    "resource_data_location = '../resources/'    \n",
    "\n",
    "# directory to use for unprocessed data\n",
    "unprocessed_data_location = '../resources/processed_data/unprocessed_data/'\n",
    "\n",
    "# directory to use for processed data\n",
    "processed_data_location = '../resources/processed_data/'\n",
    "\n",
    "# directory to write relations data to\n",
    "relations_data_location = '../resources/relations_data/'\n",
    "\n",
    "# directory to write ontology data to\n",
    "ontology_data_location = '../resources/ontologies/'\n",
    "\n",
    "# directory to write edges data to\n",
    "edge_data_location = '../resources/edge_data/'\n",
    "\n",
    "# processed data url \n",
    "processed_url = 'https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/'\n",
    "\n",
    "# owltools location\n",
    "owltools_location = '../pkt_kg/libs/owltools'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "### DOWNLOAD ONTOLOGIES <a class=\"anchor\" id=\"download-ontologies\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_list = ['chebi', 'pr', 'ro', 'mondo', 'go/extensions/go-plus', 'pw', 'so', 'hp/hp-international', 'uberon', 'vo', 'clo']\n",
    "# If pr and chebi servers are down (occurs frequently), download the ontologies from the following URLs:\n",
    "# pr --> http://web.archive.org/web/20240926155115/https://proconsortium.org/download/current/pro_reasoned.owl\n",
    "# chebi --> ftp://ftp.ebi.ac.uk/pub/databases/chebi/ontology/chebi.owl\n",
    "command = '{} {} --merge-import-closure -o {}'\n",
    "for ontology in onto_list:\n",
    "    print('Processing ' + ontology + ' ontology')\n",
    "    os.system(command.format(owltools_location, 'http://purl.obolibrary.org/obo/' + ontology + '.owl',\n",
    "                             ontology_data_location + ontology + '_with_imports.owl'))\n",
    "\n",
    "# For compatibility with the PheKnowLator ecosystem, we rename the ontology files to match the naming convention\n",
    "os.rename(os.path.join(ontology_data_location,\n",
    "                       'go/extensions/go-plus_with_imports.owl'), os.path.join(ontology_data_location, 'go_with_imports.owl'))\n",
    "os.rename(os.path.join(ontology_data_location,\n",
    "                       'hp/hp-international_with_imports.owl'), os.path.join(ontology_data_location, 'hp_with_imports.owl'))\n",
    "os.rename(os.path.join(ontology_data_location, 'uberon_with_imports.owl'), os.path.join(ontology_data_location, 'ext_with_imports.owl'))\n",
    "\n",
    "# Remove empty directories\n",
    "os.rmdir(os.path.join(ontology_data_location, 'hp'))\n",
    "os.rmdir(os.path.join(ontology_data_location, 'go/extensions'))\n",
    "os.rmdir(os.path.join(ontology_data_location, 'go'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a version of the PRotein Ontology that contains only human and viral proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx_mdg: networkx.MultiDiGraph = networkx.MultiDiGraph()\n",
    "    \n",
    "pr_graph = Graph().parse(ontology_data_location + 'pr_with_imports.owl')\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(pr_graph), datetime.datetime.now().strftime('%m/%d/%Y')))\n",
    "\n",
    "for s, p, o in tqdm(pr_graph):\n",
    "    networkx_mdg.add_edge(s, o, **{'key': p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obo = Namespace('http://purl.obolibrary.org/obo/')\n",
    "\n",
    "human_classes_restriction = list(pr_graph.triples((None, OWL.someValuesFrom, obo.NCBITaxon_9606)))\n",
    "human_classes = [list(pr_graph.subjects(RDFS.subClassOf, x[0])) for x in human_classes_restriction]\n",
    "human_pro_classes = list(str(i) for j in human_classes for i in j if 'PR_' in str(i))\n",
    "\n",
    "print('There are {} edges in the ontology (date:{})'.format(len(human_pro_classes), datetime.datetime.now().strftime('%m/%d/%Y')))\n",
    "human_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the list of virus taxa to extract terms connected to them (as with Homo Sapiens -- NCBITaxon_9606)\n",
    "viruses_NCBI = list(pr_graph.subjects(RDFS.subClassOf, obo.NCBITaxon_10239))\n",
    "viral_classes_restriction = []\n",
    "for i in viruses_NCBI :\n",
    "    viral_classes_restriction.append(list(pr_graph.triples((None, OWL.someValuesFrom, i))))\n",
    "\n",
    "temp = []\n",
    "for i in viral_classes_restriction:\n",
    "    for j in i:\n",
    "        temp.append(j)\n",
    "viral_classes_restriction = temp\n",
    "viral_classes = [list(pr_graph.subjects(RDFS.subClassOf, x[0])) for x in viral_classes_restriction]\n",
    "# remove empty classes\n",
    "viral_classes = [x for x in viral_classes if x]\n",
    "viral_pro_classes = list(str(i) for j in viral_classes for i in j if 'PR_' in str(i))\n",
    "print('There are {} edges in the ontology (date:{})'.format(len(viral_pro_classes), datetime.datetime.now().strftime('%m/%d/%Y')))\n",
    "viral_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_pro_classes = human_pro_classes + viral_pro_classes\n",
    "\n",
    "# create a new graph using bfs paths\n",
    "human_pro_graph = Graph()\n",
    "human_networkx_mdg = networkx.MultiDiGraph()\n",
    "\n",
    "for node in tqdm(human_pro_classes):\n",
    "    forward = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='original'))\n",
    "    reverse = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='reverse'))\n",
    "    \n",
    "    # add edges from forward and reverse bfs paths\n",
    "    for path in set(forward + reverse):\n",
    "        human_pro_graph.add((path[0], path[2], path[1]))\n",
    "        human_networkx_mdg.add_edge(path[0], path[1], **{'key': path[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get connected component information\n",
    "print('Finding Connected Components')\n",
    "components = list(networkx.connected_components(human_networkx_mdg.to_undirected()))\n",
    "component_dict = sorted(components, key=len, reverse=True)\n",
    "\n",
    "# if more than 1 connected component, only keep the biggest\n",
    "if len(component_dict) > 1:\n",
    "    print('Cleaning Graph: Removing Small Disconnected Components')\n",
    "    for node in tqdm([x for y in component_dict[1:] for x in list(y)]):\n",
    "        human_pro_graph.remove((node, None, None))\n",
    "\n",
    "# save data\n",
    "print('Saving Human Subset of the Protein Ontology')\n",
    "human_pro_graph.serialize(destination=unprocessed_data_location + 'human_pro.owl', format='xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run reasoner to ensure that there are no incomplete triples or inconsistent classes\n",
    "command = '{} {} --reasoner {} --run-reasoner --assert-implied -o {}'\n",
    "os.system(command.format(owltools_location, unprocessed_data_location + 'human_pro.owl', 'elk',\n",
    "                         ontology_data_location + 'pr_with_imports.owl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genomic Typing Dictionary**  \n",
    "Read in the  `genomic_typing_dict.pkl` dictionary, which is needed in order to preprocess the genomic identifier datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://zenodo.org/records/10056198/files/genomic_typing_dict.pkl.zip?download=1'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "genomic_type_mapper = pickle.load(open(unprocessed_data_location + 'genomic_typing_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**HGNC Data** \n",
    "\n",
    "_Human Gene Set Data_ - `hgnc_complete_set.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://storage.googleapis.com/public-download-files/hgnc/tsv/tsv/hgnc_complete_set.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "hgnc = pandas.read_csv(unprocessed_data_location + 'hgnc_complete_set.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "hgnc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, updating data types (i.e. making all columns type `str`), and unnesting `|` delimited data. The final step is to update the gene_type variable such that each of the variable values is re-grouped to be protein-coding, other or ncRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc = hgnc.loc[hgnc['status'].apply(lambda x: x == 'Approved')]\n",
    "hgnc = hgnc[['hgnc_id', 'entrez_id', 'ensembl_gene_id', 'uniprot_ids', 'symbol', 'locus_type', 'alias_symbol', 'name', 'location', 'alias_name']]\n",
    "hgnc.rename(columns={'uniprot_ids': 'uniprot_id', 'location': 'map_location', 'locus_type': 'hgnc_gene_type'}, inplace=True)\n",
    "hgnc['hgnc_id'].replace('.*\\:', '', inplace=True, regex=True)  # strip 'HGNC' off of the identifiers\n",
    "hgnc.fillna('None', inplace=True)  # replace NaN with 'None'\n",
    "hgnc['entrez_id'] = hgnc['entrez_id'].apply(lambda x: str(int(x)) if x != 'None' else 'None')  # make col str\n",
    "\n",
    "# combine certain columns into single column\n",
    "hgnc['name'] = hgnc['name'] + '|' + hgnc['alias_name']\n",
    "hgnc['synonyms'] = hgnc['alias_symbol'] + '|' + hgnc['alias_name'] + '|' + hgnc['name']\n",
    "hgnc['symbol'] = hgnc['symbol'] + '|' + hgnc['alias_symbol']\n",
    "\n",
    "# explode nested data and reformat values in preparation for combining it with other gene identifiers\n",
    "explode_df_hgnc = explodes_data(hgnc.copy(), ['ensembl_gene_id', 'uniprot_id', 'symbol', 'name', 'synonyms'], '|')\n",
    "\n",
    "# reformat hgnc gene type\n",
    "for val in genomic_type_mapper['hgnc_gene_type'].keys():\n",
    "    explode_df_hgnc['hgnc_gene_type'].replace(val, genomic_type_mapper['hgnc_gene_type'][val], inplace=True)\n",
    "\n",
    "# reformat master hgnc gene type\n",
    "explode_df_hgnc['master_gene_type'] = explode_df_hgnc['hgnc_gene_type']\n",
    "master_dict = genomic_type_mapper['hgnc_master_gene_type']\n",
    "for val in master_dict.keys():\n",
    "    explode_df_hgnc['master_gene_type'].replace(val, master_dict[val], inplace=True)\n",
    "\n",
    "# post-process reformatted data\n",
    "explode_df_hgnc.drop(['alias_symbol', 'alias_name'], axis=1, inplace=True)  # remove original gene type column\n",
    "explode_df_hgnc.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_hgnc.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Ensembl Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Human Gene Set Data_ - `Homo_sapiens.GRCh38.113.gtf.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://ftp.ensembl.org/pub/release-113/gtf/homo_sapiens/Homo_sapiens.GRCh38.113.gtf.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ensembl_geneset = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.113.gtf',\n",
    "                                  header = None, delimiter='\\t', skiprows=5, usecols=[8], low_memory=False)\n",
    "ensembl_geneset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be reformatted in order for it to be able to be merged with the other gene, RNA, and protein identifier data. To do this, we iterate over each row of the data and extract the fields shown below in `column_names`, making each of these extracted fields their own column. The final step is to update the gene_type variable such that each of the variable values is re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_data = list(ensembl_geneset[8]); ensembl_df_data = []\n",
    "for i in tqdm(range(0, len(ensembl_data))):\n",
    "    if 'gene_id' in ensembl_data[i] and 'transcript_id' in ensembl_data[i]:\n",
    "        row_dict = {x.split(' \"')[0].lstrip(): x.split(' \"')[1].strip('\"') for x in ensembl_data[i].split(';')[0:-1]}\n",
    "        if 'gene_name' not in row_dict:\n",
    "            row_dict['gene_name'] = 'None'\n",
    "        if 'gene_biotype' not in row_dict:\n",
    "            row_dict['gene_biotype'] = 'None'\n",
    "        if 'transcript_name' not in row_dict:\n",
    "            row_dict['transcript_name'] = 'None'\n",
    "        if 'transcript_biotype' not in row_dict:\n",
    "            row_dict['transcript_biotype'] = 'None'\n",
    "        ensembl_df_data += [(row_dict['gene_id'], row_dict['transcript_id'], row_dict['gene_name'],\n",
    "                           row_dict['gene_biotype'], row_dict['transcript_name'], row_dict['transcript_biotype'])]\n",
    "# convert to data frame\n",
    "ensembl_geneset = pandas.DataFrame(ensembl_df_data,\n",
    "                                   columns=['ensembl_gene_id', 'transcript_stable_id', 'symbol',\n",
    "                                            'ensembl_gene_type', 'transcript_name', 'ensembl_transcript_type'])\n",
    "\n",
    "# reformat ensembl gene type\n",
    "gene_dict = genomic_type_mapper['ensembl_gene_type']\n",
    "for val in gene_dict.keys(): ensembl_geneset['ensembl_gene_type'].replace(val, gene_dict[val], inplace=True)\n",
    "# reformat master gene type\n",
    "ensembl_geneset['master_gene_type'] = ensembl_geneset['ensembl_gene_type']\n",
    "gene_dict = genomic_type_mapper['ensembl_master_gene_type']\n",
    "for val in gene_dict.keys(): ensembl_geneset['master_gene_type'].replace(val, gene_dict[val], inplace=True)\n",
    "# reformat master transcript type\n",
    "ensembl_geneset['ensembl_transcript_type'].replace('vault_RNA', 'vaultRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'] = ensembl_geneset['ensembl_transcript_type']\n",
    "trans_dict = genomic_type_mapper['ensembl_master_transcript_type']\n",
    "for val in trans_dict.keys(): ensembl_geneset['master_transcript_type'].replace(val, trans_dict[val], inplace=True)\n",
    "\n",
    "# post-process reformatted data\n",
    "ensembl_geneset.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_geneset.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Ensembl Annotation Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ensembl-UniProt_ - `Homo_sapiens.GRCh38.113.uniprot.tsv`  \n",
    "Once the main ensembl gene set has been read in, the next step is to read in the `ensembl-uniprot` mapping file. These files are vital for successfully merging the ensembl identifiers with the uniprot data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url_uniprot = 'ftp://ftp.ensembl.org/pub/release-113/tsv/homo_sapiens/Homo_sapiens.GRCh38.113.uniprot.tsv.gz'\n",
    "data_downloader(url_uniprot, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "ensembl_uniprot = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.113.uniprot.tsv', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# preprocess data\n",
    "ensembl_uniprot.rename(columns={'xref': 'uniprot_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "ensembl_uniprot.replace('-', 'None', inplace=True)\n",
    "ensembl_uniprot.fillna('None', inplace=True)\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['xref_identity'].apply(lambda x: x != 'None')]\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['uniprot_id'].apply(lambda x: '-' not in x)]  # remove isoforms\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['info_type'].apply(lambda x: x == 'DIRECT')]\n",
    "# ensembl_uniprot['master_gene_type'] = ['protein-coding'] * len(ensembl_uniprot)\n",
    "# ensembl_uniprot['master_transcript_type'] = ['protein-coding'] * len(ensembl_uniprot)\n",
    "ensembl_uniprot.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "ensembl_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "ensembl_uniprot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ensembl-Entrez_ - `Homo_sapiens.GRCh38.113.entrez.tsv`  \n",
    "Once the main ensembl gene set has been read in, the next step is to read in the `ensembl-entrez` mapping file. These files are vital for successfully merging the ensembl identifiers with the entrez data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url_entrez = 'ftp://ftp.ensembl.org/pub/release-113/tsv/homo_sapiens/Homo_sapiens.GRCh38.113.entrez.tsv.gz'\n",
    "data_downloader(url_entrez, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "ensembl_entrez = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.113.entrez.tsv', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# preprocess data\n",
    "ensembl_entrez.rename(columns={'xref': 'entrez_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "ensembl_entrez = ensembl_entrez.loc[ensembl_entrez['db_name'].apply(lambda x: x == 'EntrezGene')]\n",
    "ensembl_entrez = ensembl_entrez.loc[ensembl_entrez['info_type'].apply(lambda x: x == 'DEPENDENT')]\n",
    "ensembl_entrez.replace('-', 'None', inplace=True)\n",
    "ensembl_entrez.fillna('None', inplace=True)\n",
    "ensembl_entrez.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "ensembl_entrez.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "ensembl_entrez.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge Annotation Data_ - `ensembl_uniprot` + `ensembl_entrez`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_entrez).intersection(set(ensembl_uniprot)))\n",
    "ensembl_annot = pandas.merge(ensembl_uniprot, ensembl_entrez, on=merge_cols, how='outer')\n",
    "ensembl_annot.fillna('None', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_annot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge Ensembl Annotation and Gene Set Data_ - `ensembl_geneset` + `ensembl_annot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_annot).intersection(set(ensembl_geneset)))\n",
    "ensembl = pandas.merge(ensembl_geneset, ensembl_annot, on=merge_cols, how='outer')\n",
    "ensembl.fillna('None', inplace=True)\n",
    "ensembl.replace('NA','None', inplace=True, regex=False)\n",
    "\n",
    "# preview data\n",
    "ensembl.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Save Cleaned Ensembl Data_  \n",
    "Save the cleaned Ensembl data so that it can be used when generating node metadata for transcript identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl.to_csv(processed_data_location + 'ensembl_identifier_data_cleaned.txt', header=True, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**UniProt Data**   \n",
    "_Human Gene Set Data_ - `uniprot_identifier_mapping.tab`\n",
    "\n",
    "This data was obtained by querying the [UniProt Knowledgebase](https://www.uniprot.org/uniprot/) using the *organism:\"Homo sapiens (Human) [9606]\"* keyword and including the following columns:\n",
    "- Entry (Standard)    \n",
    "- GeneID (*Genome Annotation*)  \n",
    "- Ensembl (*Genome Annotation*)  \n",
    "- HGNC (*Organism-specific*)  \n",
    "- Gene names (primary) (*Names & Taxonomy*)    \n",
    "- Gene synonym (primary) (*Names & Taxonomy*)    \n",
    "\n",
    "The URL to access the results of this query is obtained by clicking on the share symbol and copying the free-text from the box. To obtain the data in a tab-delimited format the following string is appended to the end of the URL: \"&format=tab\".\n",
    "\n",
    "**NOTE.** Be sure to obtain a new URL from the [UniProt Knowledgebase](https://www.uniprot.org/uniprot/) when rebuilding to ensure you are getting the most up-to-date data. This query was last generated on `01/02/2025`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://rest.uniprot.org/uniprotkb/stream?fields=accession%2Cid%2Cxref_geneid%2Cxref_ensembl%2Cxref_hgnc%2Cgene_primary%2Cgene_synonym%2Corganism_id%2Creviewed&format=tsv&query=%28%28organism_id%3A9606%29%29'\n",
    "data_downloader(url, unprocessed_data_location, 'uniprot_identifier_mapping.tab')\n",
    "\n",
    "# load data\n",
    "uniprot = pandas.read_csv(unprocessed_data_location + 'uniprot_identifier_mapping.tab', header=0, delimiter='\\t')\n",
    "uniprot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot.drop(columns=['Organism (ID)'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, and unnesting `\"|\"` delimited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot.fillna('None', inplace=True)  # replace NaN with 'None'\n",
    "uniprot.rename(columns={'Entry': 'uniprot_id',\n",
    "                        'GeneID': 'entrez_id',\n",
    "                        'Ensembl': 'transcript_stable_id',\n",
    "                        'HGNC': 'hgnc_id',\n",
    "                        'Gene Names (synonym)': 'synonyms',\n",
    "                        'Gene Names (primary)' :'symbol',\n",
    "                        'Reviewed': 'Status'}, inplace=True)\n",
    "\n",
    "# update space-delimited synonyms to a pipe (i.e. '|')\n",
    "uniprot['synonyms'] = uniprot['synonyms'].apply(lambda x: '|'.join(x.split()) if x.isupper() else x)\n",
    "\n",
    "# only keep reviewed entries\n",
    "uniprot = uniprot.loc[uniprot['Status'].apply(lambda x: x != 'unreviewed')]\n",
    "\n",
    "# explode nested data\n",
    "explode_df_uniprot = explodes_data(uniprot.copy(), ['transcript_stable_id', 'entrez_id', 'hgnc_id'], ';')\n",
    "explode_df_uniprot = explodes_data(explode_df_uniprot.copy(), ['symbol', 'synonyms'], '|')\n",
    "\n",
    "# strip out uniprot names\n",
    "explode_df_uniprot['transcript_stable_id'].replace('\\s.*','', inplace=True, regex=True)\n",
    "\n",
    "# remove duplicates\n",
    "explode_df_uniprot.drop(['Status'], axis=1, inplace=True)\n",
    "explode_df_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_uniprot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**NCBI Data**   \n",
    "_Human Gene Set Data_ - `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "ncbi_gene = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info', header=0, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, updating data types (i.e. making all columns type `str`), and unnesting `|` delimited data. Then, the `gene_type` variable is cleaned such that each of the variable's values are re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "ncbi_gene = ncbi_gene.loc[ncbi_gene['#tax_id'].apply(lambda x: x == 9606)]  # remove non-human rows\n",
    "ncbi_gene.replace('-', 'None', inplace=True)\n",
    "ncbi_gene.rename(columns={'GeneID': 'entrez_id', 'Symbol': 'symbol', 'Synonyms': 'synonyms'}, inplace=True)\n",
    "ncbi_gene['synonyms'] = ncbi_gene['synonyms'] + '|' + ncbi_gene['description'] + '|' + ncbi_gene['Full_name_from_nomenclature_authority'] + '|' + ncbi_gene['Other_designations']\n",
    "ncbi_gene['symbol'] = ncbi_gene['Symbol_from_nomenclature_authority'] + '|' + ncbi_gene['symbol']\n",
    "ncbi_gene['name'] = ncbi_gene['Full_name_from_nomenclature_authority'] + '|' + ncbi_gene['description']\n",
    "\n",
    "# explode nested data\n",
    "explode_df_ncbi_gene = explodes_data(ncbi_gene.copy(), ['symbol', 'synonyms', 'name', 'dbXrefs'], '|')\n",
    "\n",
    "# clean up results\n",
    "explode_df_ncbi_gene['entrez_id'] = explode_df_ncbi_gene['entrez_id'].astype(str)\n",
    "explode_df_ncbi_gene = explode_df_ncbi_gene.loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.split(':')[0] in ['Ensembl', 'HGNC', 'IMGT/GENE-DB'])]\n",
    "explode_df_ncbi_gene['hgnc_id'] = explode_df_ncbi_gene['dbXrefs'].loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.startswith('HGNC'))]\n",
    "explode_df_ncbi_gene['ensembl_gene_id'] = explode_df_ncbi_gene['dbXrefs'].loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.startswith('Ensembl'))]\n",
    "explode_df_ncbi_gene.fillna('None', inplace=True)\n",
    "\n",
    "# reformat entrez gene type\n",
    "explode_df_ncbi_gene['entrez_gene_type'] = explode_df_ncbi_gene['type_of_gene']\n",
    "gene_dict = genomic_type_mapper['entrez_gene_type']\n",
    "for val in gene_dict.keys(): explode_df_ncbi_gene['entrez_gene_type'].replace(val, gene_dict[val], inplace=True)\n",
    "# reformat master gene type\n",
    "explode_df_ncbi_gene['master_gene_type'] = explode_df_ncbi_gene['entrez_gene_type']\n",
    "gene_dict = genomic_type_mapper['master_gene_type']\n",
    "for val in gene_dict.keys(): explode_df_ncbi_gene['master_gene_type'].replace(val, gene_dict[val], inplace=True)\n",
    "\n",
    "# post-process reformatted data\n",
    "explode_df_ncbi_gene.drop(['type_of_gene', 'dbXrefs', 'description', 'Nomenclature_status', 'Modification_date',\n",
    "                           'LocusTag', '#tax_id', 'Full_name_from_nomenclature_authority', 'Feature_type',\n",
    "                           'Symbol_from_nomenclature_authority'], axis=1, inplace=True)\n",
    "explode_df_ncbi_gene['hgnc_id'] = explode_df_ncbi_gene['hgnc_id'].replace('HGNC:', '', regex=True)\n",
    "explode_df_ncbi_gene['ensembl_gene_id'] = explode_df_ncbi_gene['ensembl_gene_id'].replace('Ensembl:', '', regex=True)\n",
    "explode_df_ncbi_gene.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_ncbi_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Protein Ontology Identifier Mapping Data**   \n",
    "_Protein Ontology Identifier Data_ - `promapping.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://proconsortium.org/download/current/promapping.txt'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "pro_map = pandas.read_csv(unprocessed_data_location + 'promapping.txt', header=None, names=['pro_id', 'entry', 'pro_mapping'], delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Basic filtering to to include `Protein Ontology` mappings to `Uniprot` identifiers and cleaning to update formatting of accession values (i.e. removing `UniProtKB:`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_map = pro_map.loc[pro_map['entry'].apply(lambda x: x.startswith('Uni') and '_VAR' not in x and ', ' not in x)]  # keep 'UniProtKB' rows\n",
    "pro_map = pro_map.loc[pro_map['pro_mapping'].apply(lambda x: x.startswith('exact'))] # keep exact mappings\n",
    "pro_map['pro_id'].replace('PR:','PR_', inplace=True, regex=True)  # replace PR: with PR_\n",
    "pro_map['entry'].replace('(^\\w*\\:)','', inplace=True, regex=True)  # remove id prefixes\n",
    "pro_map = pro_map.loc[pro_map['pro_id'].apply(lambda x: '-' not in x)] # remove isoforms\n",
    "pro_map.rename(columns={'entry': 'uniprot_id'}, inplace=True)  # rename columns before merging\n",
    "pro_map.drop(['pro_mapping'], axis=1, inplace=True)  # remove uneeded columns\n",
    "pro_map.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "pro_map.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Merging Processed Genomic Identifier Data Sources  \n",
    "Merging all of the genomic identifier data sources is needed in order to create a map that can be used to integrate the different genomic data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Data Sources:* `hgnc` + `ensembl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(explode_df_hgnc.columns).intersection(set(ensembl.columns)))\n",
    "ensembl_hgnc_merged_data = pandas.merge(ensembl, explode_df_hgnc, on=merge_cols, how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "ensembl_hgnc_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Data Sources:* `ensembl_hgnc_merged_data` + `explode_df_uniprot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_hgnc_merged_data.columns).intersection(set(explode_df_uniprot.columns)))\n",
    "ensembl_hgnc_uniprot_merged_data = pandas.merge(ensembl_hgnc_merged_data, explode_df_uniprot, on=merge_cols, how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "ensembl_hgnc_uniprot_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_uniprot_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Data Sources:* `ensembl_hgnc_uniprot_merged_data` + `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = merge_cols = list(set(ensembl_hgnc_uniprot_merged_data).intersection(set(explode_df_ncbi_gene.columns)))\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data = pandas.merge(ensembl_hgnc_uniprot_merged_data, explode_df_ncbi_gene, on=merge_cols, how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Data Sources:* `ensembl_hgnc_uniprot_ncbi_merged_data` + `promapping.txt`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pandas.merge(ensembl_hgnc_uniprot_ncbi_merged_data, pro_map, on='uniprot_id', how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "merged_data.fillna('None', inplace=True)\n",
    "merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fix Symbol Formatting_  \n",
    "some genes are formatted similarly to dates (e.g. `DEC1`), which can be erroneously re-formatted during input as a date value (i.e. `1-DEC`). In order for the data to be successfully merged with other data sources, all date-formatted genes need to be resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dates = []\n",
    "for x in tqdm(list(merged_data['symbol'])):\n",
    "    if '-' in x and len(x.split('-')[0]) < 3 and len(x.split('-')[1]) == 3:\n",
    "        clean_dates.append(x.split('-')[1].upper() + x.split('-')[0])\n",
    "    else: clean_dates.append(x)\n",
    "\n",
    "# add cleaned date var back to data set\n",
    "merged_data['symbol'] = clean_dates\n",
    "merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# make sure that all gene and transcript type colunmns have none recoded to unknown or not protein-coding\n",
    "merged_data['hgnc_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['ensembl_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['entrez_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['master_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['master_transcript_type'].replace('None', 'not protein-coding', inplace=True, regex=False)\n",
    "merged_data['ensembl_transcript_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "\n",
    "# remove duplicates\n",
    "merged_data_clean = merged_data.drop_duplicates(subset=None, keep='first')\n",
    "\n",
    "# write data\n",
    "merged_data_clean.to_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt', header=True, sep='\\t', index=False)\n",
    "    \n",
    "# preview data\n",
    "merged_data_clean.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Create a Master Mapping Dictionary**  \n",
    "Although the above steps result in a `pandas.Dataframe` of the merged identifiers, there is still work needed in order to be able to obtain a complete mapping between the identifiers. For example, if you were to search for Entrez gene identifier `entrez_259234` you would find the following mappings: `entrez_259234-ENSG00000233316`, `entrez_259234-DSCR10`. If you only had `ENSG00000233316`, with the current data you would be unable to obtain the gene symbol without first mapping to the Entrez gene identifier. \n",
    "\n",
    "To solve this problem, we build a master dictionary where the keys are `ensembl_gene_id`, `transcript_stable_id`, `protein_stable_id`, `uniprot_id`, `entrez_id`, `hgnc_id`, `pro_id`, and `symbol` identifiers and values are the list of genomic identifiers that match to each identifier. It's important to note that there are several labeling identifiers (i.e. `name`, `chromosome`, `map_location`, `Other_designations`, `synonyms`, `transcript_name`, `*_gene_types`, and `trasnscript_type_update`), which will only be mapped when clustered against one of the primary identifier types (i.e. the keys described above).\n",
    "\n",
    "_Note_. The next chunk does a lot of heavy lifting and takes approximately ~40 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data to convert all nones, empty values, and unknowns to NaN\n",
    "for col in merged_data_clean.columns:\n",
    "    merged_data_clean[col] = merged_data_clean[col].apply(lambda x: '|'.join([i for i in x.split('|') if i != 'None']))\n",
    "merged_data_clean.replace(to_replace=['None', '', 'unknown'], value=numpy.nan, inplace=True)\n",
    "identifiers = [x for x in merged_data_clean.columns if x.endswith('_id')] + ['symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to dictionary\n",
    "master_dict = {}\n",
    "for idx in tqdm(identifiers):\n",
    "    grouped_data = merged_data_clean.groupby(idx)\n",
    "    grp_ids = set([x for x in list(grouped_data.groups.keys()) if x != numpy.nan])\n",
    "    for grp in grp_ids:\n",
    "        df = grouped_data.get_group(grp).dropna(axis=1, how='all')\n",
    "        df_cols, key = df.columns, idx + '_' + grp\n",
    "        val_df = [[col + '_' + x for x in set(df[col]) if isinstance(x, str)] for col in df_cols if col != idx]\n",
    "        if len(val_df) > 0:\n",
    "            if key in master_dict.keys(): master_dict[key] += [i for j in val_df for i in j if len(i) > 0]\n",
    "            else: master_dict[key] = [i for j in val_df for i in j if len(i) > 0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finalizing Master Mapping Dictionary_  \n",
    "Then, we need to identify a master gene and transcript type for each entity because the last ran code chunk can result in several genes and transcripts with differing types (i.e. `protein-coding` or `not protein-coding`). The next step collects all information for each gene and transcript and performs a voting procedure to select a single primary gene and transcript type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_mapped_identifiers = dict()\n",
    "for key, values in tqdm(master_dict.items()):\n",
    "    identifier_info = set(values); gene_prefix = 'master_gene_type_'; trans_prefix = 'master_transcript_type_'\n",
    "    if key.split('_')[0] in ['protein', 'uniprot', 'pro']: pass\n",
    "    elif 'transcript' in key:\n",
    "        trans_match = [x.replace(trans_prefix, '') for x in values if trans_prefix in x]\n",
    "        if len(trans_match) > 0:\n",
    "            t_type_list = ['protein-coding' if ('protein-coding' in trans_match or 'protein_coding' in trans_match) else 'not protein-coding']\n",
    "            identifier_info |= {'transcript_type_update_' + max(set(t_type_list), key=t_type_list.count)}\n",
    "    else:\n",
    "        gene_match = [x.replace(gene_prefix, '') for x in values if x.startswith(gene_prefix) and 'type' in x]\n",
    "        if len(gene_match) > 0:\n",
    "            g_type_list = ['protein-coding' if ('protein-coding' in gene_match or 'protein_coding' in gene_match) else 'not protein-coding']\n",
    "            identifier_info |= {'gene_type_update_' + max(set(g_type_list), key=g_type_list.count)}\n",
    "    reformatted_mapped_identifiers[key] = identifier_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of the dictionary\n",
    "# output > 4GB requires special approach: https://stackoverflow.com/questions/42653386/does-pickle-randomly-fail-with-oserror-on-large-files\n",
    "filepath = processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl'\n",
    "\n",
    "# defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "max_bytes, bytes_out = 2**31 - 1, pickle.dumps(reformatted_mapped_identifiers)\n",
    "n_bytes = sys.getsizeof(bytes_out)\n",
    "\n",
    "with open(filepath, 'wb') as f_out:\n",
    "    for idx in range(0, n_bytes, max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "### CLEAN ONTOLOGIES <a class=\"anchor\" id=\"clean-ontologies\"></a>\n",
    "***\n",
    "\n",
    "The ontology cleaning step includes the following error checks, each of which are explained below and each of which are applied to individual ontologies, the set of merged ontologies or both: (1) Value Errors, (2) Identifier Errors, (3) Duplicate and Obsolete Entities, (4) Punning Errors, and (5) Entity Normalization Errors.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Value Errors  \n",
    "*** \n",
    "**Level:** `individual ontology`; `merged-ontology`    \n",
    "\n",
    "**Description:** This check utilizes the [`owlready2`](https://pypi.org/project/Owlready2/) library to read in each of the ontologies. This library is strict and will catch a wide variety of value errors. \n",
    "\n",
    "**Solution:** Parse the error message using the provided `ErrorType` and line number and repair it. For `ValueErrors` incorrectly typed input are re-typed.\n",
    "\n",
    "*Example Findings*  \n",
    "The [Cell Line Ontology](http://www.clo-ontology.org/) yield the following error message:\n",
    "\n",
    "```python\n",
    "ValueError: invalid literal for int() with base 10: '永生的乳腺衍生细胞系细胞'\n",
    "...\n",
    "OwlReadyOntologyParsingError: RDF/XML parsing error in file clo_with_imports.owl, line 10970, column 99.\n",
    "```\n",
    "\n",
    "This tells us that we need to repair the triple containing the Literal '永生的乳腺衍生细胞系细胞' by removing it and redefining it as a `string`, rather than an `int` as it is currently defined as. This is currently noted as an issue in the [Cell Line Ontology's](http://www.clo-ontology.org/) GitHub repo ([issue #48](https://github.com/CLO-ontology/CLO/issues/48)). \n",
    "\n",
    "<br>\n",
    "\n",
    "### Identifier Errors  \n",
    "*** \n",
    "**Level:** `individual ontology`; `merged-ontology`  \n",
    "\n",
    "**Description:** This check verifies consistency of identifier prefixes. For example, we want to find identifiers that are incorrectly formatted like occurrences of `PRO_XXXXXXX` which should be `PR_XXXXXXX`.\n",
    "\n",
    "**Solution:** Incorrectly formatted class identifiers are updated. This is a tricky task to do in an automated manner and is something that should be updated if any new ontologies are added to the `PheKnowLator` build. Currently, the code below checks and logs any hits, but only fixes the following known errors: Vaccine Ontology: `PRO` which should be `PR`.\n",
    "\n",
    "*Example Findings*  \n",
    "Running this check revealed mislabeling of `2` [pROtein Ontology](https://proconsortium.org/) identifiers in the [Vaccine Ontology](http://www.violinet.org/vaccineontology/) (see [this](https://github.com/vaccineontology/VO/issues/4) GitHub issue).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Obsolete and/or Deprecated Entities\n",
    "*** \n",
    "**Level:** `individual ontology`; `merged-ontology`  \n",
    "\n",
    "**Description:** Verify that the ontology only contains current content.\n",
    "\n",
    "**Solution:** All obsolete classes and any triples that they participate in are removed from an ontologies.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Normalization Errors  \n",
    "*** \n",
    "**Level:** `merged-ontology`\n",
    "\n",
    "These checks are performed at the individual- and merged-ontology levels. There are two types of checks that are performed:  \n",
    "\n",
    "<u>Normalize Existing Ontology Classes</u>  \n",
    "  - **Description:** Checks for inconsistencies in ontology classes that overlap with non-ontology entity identifiers (e.g. if HP includes `HGNC` identifiers, but PheKnowLator utilizes `Entrez` identifiers). \n",
    "\n",
    "  - **Solution:** While there are other types of identifiers, we currently focus primarily on resolving errors involving the genomic identifiers, since we have a master dictionary we can use (`Merged_gene_rna_protein_identifiers.pkl` -- which is generated during the data preporcessing steps of the build). This check can be updated in future iterations to include other types of identifiers, but given our detailed examination of the `v2.0.0` ontologies, these were the identifier types that needed repair.\n",
    "\n",
    "<u>Normalize Duplicate Ontology Concepts</u>  \n",
    "  - **Description:** Make sure that all classes that represent the same entity are connected to each other. For example, consider the following: the [Sequence Ontology](http://www.sequenceontology.org/), [ChEBI](https://www.ebi.ac.uk/chebi), and [PRotein Ontology](https://proconsortium.org/) all include terms for protein, but none of these classes are connected to each other.\n",
    "\n",
    "  - **Solution:** Choose a primary concept for all duplicate scenarios and make duplicate concepts an `RDFS:subClassOf` the primary concept. In the future, this check could be improved by leveraging [KBOOM](https://www.biorxiv.org/content/10.1101/048843v3).\n",
    "\n",
    "*Example Findings*  \n",
    "The follow classes occur in all of the ontologies used in the current build and have to be normalized so that there are not multiple versions of the same concept:  \n",
    "\n",
    "- Gene: [VO](http://purl.obolibrary.org/obo/OGG_0000000002)  \n",
    "  - <u>Solution</u>: Make the `VO` imported `OGG` class a subclass of the `SO` gene term  \n",
    "\n",
    "- Protein: [SO](http://purl.obolibrary.org/obo/SO_0000104), [PRO](http://purl.obolibrary.org/obo/PR_000000001), [ChEBI](http://purl.obolibrary.org/obo/CHEBI_36080) \n",
    "  - <u>Solution</u>: Make the `CHEBI` and `PRO` classes a subclass of the `SO` protein term  \n",
    "  \n",
    "- Disorder: [VO](http://purl.obolibrary.org/obo/OGMS_0000045)  \n",
    "  - <u>Solution</u>: Make the `VO` imported `OGMS` class a subclass of the `MONDO` disease term  \n",
    "\n",
    "- Antigen: [VO](http://purl.obolibrary.org/obo/OBI_1110034)  \n",
    "  - <u>Solution</u>: Make the `VO` imported OBI class a subclass of the `CHEBI` antigen term  \n",
    "\n",
    "- Gelatin: [VO]('http://purl.obolibrary.org/obo/VO_0003030') \n",
    "  - <u>Solution</u>: Make the `VO` class a subclass of the `CHEBI` gelatin term \n",
    "\n",
    "- Hormone: [VO](http://purl.obolibrary.org/obo/FMA_12278) \n",
    "  - <u>Solution</u>: Make the `VO` imported `FMA` class a subclass of the `CHEBI` hormone term\n",
    "\n",
    "<br>\n",
    "\n",
    "### Punning Errors \n",
    "*** \n",
    "**Level:** `individual ontology`; `merged-ontology`\n",
    "\n",
    "**Description:** [Punning](https://www.w3.org/2007/OWL/wiki/Punning) or redeclaration errors occur for a few different reasons, but the primary or most prevalent cause observed in the ontologies used in `PheKnowLator` is due to an `owl:ObjectProperty` being incorrectly redeclared as an `owl:AnnotationProperty` or an `owl:Class` also being defined as an `OWL:ObjectProperty`. \n",
    "\n",
    "**Solution:** Consistent with the solution described [here](https://github.com/oborel/obo-relations/issues/130), for `owl:ObjectProperty` redeclarations we remove all `owl:AnnotationProperty` declarations. For all `owl:Class` redeclarations, we remove all `owl:ObjectProperty` redeclarations. \n",
    "\n",
    "*Example Findings* \n",
    "The [Cell Line Ontology](http://www.clo-ontology.org/) had 7 object properties that were illegally redeclared and triggered punning errors. More details regarding these errors are shown below. \n",
    "\n",
    "```bash\n",
    "2020-12-03 20:57:15,616 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002091 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002091>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002091>))]\n",
    "2020-12-03 20:57:15,619 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/BFO_0000062 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/BFO_0000062>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/BFO_0000062>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/BFO_0000063 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/BFO_0000063>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/BFO_0000063>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002222 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002222>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002222>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0000087 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0000087>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0000087>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002161 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002161>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002161>))]\n",
    "```\n",
    "\n",
    "From this message, we can see that we need to remove the following `owl:ObjectProperty` redeclared to `owl:AnnotationProperty`: `RO_0002091`, `BFO_0000062`, `BFO_0000063`, `RO_0002222`, `RO_0000087`, `RO_0002161`. There were also 2 classes (i.e. `CLO_0054407` and `CLO_0054409`) defined as being a `owl:Class` and an `owl:ObjectProperty`. This is currently noted as an issue in the Cell Line Ontology's GitHub repo [issue #43](https://github.com/CLO-ontology/CLO/issues/43))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***  \n",
    "### Set-Up Environment\n",
    "***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment and run to install any required modules from notebooks/requirements.txt\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure builds/*.py files and pkt_kg scripts can be reached from notebooks dir\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from rdflib import Graph\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import script containing helper functions\n",
    "from pkt_kg.utils import * \n",
    "from builds.ontology_cleaning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment variables\n",
    "write_location = '../resources/ontologies'\n",
    "knowledge_graphs_location = '../resources/knowledge_graphs'\n",
    "processed_data_location = '../resources/processed_data/'\n",
    "\n",
    "# set global namespaces\n",
    "schema = Namespace('http://www.w3.org/2001/XMLSchema#')\n",
    "obo = Namespace('http://purl.obolibrary.org/obo/')\n",
    "oboinowl = Namespace('http://www.geneontology.org/formats/oboInOwl#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions needed for processing ontologies\n",
    "def logically_verifies_cleaned_ontologies(graph, temp_dir, file_location, owltools_location):\n",
    "    \"\"\"Logically verifies an ontology by running the ELK deductive logic reasoner. Before running the reasoner\n",
    "    the instantiated RDFLib object is saved locally.\n",
    "\n",
    "    Args:\n",
    "        graph: An RDFLib Graph object containing data.\n",
    "        temp_dir: A string specifying where where to read from and write to.\n",
    "        file_location: The name of the file to read and write to in the temp_dir directory.\n",
    "        owltools_location: A string specifying the location of OWLTOOLs (included in pkt_kg no need to download).\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Logically Verifying Ontology')\n",
    "\n",
    "    # save graph in order to run reasoner\n",
    "    filename = temp_dir + '/' + file_location\n",
    "    graph.serialize(destination=filename, format='xml')\n",
    "    \n",
    "    # run reasoner\n",
    "    command = \"{} {} --reasoner {} --run-reasoner --assert-implied -o {}\"\n",
    "    return_code = os.system(command.format(owltools_location, filename, 'elk', filename))\n",
    "    if return_code != 0: raise ValueError('Reasoner Finished with Errors.')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "### INDIVIDUAL ONTOLOGIES <a class=\"anchor\" id=\"individual-ontologies\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** This section focuses on cleaning the individual ontologies which consists of fixing: (1) Parsing Errors; (2) Identifier Errors; (3) Deprecated and Obsolete Classes; and (4) Punning Errors.\n",
    "\n",
    "\n",
    "**Inputs:** A directory (`write_location`) containing ontology files (`.owl`)\n",
    "\n",
    "**Outputs:** A directory (`write_location`) containing cleaned ontology files (`.owl`)  \n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### ⚡ Important ⚡\n",
    "\n",
    "The `OWL API`, when running the [ELK reasoner](), seems to add back some of the errors that this script removes.\n",
    "\n",
    "- <u>Example 1</u>: In the Vaccine Ontology, we fix prefix errors where `\"PR\"` is recorded as `\"PRO\"`. If you save the ontology without running the reasoner and reload it, the fix remains. If you open it after running ELK, the fix has been reversed. \n",
    "\n",
    "\n",
    "- <u>Example 2</u>: When we create the human subset of the Protein Ontology we verify that it contains only a single large connected component. If you re-calculate the number of connected components after running ELK, there will be three components.  \n",
    "\n",
    "Luckily, the merged ontologies are not logically verified using a reasoner, thus the version used to build knowledge graphs remains free of these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# instantiate and set-up class\n",
    "ont_data = OntologyCleaner('', '', '', write_location)\n",
    "\n",
    "# updating ontology info dictionary\n",
    "ont_data.ontology_info = {k.split('/')[-1]: {} for k, v in ont_data.ontology_info.items()}\n",
    "\n",
    "# set owl tools location\n",
    "ont_data.owltools_location = '../pkt_kg/libs/owltools'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# clean data\n",
    "for ont in ont_data.ontology_info.keys():\n",
    "    print('\\n#### Processing Ontology: {} ####'.format(ont.upper()))\n",
    "    ont_data.ont_file_location = ont\n",
    "    ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "    \n",
    "    # get starting statistics\n",
    "    ont_data.updates_ontology_reporter()\n",
    "    \n",
    "    # clean ontologies\n",
    "    ont_data.fixes_ontology_parsing_errors()\n",
    "    ont_data.fixes_identifier_errors()\n",
    "    ont_data.removes_deprecated_obsolete_entities()\n",
    "    ont_data.fixes_punning_errors()\n",
    "    \n",
    "    # run cleaned ontology through the elk reasoner\n",
    "    logically_verifies_cleaned_ontologies(ont_data.ont_graph,\n",
    "                                          ont_data.temp_dir,\n",
    "                                          ont_data.ont_file_location,\n",
    "                                          ont_data.owltools_location)\n",
    "\n",
    "    # verifies no errors caused during cleaning\n",
    "#     ontology_file_formatter(ont_data.temp_dir, '/' + ont_data.ont_file_location, ont_data.owltools_location)\n",
    "    \n",
    "    # read in cleaned, verified, and updated ontology containing inference\n",
    "    print('Reading in Cleaned Ontology -- Needed to Calculate Final Statistics')\n",
    "    ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "    \n",
    "    # get finishing statistics\n",
    "    ont_data.updates_ontology_reporter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "### MERGED ONTOLOGIES <a class=\"anchor\" id=\"merge-ontologies\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** In this step, the [OWLTools](https://github.com/owlcollab/owltools) library is used to merge the directory of cleaned ontology files into a single ontology file. Then, the following cleaning steps are performed: (1) Identifier Errors; (2) Duplicate Classes; (3) Duplicate Class Concepts; and (4) Punning Errors.  \n",
    "\n",
    "**Inputs:** A directory of ontology files (`.owl`)\n",
    "\n",
    "**Outputs:** `PheKnowLator_MergedOntologies.owl`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merge Clean Ontology Data')\n",
    "ont_data.ont_file_location = ont_data.merged_ontology_filename\n",
    "\n",
    "# reorder list of ontology files to prepare for merging\n",
    "onts = [ont_data.temp_dir + '/' + x for x in list(ont_data.ontology_info.keys())\n",
    "        if x != ont_data.merged_ontology_filename]\n",
    "\n",
    "# merge ontologies\n",
    "merges_ontologies(onts, ont_data.temp_dir + '/', ont_data.ont_file_location, ont_data.owltools_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load merged ontologies into RDF Lib Graph object\n",
    "print('Loading Merged Ontology Data')\n",
    "ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "\n",
    "# add merged ontology to dict\n",
    "ont_data.ontology_info[ont_data.ont_file_location] = {}\n",
    "\n",
    "# get stats on merged ontologies\n",
    "print(derives_graph_statistics(ont_data.ont_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Merged Ontologies\n",
    "🤔 *IMPORTANT*🤔  Please note there are a few decisions that can made be made at this point that you may want to consider. For our monthly `PheKnowLator` builds, we prefer to use Entrez gene identifiers. If you have run the [`Data_Preparation.ipynb`](https://github.com/callahantiff/PheKnowLator/blob/master/notebooks/Data_Preparation.ipynb) Jupyter Notebook without makeing updates, you would have also committed yourself to using this type of gene identifier. If you have not done with this and do not want to use Entrez gene, but rather prefer to use what the ontologies provide, please comment out `ont_data.normalizes_existing_classes()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get starting statistics\n",
    "ont_data.updates_ontology_reporter()\n",
    "\n",
    "# clean merged ontologies\n",
    "ont_data.fixes_identifier_errors()\n",
    "ont_data.normalizes_duplicate_classes()\n",
    "ont_data.normalizes_existing_classes()\n",
    "ont_data.fixes_punning_errors()\n",
    "\n",
    "# get finishing statistics\n",
    "print(derives_graph_statistics(ont_data.ont_graph))\n",
    "ont_data.updates_ontology_reporter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output and Save Results\n",
    "The cleaned merged ontology file is saved to the `resources/knowledge_graphs` directory where it can be detected by the `PheKnowLator` algorithm during the build process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save and Format Merged Ontology Data')\n",
    "ont_data.ont_graph.serialize(destination=knowledge_graphs_location + '/' + ont_data.ont_file_location, format='xml')\n",
    "ontology_file_formatter(knowledge_graphs_location, '/' + ont_data.ont_file_location, ont_data.owltools_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Ontology Cleaning Results  \n",
    "To view the results of the ontology cleaning process print the `ont_data.ontology_info` dictionary. This dictionary is keyed by ontology filename and contains a separate dictionary for each ontology with descriptions of the results for each error check that is performed at the individual- and merged-ontology level. The results are also saved to `resources/ontologies/ontology_cleaning_report.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output locally\n",
    "ont_order = sorted([x for x in ont_data.ontology_info.keys() if not x.startswith('Phe')]) + [ont_data.ont_file_location]\n",
    "with open(ont_data.temp_dir + '/ontology_cleaning_report.txt', 'w') as o:\n",
    "    o.write('=' * 50 + '\\n{}'.format('ONTOLOGY CLEANING REPORT'))\n",
    "    o.write('\\n{}\\n'.format(str(datetime.datetime.utcnow().strftime('%a %b %d %X UTC %Y'))) + '=' * 50 + '\\n\\n')\n",
    "    for key in ont_order:\n",
    "        o.write('\\n\\n\\nONTOLOGY: {}\\n'.format(key)); o.write('*' * (len(key) + 10) + '\\n\\n')\n",
    "        x = ont_data.ontology_info[key]\n",
    "        if 'Original GCS URL' in x.keys(): o.write('\\t- Original GCS URL: {}\\n'.format(x['Original GCS URL']))\n",
    "        if 'Processed GCS URL' in x: o.write('\\t- Processed GCS URL: {}\\n'.format(x['Processed GCS URL']))\n",
    "        o.write('\\t- Statistics Before Cleaning:\\n\\t\\t- {}\\n'.format(x['Starting Statistics']))\n",
    "        o.write('\\t- Statistics After Cleaning:\\n\\t\\t- {}\\n'.format(x['Final Statistics']))\n",
    "        if 'ValueErrors' in x.keys():\n",
    "            if isinstance( x['ValueErrors'], str): o.write('\\t- Value Errors (n=1):\\n\\t\\t- {}\\n'.format(x['ValueErrors']))\n",
    "            else:\n",
    "                for i in x['ValueErrors']: o.write('\\t\\t- {}\\n'.format(str(i)))\n",
    "        else: o.write('\\t- Value Errors: 0\\n')     \n",
    "        if x['IdentifierErrors'] != 'None':\n",
    "            o.write('\\t- Identifier Errors (n={}):\\n'.format(len(x['IdentifierErrors'].split(', '))))\n",
    "            for i in x['IdentifierErrors'].split(', '): o.write('\\t\\t- {}\\n'.format(str(i)))\n",
    "        else: o.write('\\t- Identifier Errors: 0\\n')\n",
    "        if 'PheKnowLator_MergedOntologies' not in key:\n",
    "            if x['Deprecated'] != 'None':\n",
    "                o.write('\\t- Deprecated Classes (n={}):\\n'.format(len(x['Deprecated'])))\n",
    "                for i in x['Deprecated']: o.write('\\t\\t- {}\\n'.format(str(i)))\n",
    "            else: o.write('\\t- Deprecated Classes: 0\\n') \n",
    "            if x['Obsolete'] != 'None':\n",
    "                o.write('\\t- Obsolete Classes (n={}):\\n'.format(len(x['Obsolete'])))\n",
    "                for i in x['Obsolete']: o.write('\\t\\t- {}\\n'.format(str(i)))\n",
    "            else: o.write('\\t- Obsolete Classes: 0\\n')\n",
    "        o.write('\\t- Punning Errors:\\n')\n",
    "        if x['PunningErrors - Classes'] != 'None':\n",
    "            o.write('\\t\\t- Classes (n={}):\\n'.format(len(x['PunningErrors - Classes'].split(', '))))\n",
    "            for i in x['PunningErrors - Classes'].split(', '): o.write('\\t\\t\\t- {}\\n'.format(i))\n",
    "        else: o.write('\\t\\t- Classes: 0\\n')\n",
    "        if x['PunningErrors - ObjectProperty'] != 'None':\n",
    "            o.write('\\t\\t- Object Properties (n={}):\\n'.format(len(x['PunningErrors - ObjectProperty'].split(', '))))\n",
    "            for i in x['PunningErrors - ObjectProperty'].split(', '): o.write('\\t\\t\\t- {}\\n'.format(i))\n",
    "        else: o.write('\\t\\t- Object Properties: 0\\n')\n",
    "        if 'Normalized - Duplicates' in x.keys():\n",
    "            o.write('\\t- Normalization:\\n')\n",
    "            if x['Normalized - Duplicates'] != 'None':\n",
    "                o.write('\\t\\t- Existing Entity Normalization (n={}):\\n'.format(len(x['Normalized - Duplicates'].split(', '))))\n",
    "                for i in x['Normalized - Duplicates'].split(', '): o.write('\\t\\t\\t- {}\\n'.format(i))\n",
    "            else: o.write('\\t\\t- Entity Normalization: 0\\n')\n",
    "            if x['Normalized - Gene IDs'] != 'None': o.write('\\t\\t- Normalized HGNC IDs: {}\\n'.format(x['Normalized - Gene IDs']))\n",
    "            if x['Normalized - NonOnt'] != 'None': o.write('\\t\\t- Other Classes that May Need Normalization: {}\\n'.format(x['Normalized - NonOnt']))\n",
    "            if x['Normalized - Dep'] != 'None':\n",
    "                o.write('\\t\\t- Deprecated Ontology HGNC Identifiers Needing Alignment (n={}):\\n'.format(len(x['Normalized - Dep'])))\n",
    "                for i in x['Normalized - Dep']: o.write('\\t\\t- {}\\n'.format(i))\n",
    "            else: o.write('\\t\\t- Deprecated Ontology HGNC Identifiers Needing Alignment: 0\\n')\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "### CREATE MERGED ONTOLOGIES GRAPH <a class=\"anchor\" id=\"graph-ontologies\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run OWL-NETS via PheKnowLator on each ontology file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"nan-nan, resources/edge_data/nan.txt\" > $resource_data_location/edge_source_list.txt\n",
    "!echo \"nan-nan|;;|class-class|RO_0002001|http://purl.obolibrary.org/obo/|http://purl.obolibrary.org/obo/|t|0;1|None|None|None\" > $resource_data_location/resource_info.txt\n",
    "!echo \"nan  nan\" > $edge_data_location/nan.txt\n",
    "\n",
    "import os\n",
    "ontology_kg = []\n",
    "os.rename(ontology_data_location + 'PheKnowLator_MergedOntologies.owl', ontology_data_location + 'merged_with_imports.owl')\n",
    "for i in ['ro', 'chebi', 'pr', 'mondo', 'go', 'pw', 'so', 'hp', 'ext', 'vo', 'clo']:\n",
    "    shutil.copy(ontology_data_location + i + '_with_imports.owl', resource_data_location + 'knowledge_graphs/PheKnowLator_MergedOntologies.owl')\n",
    "    !cd ..; source ../pky/bin/activate; python3 main.py\n",
    "    os.rename(resource_data_location + 'knowledge_graphs/PheKnowLator_v3.1.2_full_subclass_inverseRelations_OWLNETS_SUBCLASS_purified.nt',\n",
    "              ontology_data_location + i + '.nt')\n",
    "    df = pd.read_csv(ontology_data_location + i + '.nt', sep=' ', header=None)\n",
    "    df['Source'] = i.upper()\n",
    "    ontology_kg.append(df)\n",
    "    for f in os.listdir(resource_data_location + 'knowledge_graphs/'):\n",
    "        os.remove(resource_data_location + 'knowledge_graphs/' + f)\n",
    "\n",
    "ontology_kg = pd.concat(ontology_kg)\n",
    "ontology_kg['Source'] = ontology_kg['Source'].replace('EXT', 'Uberon')\n",
    "ontology_kg['Source'] = ontology_kg['Source'].replace('CHEBI', 'ChEBI')\n",
    "ontology_kg['Source'] = ontology_kg['Source'].replace('MONDO', 'Mondo')\n",
    "ontology_kg['Source'] = ontology_kg['Source'].replace('HP', 'HPO')\n",
    "ontology_kg['Source'] = ontology_kg['Source'].replace('PR', 'PRO')\n",
    "ontology_kg.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run OWL-NETS also on the merged ontology graph. This ensures to type \"extra\" edges introduced by OWL-NETS to ensure a single connected component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(ontology_data_location + 'merged_with_imports.owl', resource_data_location + 'knowledge_graphs/PheKnowLator_MergedOntologies.owl')\n",
    "!cd ..; source ../pky/bin/activate; python3 main.py\n",
    "os.rename(resource_data_location + 'knowledge_graphs/PheKnowLator_v3.1.2_full_subclass_inverseRelations_OWLNETS_SUBCLASS_purified.nt',\n",
    "        ontology_data_location + 'merged.nt')\n",
    "merged_ontology = pd.read_csv(ontology_data_location + 'merged.nt', sep=' ', header=None)\n",
    "for f in os.listdir(resource_data_location + 'knowledge_graphs/'):\n",
    "    os.remove(resource_data_location + 'knowledge_graphs/' + f)\n",
    "merged_ontology = merged_ontology[~merged_ontology[0].str.startswith('<https://o')] # This refers to orcid Ids\n",
    "merged_ontology = merged_ontology[~merged_ontology[2].str.startswith('<https://o')]\n",
    "merged_ontology = merged_ontology[~merged_ontology[0].str.startswith('<https://search.clinicalgenome.org/kb/conditions/MONDO:')] # Wrong Mondo IDs\n",
    "merged_ontology = merged_ontology[~merged_ontology[2].str.startswith('<https://search.clinicalgenome.org/kb/conditions/MONDO:')]\n",
    "merged_ontology[0] = merged_ontology[0].str.replace('http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=','http://identifiers.org/hgnc/')\n",
    "merged_ontology[2] = merged_ontology[2].str.replace('http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=','http://identifiers.org/hgnc/')\n",
    "merged_ontology_kg = pd.merge(merged_ontology, ontology_kg, how='left', on=[0, 1, 2])\n",
    "merged_ontology_kg['Source'] = merged_ontology_kg['Source'].fillna('OWLNETS')\n",
    "merged_ontology_kg['Source'] = merged_ontology_kg['Source'].replace('None', 'OWLNETS')\n",
    "\n",
    "mask = ( # Fix mislabeled PRO\n",
    "    merged_ontology_kg[0].str.startswith('<http://purl.obolibrary.org/obo/PR_') &\n",
    "    (merged_ontology_kg[1] == '<http://purl.obolibrary.org/obo/pr#has_gene_template>') &\n",
    "    merged_ontology_kg[2].str.startswith('<http://identifiers.org/hgnc/')\n",
    ")\n",
    "merged_ontology_kg.loc[mask, 'Source'] = 'PRO'\n",
    "\n",
    "merged_ontology_kg = merged_ontology_kg.drop(columns=['3_x','3_y'])\n",
    "merged_ontology_kg.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still noticed the presence of HGNC identifiers and DOID after running OWL-NETS. We map them on NCBI Entrez identifiers and Mondo terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ontology_kg[0].str[:20].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mondo_graph = Graph().parse(ontology_data_location + 'mondo_with_imports.owl')\n",
    "\n",
    "mondo_dbxref = gets_ontology_class_dbxrefs(mondo_graph)[0]\n",
    "\n",
    "# Fix DOIDs (substitute : with _)\n",
    "mondo_dict = {str(k).replace(':','_').upper(): {str(i).split('/')[-1].replace(':','_') for i in v}\n",
    "              for k, v in mondo_dbxref.items() if 'doid' in str(k)}\n",
    "list({**mondo_dict}.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'DOID_MONDO_MAP.txt', 'w') as outfile:\n",
    "    for k, v in mondo_dict.items():\n",
    "        outfile.write(str(k) + '\\t' + str(v).replace('{','').replace('\\'','').replace('}','') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_mondo_map = pd.read_csv(processed_data_location+'DOID_MONDO_MAP.txt', header=None, delimiter='\\t')\n",
    "doid_mondo_map[1] = doid_mondo_map[1].str.split(', ')\n",
    "doid_mondo_map = doid_mondo_map.explode(1)\n",
    "doid_mondo_map[0] = \"<http://purl.obolibrary.org/obo/\" + doid_mondo_map[0].astype(str) + \">\"\n",
    "doid_mondo_map[1] = \"<http://purl.obolibrary.org/obo/\" + doid_mondo_map[1].astype(str) + \">\"\n",
    "doid_mondo_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ontology_kg = merged_ontology_kg.rename(columns={0: 'Subject', 1: 'Predicate', 2: 'Object'})\n",
    "merged_ontology_kg = pd.merge(merged_ontology_kg, doid_mondo_map, left_on='Subject', right_on=0, how='left')\n",
    "merged_ontology_kg[1] = merged_ontology_kg[1].fillna(merged_ontology_kg['Subject'])\n",
    "merged_ontology_kg.drop(columns=[0, 'Subject'], inplace=True)\n",
    "merged_ontology_kg = merged_ontology_kg.rename(columns={1: 'Subject'})\n",
    "merged_ontology_kg = pd.merge(merged_ontology_kg, doid_mondo_map, left_on='Object', right_on=0, how='left')\n",
    "merged_ontology_kg[1] = merged_ontology_kg[1].fillna(merged_ontology_kg['Object'])\n",
    "merged_ontology_kg.drop(columns=[0, 'Object'], inplace=True)\n",
    "merged_ontology_kg = merged_ontology_kg.rename(columns={1: 'Object'})\n",
    "merged_ontology_kg = merged_ontology_kg.rename(columns={'Subject': 0, 'Predicate': 1, 'Object':2})\n",
    "merged_ontology_kg = merged_ontology_kg[~merged_ontology_kg[0].str.startswith('<http://purl.obolibrary.org/obo/DOID_')]\n",
    "merged_ontology_kg = merged_ontology_kg[~merged_ontology_kg[2].str.startswith('<http://purl.obolibrary.org/obo/DOID_')]\n",
    "merged_ontology_kg.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc_ncbi = pd.read_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt',\n",
    "                        sep='\\t', dtype={'entrez_id':'Int64'})[['hgnc_id', 'entrez_id']]\n",
    "hgnc_ncbi = hgnc_ncbi[(hgnc_ncbi['entrez_id'].astype(str) != '<NA>') & (~hgnc_ncbi['hgnc_id'].isna())]\n",
    "hgnc_ncbi['hgnc_id'] = hgnc_ncbi['hgnc_id'].astype(str).str.replace('.0', '')\n",
    "hgnc_ncbi['hgnc_id'] = hgnc_ncbi['hgnc_id'].str.replace('HGNC:', '')\n",
    "hgnc_ncbi['hgnc_id'] = '<http://identifiers.org/hgnc/' + hgnc_ncbi['hgnc_id'].astype(str) + '>'\n",
    "hgnc_ncbi['entrez_id'] = '<http://www.ncbi.nlm.nih.gov/gene/' + hgnc_ncbi['entrez_id'].astype(str) + '>'\n",
    "hgnc_ncbi.drop_duplicates(inplace=True)\n",
    "hgnc_ncbi.rename(columns={'hgnc_id': 0, 'entrez_id': 1}, inplace=True)\n",
    "hgnc_ncbi.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ontology_kg = merged_ontology_kg.rename(columns={0: 'Subject', 1: 'Predicate', 2: 'Object'})\n",
    "merged_ontology_kg = pd.merge(merged_ontology_kg, doid_mondo_map, left_on='Subject', right_on=0, how='left')\n",
    "merged_ontology_kg[1] = merged_ontology_kg[1].fillna(merged_ontology_kg['Subject'])\n",
    "merged_ontology_kg.drop(columns=[0, 'Subject'], inplace=True)\n",
    "merged_ontology_kg = merged_ontology_kg.rename(columns={1: 'Subject'})\n",
    "merged_ontology_kg = pd.merge(merged_ontology_kg, doid_mondo_map, left_on='Object', right_on=0, how='left')\n",
    "merged_ontology_kg[1] = merged_ontology_kg[1].fillna(merged_ontology_kg['Object'])\n",
    "merged_ontology_kg.drop(columns=[0, 'Object'], inplace=True)\n",
    "merged_ontology_kg = merged_ontology_kg.rename(columns={1: 'Object'})\n",
    "merged_ontology_kg = merged_ontology_kg.rename(columns={'Subject': 0, 'Predicate': 1, 'Object':2})\n",
    "merged_ontology_kg = merged_ontology_kg[~merged_ontology_kg[0].str.startswith('<http://identifiers.org/hgnc/')]\n",
    "merged_ontology_kg = merged_ontology_kg[~merged_ontology_kg[2].str.startswith('<http://identifiers.org/hgnc/')]\n",
    "merged_ontology_kg.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now map predicate identifiers to human-readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ety_df = pd.DataFrame(set(merged_ontology_kg[1]),columns=[\"name\"])\n",
    "ety_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split camel case strings, e.g., \"overexpressedIn\" --> \"overexpressed in\"\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "# automatically match all OBO items for the specified ontologies\n",
    "hdr = {'Accept': 'application/json'}\n",
    "ontos = [\"ro\",\"vo\",\"clo\",\"mondo\",\"ogg\",\"cl\",\"mf\",\"bspo\"]\n",
    "tomatch = \"http://purl.obolibrary.org/obo/\"\n",
    "\n",
    "def uri2etype(uri: str)->Union[str,None]:\n",
    "    label = None\n",
    "    for oy in ontos:\n",
    "        baseuri = f\"https://www.ebi.ac.uk/ols4/api/ontologies/{oy}/properties?iri={uri[1:-1]}\"\n",
    "        try:\n",
    "            res = requests.get(baseuri,hdr).json()\n",
    "            label=res['_embedded']['properties'][0]['label']\n",
    "            label=label\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #if (\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" in uri):   \n",
    "     #   label = \"Type\"  \n",
    "    #el\n",
    "    if (\"http://www.w3.org/2000/01/rdf-schema#subClassOf\" in uri):   \n",
    "        label = \"SubClassOf\" \n",
    "    # new manually set edges by splitting over the # symbol of the uri\n",
    "    elif (\"http://semanticscience.org/resource/SIO_000420\" in uri):   \n",
    "        label = \"Has expression\"\n",
    "    elif (\"http://purl.obolibrary.org/obo/CLO_0054408\" in uri):   \n",
    "        label = \"Overexpresses gene\"\n",
    "    elif (\"http://purl.obolibrary.org/obo/ddanat#part_of\" in uri):   \n",
    "        label = \"Part of\"\n",
    "    elif (\"http://purl.obolibrary.org/obo/ddanat#develops_from\" in uri):   \n",
    "        label = \"Develops from\"\n",
    "    elif (\"http://purl.obolibrary.org/obo/activates\" in uri):   \n",
    "        label = \"Activates\"\n",
    "    elif (\"http://purl.obolibrary.org/obo/GOREL_0012006\" in uri):   \n",
    "        label = \"Results in maintenance of\"\n",
    "    elif (\"http://purl.obolibrary.org/obo/GOREL_0000040\" in uri):   \n",
    "        label = \"Results in\"\n",
    "    elif (\"http://purl.obolibrary.org/obo/CLO_0054409\" in uri):   \n",
    "        label = \"Adenoma formation induced by cell lineage cells in mice\"\n",
    "    elif (\"http://purl.obolibrary.org/obo/uberon/core\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/mondo\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/so\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/envo\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/pr\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/pato\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/pw\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/exo.obo\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/cl\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/nbo\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/MF\" in uri) or \\\n",
    "         (\"http://www.obofoundry.org/ro\" in uri) or \\\n",
    "         (\"http://purl.obolibrary.org/obo/chebi\" in uri):\n",
    "            label = uri[1:-1].split(\"#\")[1]\n",
    "            label = '_'.join(camel_case_split(label))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return label.replace(' ','_').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etypes_list = []\n",
    "for u in tqdm(ety_df[\"name\"].values):\n",
    "    ety = uri2etype(u)\n",
    "    etypes_list.append(ety)\n",
    "\n",
    "ety_df.loc[:,\"type\"] = etypes_list\n",
    "print(\"Unassigned edge types:\",ety_df.type.isna().sum())\n",
    "ety_df.to_csv('../ety.csv',index=False)\n",
    "#ety_df = pd.read_csv('../ety.csv')\n",
    "ety_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix edge type semantics according to RO and ensure every inverse relationship is properly added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_graph = Graph().parse(ontology_data_location + 'ro_with_imports.owl')\n",
    "\n",
    "with open(relations_data_location + 'INVERSE_RELATIONS.txt', 'w') as outfile:\n",
    "    outfile.write('Relation' + '\\t' + 'Inverse_Relation' + '\\n')\n",
    "    for s, p, o in tqdm(ro_graph):\n",
    "        if 'owl#inverseOf' in str(p):\n",
    "            if 'RO' in str(s) and 'RO' in str(o):\n",
    "                outfile.write(str(s.split('/')[-1]) + '\\t' + str(o.split('/')[-1]) + '\\n')\n",
    "                outfile.write(str(o.split('/')[-1]) + '\\t' + str(s.split('/')[-1]) + '\\n')\n",
    "\n",
    "# Fix RO_0002529's inverseOf issue (RO_0002529 is wrongly stored as inverseOf RO_0002529 instead of RO_0002528)\n",
    "ro_data = pd.read_csv(relations_data_location + 'INVERSE_RELATIONS.txt', header=0, delimiter='\\t').drop_duplicates()\n",
    "ro_data.loc[ro_data['Relation'] == 'RO_0002529', 'Inverse_Relation'] = 'RO_0002528'\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['RO_0002528'], 'Inverse_Relation': ['RO_0002529']})], ignore_index=True)\n",
    "\n",
    "# We also specify symmetric relations (e.g., RO_0002434 inverseOf RO_0002434)\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['RO_0002434'], 'Inverse_Relation': 'RO_0002434'})], ignore_index=True)\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['RO_HOM0000016'], 'Inverse_Relation': 'RO_HOM0000016'})], ignore_index=True)\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['RO_0002436'], 'Inverse_Relation': ['RO_0002436']})], ignore_index=True)\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['RO_HOM0000000'], 'Inverse_Relation': ['RO_HOM0000000']})], ignore_index=True)\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['RO_0002526'], 'Inverse_Relation': ['RO_0002526']})], ignore_index=True)\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['RO_0002325'], 'Inverse_Relation': ['RO_0002325']})], ignore_index=True)\n",
    "# We also specify 'part of' as inverse of 'has part'\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['BFO_0000050'], 'Inverse_Relation': ['BFO_0000051']})], ignore_index=True)\n",
    "ro_data = pd.concat([ro_data, pd.DataFrame({'Relation': ['BFO_0000051'], 'Inverse_Relation': ['BFO_0000050']})], ignore_index=True)\n",
    "ro_data.to_csv(relations_data_location + 'INVERSE_RELATIONS.txt', sep='\\t', index=False)\n",
    "\n",
    "print(ro_data.head(n=3))\n",
    "\n",
    "results = {str(x[2]).lower(): str(x[0]) for x in ro_graph if '/RO_' in str(x[0]) and 'label' in str(x[1]).lower()}\n",
    "\n",
    "with open(relations_data_location + 'RELATIONS_LABELS.txt', 'w') as outfile:\n",
    "    outfile.write('Label' + '\\t' + 'Relation' + '\\n')\n",
    "    for k, v in results.items():\n",
    "        outfile.write(str(v).split('/')[-1] + '\\t' + str(k) + '\\n')\n",
    "\n",
    "ro_data_label = pd.read_csv(relations_data_location + 'RELATIONS_LABELS.txt', header=0, delimiter='\\t')\n",
    "# We also add the 'part of' relation (BFO_0000050) and its inverse 'has part' (BFO_0000051)\n",
    "ro_data_label = pd.concat([ro_data_label, pd.DataFrame({'Label': ['BFO_0000050'], 'Relation': ['part of']})], ignore_index=True)\n",
    "ro_data_label = pd.concat([ro_data_label, pd.DataFrame({'Label': ['BFO_0000051'], 'Relation': ['has part']})], ignore_index=True)\n",
    "ro_data_label['Relation'] = ro_data_label['Relation'].str.replace(\"is substitutent group from\",\"is substituent group from\")\n",
    "ro_data_label['Relation'] = ro_data_label['Relation'].str.lower().str.replace(\" \", \"_\")\n",
    "ro_data_label.to_csv(relations_data_location + 'RELATIONS_LABELS.txt', sep='\\t', index=False)\n",
    "\n",
    "print(ro_data_label.head(n=3))\n",
    "ro_data_inverse = ro_data.merge(ro_data_label, left_on='Relation', right_on='Label')\n",
    "ro_data_inverse = ro_data_inverse.merge(ro_data_label, left_on='Inverse_Relation', right_on='Label')[\n",
    "    ['Relation', 'Relation_y']].rename(columns={'Relation_y': 'Inverse_Relation'})\n",
    "ro_data_inverse.head(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ety_df['type'] = ety_df['type'].replace('disease_arises_from_feature','disease_has_basis_in_feature')\n",
    "ety_df['type'] = ety_df['type'].replace('non_functional_homolog_of','in_non_functional_homology_relationship_with')\n",
    "ety_df['type'] = ety_df['type'].replace('has_gene_template','gene_product_of')\n",
    "ety_df['type'] = ety_df['type'].replace('variant_of','evolutionary_variant_of')\n",
    "ety_df['type'] = ety_df['type'].replace('in_response_to','realized_in_response_to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ontology_kg = pd.merge(merged_ontology_kg, ety_df, left_on=1, right_on='name').drop(columns=['name',1], axis=1)\n",
    "merged_ontology_kg.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ontology_kg_inverse = merged_ontology_kg.rename(columns={2: 0, 0: 2})\n",
    "merged_ontology_kg_inverse = merged_ontology_kg_inverse.merge(\n",
    "    ro_data_inverse, left_on='type',right_on='Relation', how='left').drop(columns=['type']).rename(columns={'Inverse_Relation': 'type'})\n",
    "merged_ontology_kg = pd.concat([merged_ontology_kg, merged_ontology_kg_inverse]).drop_duplicates()\n",
    "merged_ontology_kg = merged_ontology_kg.groupby([0, 'type', 2]).agg({'Source': set}).reset_index()\n",
    "merged_ontology_kg.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ontology_kg[0] = merged_ontology_kg[0].apply(lambda x: x[1:-1] if x.startswith('<') and x.endswith('>') else x)\n",
    "merged_ontology_kg[2] = merged_ontology_kg[2].apply(lambda x: x[1:-1] if x.startswith('<') and x.endswith('>') else x)\n",
    "\n",
    "# Remove OWLNETS if it is not the only source\n",
    "merged_ontology_kg['Source'] = merged_ontology_kg['Source'].apply(\n",
    "    lambda source_set: source_set - {'OWLNETS'} if (len(source_set) > 1 and 'OWLNETS' in source_set) else source_set\n",
    ")\n",
    "\n",
    "merged_ontology_kg['Source'] = merged_ontology_kg['Source'].apply(lambda x: list(eval(x)))\n",
    "merged_ontology_kg.to_csv(ontology_data_location + 'merged_ontology_kg.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "```\n",
    "@misc{callahan_tj_2019_3401437,\n",
    "  author       = {Callahan, TJ},\n",
    "  title        = {PheKnowLator},\n",
    "  month        = mar,\n",
    "  year         = 2019,\n",
    "  doi          = {10.5281/zenodo.3401437},\n",
    "  url          = {https://doi.org/10.5281/zenodo.3401437}\n",
    "}\n",
    "```\n",
    "```\n",
    "@misc{cavalleri_e_2024_rna_kg,\n",
    "  author       = {Cavalleri, E},\n",
    "  title        = {RNA-KG},\n",
    "  year         = 2024,\n",
    "  doi          = {10.5281/zenodo.10078876},\n",
    "  url          = {https://doi.org/10.5281/zenodo.10078876}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
