{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f86aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import datetime\n",
    "import glob\n",
    "import itertools\n",
    "import networkx\n",
    "import numpy\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import gffpandas.gffpandas as gffpd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from reactome2py import content\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "from pkt_kg.utils import * \n",
    "from builds.ontology_cleaning import *\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6aa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store resources\n",
    "resource_data_location = '../resources/'\n",
    "\n",
    "# directory to use for unprocessed data\n",
    "unprocessed_data_location = '../resources/processed_data/unprocessed_data/'\n",
    "\n",
    "# directory to use for processed data\n",
    "processed_data_location = '../resources/processed_data/'\n",
    "\n",
    "# directory to write ontology data to\n",
    "ontology_data_location = '../resources/ontologies/'\n",
    "\n",
    "# directory to write edges data to\n",
    "edge_data_location = '../resources/edge_data/'\n",
    "\n",
    "# directory to write node properties to\n",
    "properties_location = '../resources/properties_data/'\n",
    "\n",
    "# processed data url \n",
    "processed_url = 'https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/'\n",
    "\n",
    "# original data url \n",
    "original_url = 'https://storage.googleapis.com/pheknowlator/current_build/data/original_data/'\n",
    "\n",
    "# owltools location\n",
    "owltools_location = '../pkt_kg/libs/owltools'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec013b6",
   "metadata": {},
   "source": [
    "# pre-miRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "data_downloader('https://www.mirbase.org/download/miRNA.dat', processed_data_location)\n",
    "\n",
    "# Open the EMBL file\n",
    "embl_file = processed_data_location + 'miRNA.dat'\n",
    "\n",
    "# Create empty lists to store the data\n",
    "data = {\n",
    "    \"ID\": [],\n",
    "    \"Description\": [],\n",
    "    \"Sequence\": [],\n",
    "    \"Comments\": [],\n",
    "    \"References\": [],\n",
    "    \"Feature Table\": []\n",
    "}\n",
    "\n",
    "# Iterate through the records in the EMBL file\n",
    "for record in SeqIO.parse(embl_file, \"embl\"):\n",
    "    data[\"ID\"].append(record.id)\n",
    "    data[\"Description\"].append(record.description)\n",
    "    data[\"Sequence\"].append(str(record.seq))\n",
    "    data[\"Comments\"].append(str(record.annotations.get('comment', '')))\n",
    "    references = []\n",
    "    i = 0\n",
    "    for ref in record.annotations.get('references', []):\n",
    "        i = i + 1\n",
    "        references.append(f\"{[i], ref.pubmed_id}\")\n",
    "    data[\"References\"].append(\", \".join(references))\n",
    "    feature_table = \"\\n\".join(str(feature) for feature in record.features)\n",
    "    data[\"Feature Table\"].append(feature_table)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[df['Description'].astype(str).str.contains('Homo sapiens')]\n",
    "\n",
    "df['Feature Table'] = df['Feature Table'].str.split(\"type: miRNA\")\n",
    "df = df.explode('Feature Table')\n",
    "df = df[df['Feature Table'] != '']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6326f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Feature Table'] = df['Feature Table'].str.split(\"\\n\")\n",
    "list(df['Feature Table'].loc[57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(row):\n",
    "    result = {}\n",
    "    for item in row:\n",
    "        if \"location: \" in item:\n",
    "            key_value = item.split(\"location: \")\n",
    "            value = key_value[1]\n",
    "            result['location'] = value\n",
    "        elif \"Key: \" in item:\n",
    "            key_value = item.split(\"Key: \")\n",
    "            key = key_value[1].split(\", Value:\")[0].strip()\n",
    "            value = key_value[1].split(\", Value:\")[1].strip(\" ['\").strip(\"'']\")\n",
    "            result[key] = value\n",
    "    return pd.Series(result)\n",
    "\n",
    "# Apply the function to create new columns\n",
    "new_columns = df['Feature Table'].apply(extract_values)\n",
    "\n",
    "# Concatenate the new columns with the original DataFrame\n",
    "df = pd.concat([df, new_columns], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "premirna = df[['ID', 'Description', 'Sequence', 'Comments', 'References', 'mod_base']]\n",
    "premirna = premirna.rename(columns={'mod_base':'Modification'})\n",
    "premirna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "miRBaseMap = pd.read_csv(processed_data_location + 'MIRNA_MIRBASE_MAP.txt', header=None, sep='\\t')\n",
    "miRBaseMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28234a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "premirna = pd.merge(df, miRBaseMap, left_on=['ID'], right_on=[1])\n",
    "premirna['Label'] = premirna[0]\n",
    "premirna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "miRNA_variant = pd.read_csv(unprocessed_data_location + \"miRNet-snp-mir-hsa.csv?dl=0\")\n",
    "miRNA_variant = miRNA_variant[miRNA_variant['High_Confidence']=='YES']\n",
    "miRNA_variant = miRNA_variant[['MIRNA_Name','Family_Name']]\n",
    "miRNA_variant = pd.merge(miRNA_variant, miRBaseMap, left_on=['MIRNA_Name'], right_on=[0]).drop(columns=['MIRNA_Name',0])\n",
    "\n",
    "miRNA_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12abab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "premirna = pd.merge(premirna, miRNA_variant, left_on=['ID'], right_on=[1], how='outer').rename(columns={'Family_Name':'Family name'})\n",
    "premirna[['ID','Label','Description','Sequence','Family name','Comments','References']].drop_duplicates().to_csv(properties_location + 'premiRNA.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "premirna[['ID','Label','Description','Sequence','Family name','Comments','References']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62c6c5",
   "metadata": {},
   "source": [
    "***\n",
    "# miRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna = df.drop(columns=['ID']).rename(columns={'accession':'ID',\n",
    "                                                'location':'Location',\n",
    "                                                'evidence':'Evidence',\n",
    "                                                'experiment':'Experiment',\n",
    "                                                'product':'Label'})\n",
    "mirna['Experiment'] = mirna['Experiment'] + ']'\n",
    "mirna.evidence = mirna.evidence.replace('experimental',\n",
    "                                        'http://purl.obolibrary.org/obo/NCIT_C43622 (experimental method)')\n",
    "mirna = mirna[['ID','Label','References','Location','Evidence','Experiment']]\n",
    "mirna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna.drop_duplicates().to_csv(properties_location + 'miRNA.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef64b6",
   "metadata": {},
   "source": [
    "***\n",
    "# tsRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd769a37",
   "metadata": {},
   "source": [
    "## tsRFun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa872aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsRNA = pd.read_csv(unprocessed_data_location + 'newID_20210202.txt', sep=\"\\t\")  \n",
    "tsRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db9fc4",
   "metadata": {},
   "source": [
    "***\n",
    "# tRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457a07d",
   "metadata": {},
   "source": [
    "## tRFdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a49c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://genome.bioch.virginia.edu/trfdb/index.php\n",
    "tRF1_tRNA = pd.read_html(unprocessed_data_location+'trf1.html')[2]\n",
    "tRF1_tRNA.drop(columns=['Organism'],inplace=True)\n",
    "tRF1_tRNA.head()\n",
    "\n",
    "tRF3_tRNA = pd.read_html(unprocessed_data_location+'trf3.html')[2]\n",
    "tRF3_tRNA.drop(columns=['Organism'],inplace=True)\n",
    "\n",
    "tRF5_tRNA = pd.read_html(unprocessed_data_location+'trf5.html')[2]\n",
    "tRF5_tRNA.drop(columns=['Organism'],inplace=True)\n",
    "\n",
    "tRF_tRNA = pd.concat([tRF1_tRNA,tRF3_tRNA,tRF5_tRNA])\n",
    "tRF_tRNA = tRF_tRNA.drop(columns=['Experiment Info', 'Sequence'])\n",
    "tRF_tRNA['tRF ID'] = tRF_tRNA['tRF ID'].astype(str)\n",
    "tRF_tRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_numbers(identifier):\n",
    "    \n",
    "    html_file_path = unprocessed_data_location + 'trf' + identifier + '.html'\n",
    "\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as html_file:\n",
    "        html_content = html_file.read()\n",
    "\n",
    "    pattern = r'href=\\'sequence_display.php\\?seq_id=(\\d+)'\n",
    "    matches = re.findall(pattern, html_content)\n",
    "    numbers = [int(match) for match in matches]\n",
    "\n",
    "    pattern2 = r\"href='experiments_display.php\\?trf_id=(.*?)'\"\n",
    "    matches2 = re.findall(pattern2, html_content)\n",
    "    \n",
    "    # Return the numbers as a dictionary\n",
    "    return {'sequence_numbers': numbers, 'experiment_numbers': matches2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(original_html):\n",
    "\n",
    "    transformed_html = re.sub(r'<font face=', '\\n<font face=', original_html)\n",
    "    transformed_html = re.sub(r'<br><b>Organism:', \"</font><br>\\n<font face='Arial' size='2'><b>Organism:\", transformed_html)\n",
    "    transformed_html = re.sub(r'<br><b>tRF Sequence:', \"</font><br>\\n<font face='Arial' size='2'><b>tRF Sequence:\", transformed_html)\n",
    "    transformed_html = re.sub(r\"<font face='Courier' size='3'>\", \"</font><br>\\n<font face='Arial' size='2'>\", transformed_html)\n",
    "    transformed_html = re.sub(r\"<br><b>Map Position:\", \"\\n<font face='Arial' size='2'><b>Map Position:\", transformed_html)\n",
    "\n",
    "    return transformed_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9cd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_html(identifier):\n",
    "    url = 'http://genome.bioch.virginia.edu/trfdb/sequence_display.php?seq_id=' + identifier\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 500:\n",
    "        html_content = response.text\n",
    "        return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b143d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "df = pd.DataFrame()\n",
    "result = get_numbers('1')\n",
    "numbers_mapping = dict(zip(result['sequence_numbers'], result['experiment_numbers']))\n",
    "\n",
    "for i in result['sequence_numbers'] :\n",
    "    \n",
    "    html_content = get_html(str(i))  # Retrieve HTML content\n",
    "    if html_content is not None:\n",
    "        # Apply the transformation to the HTML content\n",
    "        transformed_html = transform(html_content)\n",
    "\n",
    "        # Continue with parsing and DataFrame creation\n",
    "        soup = BeautifulSoup(transformed_html, 'html.parser')\n",
    "        values = [font.get_text() for font in soup.find_all('font')]\n",
    "        values = [value.split(\":\")[1].strip() if \":\" in value else value for value in values]\n",
    "        \n",
    "        corresponding_experiment_number = numbers_mapping.get(i, None)\n",
    "\n",
    "        # Create a DataFrame for the current HTML page\n",
    "        temp = pd.DataFrame(values).T\n",
    "        temp.columns = range(temp.shape[1])\n",
    "\n",
    "        # Add the 'Experiment Number' column\n",
    "        temp['Experiment Number'] = corresponding_experiment_number\n",
    "\n",
    "        # Concatenate the current DataFrame with the main DataFrame\n",
    "        df = pd.concat([df, temp], ignore_index=True)\n",
    " \n",
    "result = get_numbers('3')\n",
    "numbers_mapping = dict(zip(result['sequence_numbers'], result['experiment_numbers']))\n",
    "\n",
    "for i in result['sequence_numbers'] :\n",
    "    \n",
    "    html_content = get_html(str(i))  # Retrieve HTML content\n",
    "    if html_content is not None:\n",
    "        # Apply the transformation to the HTML content\n",
    "        transformed_html = transform(html_content)\n",
    "\n",
    "        # Continue with parsing and DataFrame creation\n",
    "        soup = BeautifulSoup(transformed_html, 'html.parser')\n",
    "        values = [font.get_text() for font in soup.find_all('font')]\n",
    "        values = [value.split(\":\")[1].strip() if \":\" in value else value for value in values]\n",
    "        \n",
    "        corresponding_experiment_number = numbers_mapping.get(i, None)\n",
    "\n",
    "        # Create a DataFrame for the current HTML page\n",
    "        temp = pd.DataFrame(values).T\n",
    "        temp.columns = range(temp.shape[1])\n",
    "\n",
    "        # Add the 'Experiment Number' column\n",
    "        temp['Experiment Number'] = corresponding_experiment_number\n",
    "\n",
    "        # Concatenate the current DataFrame with the main DataFrame\n",
    "        df = pd.concat([df, temp], ignore_index=True)\n",
    "\n",
    "result = get_numbers('5')\n",
    "numbers_mapping = dict(zip(result['sequence_numbers'], result['experiment_numbers']))\n",
    "\n",
    "for i in result['sequence_numbers'] :\n",
    "    \n",
    "    html_content = get_html(str(i))  # Retrieve HTML content\n",
    "    if html_content is not None:\n",
    "        # Apply the transformation to the HTML content\n",
    "        transformed_html = transform(html_content)\n",
    "\n",
    "        # Continue with parsing and DataFrame creation\n",
    "        soup = BeautifulSoup(transformed_html, 'html.parser')\n",
    "        values = [font.get_text() for font in soup.find_all('font')]\n",
    "        values = [value.split(\":\")[1].strip() if \":\" in value else value for value in values]\n",
    "        \n",
    "        corresponding_experiment_number = numbers_mapping.get(i, None)\n",
    "\n",
    "        # Create a DataFrame for the current HTML page\n",
    "        temp = pd.DataFrame(values).T\n",
    "        temp.columns = range(temp.shape[1])\n",
    "\n",
    "        # Add the 'Experiment Number' column\n",
    "        temp['Experiment Number'] = corresponding_experiment_number\n",
    "\n",
    "        # Concatenate the current DataFrame with the main DataFrame\n",
    "        df = pd.concat([df, temp], ignore_index=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e2aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chr_substring(text):\n",
    "    start_index = text.find('chr')\n",
    "    if start_index != -1:\n",
    "        end_index = text.find('&', start_index)\n",
    "        if end_index != -1:\n",
    "            return text[start_index:end_index]\n",
    "    return ''\n",
    "\n",
    "df['Experiment Number'] = df['Experiment Number'].apply(extract_chr_substring)\n",
    "df.columns = ['tRF ID','organism','empty','Sequence','Map Position','tRNA Gene Co-ordinates']\n",
    "df = df.drop(columns=['organism','empty'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRF = pd.merge(tRF_tRNA,df,on=['tRF ID', 'tRNA Gene Co-ordinates'])\n",
    "tRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRF.drop_duplicates().to_csv(properties_location + 'tRF_tRFdb.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7c71a",
   "metadata": {},
   "source": [
    "## MINTBASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74308fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRNA_MINTbase_GtRNAdb_map=pd.read_csv(\n",
    "    processed_data_location + 'tRNA_MINTbase_GtRNAdb_MAP.txt', header=None, sep='\\t')\n",
    "tRNA_MINTbase_GtRNAdb_map=tRNA_MINTbase_GtRNAdb_map.rename(columns={0:'MINTbase tRNA name',1:'gtRNAdb name'})\n",
    "tRNA_MINTbase_GtRNAdb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cm.jefferson.edu/MINTbase/InputController?g=GRCh37&d=y&v=g&e=1.0&cl=,4,5,11,12,16,18,19,21,22,26,27,#ttop\n",
    "tRF_tRNA2 = pd.read_csv(unprocessed_data_location+'MINTbasetRF-tRNA.txt',sep='\\t')\n",
    "tRF_tRNA2['MINTbase Alternative IDs (GRCh37 assembly-derived)'] = tRF_tRNA2['MINTbase Alternative IDs (GRCh37 assembly-derived)'].str.split('@').str[0]\n",
    "tRF_tRNA2.rename(columns={'MINTbase Alternative IDs (GRCh37 assembly-derived)':'MINTbase tRNA name'},inplace=True)\n",
    "tRF_tRNA2 = pd.merge(tRF_tRNA2, tRNA_MINTbase_GtRNAdb_map, on='MINTbase tRNA name')\n",
    "tRF_tRNA2 = tRF_tRNA2[['Type','License Plate (sequence derived)','Fragment sequence','gtRNAdb name','MINTbase tRNA name']]\n",
    "tRF_tRNA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfb546",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRF_tRNA2.drop_duplicates().to_csv(properties_location + 'tRF_MINTBASE.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206daf6",
   "metadata": {},
   "source": [
    "***\n",
    "# tRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a66b76",
   "metadata": {},
   "source": [
    "## GtRNAdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c68d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/hg38-tRNAs.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "\n",
    "identifiers = []\n",
    "seq = []\n",
    "\n",
    "# Replace the URL with the path to your local FASTA file\n",
    "fasta_file_path = unprocessed_data_location + 'hg38-tRNAs.fa'\n",
    "\n",
    "with open(fasta_file_path) as fasta_file:\n",
    "    for title, sequence in SimpleFastaParser(fasta_file):\n",
    "        identifiers.append(title.split(None, 1)[0])  # First word is ID\n",
    "        seq.append(sequence)\n",
    "        \n",
    "data = {\"Identifier\": identifiers, \"Sequence\": seq}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8870a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all(df['Identifier'].str.startswith('Homo_sapiens_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Identifier'] = df['Identifier'].str[len('Homo_sapiens_'):]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl = \"tRNA.pkl\"\n",
    "#tRNA.to_pickle(pkl)\n",
    "tRNA = pd.read_pickle(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f2f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tRNA = pd.read_html('http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/genes/tRNA-Ala-AGC-1-1.html')[0].T\n",
    "tRNA2 = pd.read_html('http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/genes/tRNA-Ala-AGC-1-1.html')[1].T\n",
    "tRNA = pd.concat([tRNA,tRNA2],axis=1)\n",
    "tRNA.columns = tRNA.iloc[0]\n",
    "tRNA = tRNA[1:]\n",
    "tRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for identifier in df['Identifier'] [1:] :\n",
    "\n",
    "    temp = pd.read_html('http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/genes/' + identifier + '.html')[0].T\n",
    "    temp2 = pd.read_html('http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/genes/' + identifier + '.html')[1].T\n",
    "    temp = pd.concat([temp,temp2],axis=1)\n",
    "    temp.columns = temp.iloc[0]\n",
    "    temp = temp[1:]\n",
    "    tRNA = pd.concat([tRNA, temp])\n",
    "\n",
    "tRNA.Locus = tRNA.Locus.str.replace(' View in Genome Browser', '')\n",
    "tRNA = tRNA.drop(columns=['Organism', 'Known Modifications (Modomics)'])\n",
    "tRNA\n",
    "tRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80680c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRNA.drop_duplicates().to_csv(properties_location + 'tRNA_GtRNAdb.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46ce14",
   "metadata": {},
   "source": [
    "## tRNAdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e72b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://trna.bioinf.uni-leipzig.de/DataOutput/Result\n",
    "tRNA_aa = pd.read_html(unprocessed_data_location+'tRNAdb - Transfer RNA database.html')[3]\n",
    "tRNA_aa.drop(columns=[0,1,2,4,19,20],inplace=True)\n",
    "tRNA_aa.rename(columns=tRNA_aa.iloc[0], inplace=True)\n",
    "tRNA_aa = tRNA_aa.iloc[2:]\n",
    "tRNA_aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9750a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRNA_aa.drop_duplicates().to_csv(properties_location + 'tRNA_tRNAdb.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f460815",
   "metadata": {},
   "source": [
    "## tRFdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tRF1_tRNA = pd.read_html(unprocessed_data_location+'trf1.html')[2]\n",
    "#tRF3_tRNA = pd.read_html(unprocessed_data_location+'trf3.html')[2]\n",
    "#tRF5_tRNA = pd.read_html(unprocessed_data_location+'trf5.html')[2]\n",
    "tRF_tRNA = tRF_tRNA[['tRNA Gene Co-ordinates','tRNA Name']]\n",
    "#tRF_tRNA = pd.concat([tRF1_tRNA,tRF3_tRNA,tRF5_tRNA])\n",
    "tRF_tRNA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRF_tRNA.drop_duplicates().to_csv(properties_location + 'tRNA_tRFdb.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190e451",
   "metadata": {},
   "source": [
    "## MINTBASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cm.jefferson.edu/MINTbase/InputController?g=GRCh37&d=y&v=g&e=1.0&cl=,4,5,11,12,16,18,19,21,22,26,27,#ttop\n",
    "tRF_tRNA2 = pd.read_csv(unprocessed_data_location+'MINTbasetRF-tRNA.txt',sep='\\t')\n",
    "tRF_tRNA2['MINTbase Alternative IDs (GRCh37 assembly-derived)'] = tRF_tRNA2['MINTbase Alternative IDs (GRCh37 assembly-derived)'].str.split('@').str[0]\n",
    "tRF_tRNA2.rename(columns={'MINTbase Alternative IDs (GRCh37 assembly-derived)':'MINTbase tRNA name'},inplace=True)\n",
    "tRF_tRNA2 = pd.merge(tRF_tRNA2, tRNA_MINTbase_GtRNAdb_map, on='MINTbase tRNA name')\n",
    "tRF_tRNA2.drop(columns=['MINTbase tRNA name'],inplace=True)\n",
    "tRF_tRNA2 = tRF_tRNA2[['tRNA number','Amino acid and anticodon','Chromosome','Chromosome strand','Chromosome start position','Chromosome end position','Start position relative to start of mature tRNA','End position relative to start of mature tRNA','# Distinct anticodons','# Instances in true MT tRNAs','# Instances in tRNA lookalikes in nucleus','D-loop overlap?','Anticodon-loop overlap?','Anticodon-triplet overlap?','T-loop overlap?','Exclusively within tRNA genes?','gtRNAdb name']]\n",
    "tRF_tRNA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78277e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tRF_tRNA2.drop_duplicates().to_csv(properties_location + 'tRNA_MINTBASE.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b71c7",
   "metadata": {},
   "source": [
    "***\n",
    "# Small protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e231cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncRNA_protein = pd.read_csv(unprocessed_data_location + 'sprotein_LncBook2.0.csv.gz') \n",
    "lncRNA_protein = lncRNA_protein[lncRNA_protein['Symbol']!='-']\n",
    "lncRNA_protein.drop(columns=['Gene ID','Symbol','Transcript ID','Experimental Evidence'],inplace=True)\n",
    "lncRNA_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de80508",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncRNA_protein.drop_duplicates().to_csv(properties_location + 'smallProtein.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669a374",
   "metadata": {},
   "source": [
    "***\n",
    "# Riboswitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5fcf6c",
   "metadata": {},
   "source": [
    "## TBDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "riboswitch_protein = pd.read_csv(unprocessed_data_location+'tbdb.csv', sep=',') \n",
    "riboswitch_protein = riboswitch_protein[[\n",
    "    'unique_name', 'Name', 'Sequence', 'Tbox_start' , 'Tbox_end', 'Structure', 's1_start', 's1_loop_start',\n",
    "    's1_loop_end', 's1_end', 'antiterm_start', 'antiterm_end', 'term_start', 'term_end', 'codon_start',\n",
    "    'codon_end', 'codon', 'codon_region', 'discrim_start', 'discrim_end', 'discriminator', 'warnings',\n",
    "    'type', 'source', 'whole_antiterm_structure', 'other_stems', 'whole_antiterm_warnings', 'term_sequence',\n",
    "    'term_structure', 'terminator_energy', 'term_errors', 'antiterm_term_sequence',\n",
    "    'infernal_antiterminator_structure', 'vienna_antiterminator_structure', 'vienna_antiterminator_energy',\n",
    "    'vienna_antiterminator_errors', 'terminator_structure', 'terminator_errors', 'new_term_structure',\n",
    "    'new_term_energy', 'new_term_errors', 'whole_term_structure', 'folded_antiterm_structure', 'Trimmed_sequence',\n",
    "    'Trimmed_antiterm_struct', 'Trimmed_term_struct', 'accession_url', 'accession_name', 'locus_start', \n",
    "    'locus_end', 'locus_view_start', 'locus_view_end', 'deltadelta_g', 'TaxId'\n",
    "]]\n",
    "riboswitch_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "riboswitch_protein.drop_duplicates().to_csv(properties_location + 'riboswitch_TBDB.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f08d7f",
   "metadata": {},
   "source": [
    "## RSwitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "riboswitch_bactStrain = pd.read_csv(unprocessed_data_location + 'rswitch.csv', header=None) \n",
    "riboswitch_bactStrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e781398",
   "metadata": {},
   "outputs": [],
   "source": [
    "riboswitch_bactStrain[[0,1]].drop_duplicates().to_csv(properties_location + 'riboswitch_RSwitch.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf950557",
   "metadata": {},
   "source": [
    "***\n",
    "# Viral RNA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96520f3",
   "metadata": {},
   "source": [
    "## ViroidDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vRNA_ribozyme = pd.read_json(unprocessed_data_location + 'all.json').T \n",
    "\n",
    "# Extract ribozymes \n",
    "myre = re.compile(r\"\\n>> .*?\\n\")\n",
    "ribozyme = [myre.findall(i) for i in vRNA_ribozyme.ribozymes]\n",
    "ribozyme = [[j.replace(\"\\n\",'').replace(\">> \",'') for j in i] for i in ribozyme]\n",
    "\n",
    "# List of all possible ribozymes (useful for mapping)\n",
    "a = [i for j in ribozyme for i in j]\n",
    "set(a)\n",
    "\n",
    "vRNA_ribozyme = pd.concat([vRNA_ribozyme.reset_index().drop(columns=['index']),\n",
    "                           pd.Series(ribozyme)], axis=1)\n",
    "vRNA_ribozyme = vRNA_ribozyme.explode(0)\n",
    "vRNA_ribozyme[0] = vRNA_ribozyme[0].str.split().str[0]\n",
    "vRNA_ribozyme=vRNA_ribozyme[['identicalSeqs', 'accession', 'submitters', 'releaseDate', 'isolate', 'species',\n",
    "                            'genus', 'family', 'moleculeType', 'sequenceType', 'nucCompleteness', 'genotype', 'segment',\n",
    "                            'publications', 'geoLocation', 'host', 'isolationSource', 'collectionDate', 'bioSample',\n",
    "                            'genBankTitle', 'displayTitle', 'sequence', 'structure', 'type', 'ribozymes',\n",
    "                            'Cls_ID80', 'Cls_ID70', 'Cls_ID85', 'Cls_ID75', 'Cls_ID95', 'Cls_ID90']]\n",
    "\n",
    "vRNA_ribozyme['identicalSeqs'] = vRNA_ribozyme['identicalSeqs'].astype(str)\n",
    "vRNA_ribozyme['structure'] = vRNA_ribozyme['structure'].astype(str)\n",
    "vRNA_ribozyme.insert(1,'accession',vRNA_ribozyme.pop('accession'))\n",
    "vRNA_ribozyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459cf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vRNA_ribozyme.drop_duplicates().to_csv(properties_location + 'viralRNA.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4540810",
   "metadata": {},
   "source": [
    "***\n",
    "# Aptamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99fc68",
   "metadata": {},
   "source": [
    "## Apta-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa81552",
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_protein = pd.read_csv(unprocessed_data_location + 'aptaindex.csv',names=['Name', 'ID', 'Target', 'Sequence'],skiprows=[0]) \n",
    "aptamer_protein.Target = aptamer_protein.Target.str.lower()\n",
    "aptamer_protein['ID'] = 'aptamer-details/?id=' + aptamer_protein['ID'].astype(str)\n",
    "aptamer_protein = aptamer_protein.drop(columns=['Target'])\n",
    "aptamer_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eace9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_protein.drop_duplicates().to_csv(properties_location + 'aptamer.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289babe6",
   "metadata": {},
   "source": [
    "***\n",
    "# Ribozyme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b58322",
   "metadata": {},
   "source": [
    "## Rfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "ribozyme_rfam_map = pd.DataFrame(data=[['LC ribozyme','family/RF00011'],\n",
    "                                 ['hammerhead ribozyme','clan/CL00010'],\n",
    "                                 ['glmS ribozyme','family/RF00234'],\n",
    "                                 ['HDV-F-prausnitzii','family/RF02682'],\n",
    "                                 ['HDV ribozyme','family/RF00094'],\n",
    "                                 ['HDV_ribozyme','family/RF00094'],\n",
    "                                 ['Hairpin','family/RF00173'],\n",
    "                                 ['Hammerhead_1','clan/CL00010'],\n",
    "                                 ['Hammerhead_HH9','clan/CL00010'],\n",
    "                                 ['Hammerhead_3','clan/CL00010'],\n",
    "                                 ['Hammerhead_HH10','clan/CL00010'],\n",
    "                                 ['Hammerhead_II','clan/CL00010'],\n",
    "                                 ['Pistol','family/RF02679'],\n",
    "                                 ['Pistol ribozyme','family/RF02679'],\n",
    "                                 ['twister ribozyme','clan/CL00120'],\n",
    "                                 ['Twister-P5','clan/CL00120'],\n",
    "                                 ['Twister-P3','clan/CL00120'],\n",
    "                                 ['RNAse P','family/RF00009']#,\n",
    "                                 #['VS ribozyme',''] absent in RFAM\n",
    "                                 ])\n",
    "\n",
    "ribozyme_rfam_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "\n",
    "ribozyme_family = ribozyme_rfam_map[ribozyme_rfam_map[1].str.contains('family')]\n",
    "ribozyme_sequences = {}\n",
    "\n",
    "for ribozyme in ribozyme_family[1]:\n",
    "    url = 'http://rfamlive.xfam.org/' + ribozyme + '/alignment?acc=' + ribozyme.rsplit('/')[1] + '&format=fasta&download=1'\n",
    "    response = requests.get(url)\n",
    "    fasta_data = response.text\n",
    "    fasta_handle = StringIO(fasta_data)\n",
    "    sequences = list(SeqIO.parse(fasta_handle, 'fasta'))\n",
    "    ribozyme_sequences[ribozyme] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for ribozyme, seq_records in ribozyme_sequences.items():\n",
    "    sequences = [str(seq_record.seq) for seq_record in seq_records]\n",
    "    data.append({'ribozyme': ribozyme, 'sequence(s)': sequences})\n",
    "\n",
    "# Create a Pandas DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a087ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ribozyme_rfam_map = pd.merge(ribozyme_rfam_map,df,left_on=[1],right_on=['ribozyme'], how='outer').drop(columns=['ribozyme'])\n",
    "ribozyme_rfam_map['sequence(s)'] = ribozyme_rfam_map['sequence(s)'].apply(\n",
    "    lambda x: '; '.join(map(str, x)) if not isinstance(x, float) else '')\n",
    "ribozyme_rfam_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "ribozyme_rfam_map.drop_duplicates().to_csv(properties_location + 'ribozyme.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e78f85",
   "metadata": {},
   "source": [
    "***\n",
    "# Biological role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc1125",
   "metadata": {},
   "source": [
    "## dbEssLnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['Tumor-Suppressor-Gene', 'Oncogene', 'General']\n",
    "definition = ['A tumor suppressor gene encodes a protein that acts to regulate cell division, keeping it in check. When a tumor suppressor gene is inactivated by a mutation, the protein it encodes is not produced or does not function properly, and as a result, uncontrolled cell division may occur. Such mutations may contribute to the development of a cancer.',\n",
    "              'An oncogene is a mutated gene that has the potential to cause cancer. Before an oncogene becomes mutated, it is called a proto-oncogene, and it plays a role in regulating normal cell division. Cancer can arise when a proto-oncogene is mutated, changing it into an oncogene and causing the cell to divide and multiply uncontrollably. Some oncogenes work like an accelerator pedal in a car, pushing a cell to divide again and again. Others work like a faulty brake in a car parked on a hill, also causing the cell to divide unchecked.',\n",
    "              '']\n",
    "narration = ['Tumor Suppressor Gene. Tumor suppressor genes are present in all cells in our body. When they are switched on, they prevent ourselves from growing and dividing. You can think of them as being like the brakes of a car. However, when a tumor suppressor gene is switched off, either because the cell mistakenly deletes it or mutates it, the brake is released and the cell may start to grow and divide uncontrollably and potentially drive the cell to turn into a cancer cell.',\n",
    "             'Oncogene. The name of oncogene suggests it is a gene that can cause cancer. Initially, oncogenes were identified in viruses, which could cause cancers in animals. Later, it was found that oncogenes can be mutated copies of certain normal cellular genes also called proto-oncogenes. Intact proto-oncogenes play important functions, regulating normal cellular growth, division, and apoptosis, which is the name for programmed or controlled cell death. Oncogenes or mutated copies of the proto-oncogenes may lead to uncontrolled cell growth and the escape from cell death, which may result in cancer development.',\n",
    "             '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = pd.DataFrame({'Name': name, 'Definition': definition, 'Narration': narration})\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd82a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "role.drop_duplicates().to_csv(properties_location + 'biologicalRole.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75f021",
   "metadata": {},
   "source": [
    "***\n",
    "# piRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff5bfa",
   "metadata": {},
   "source": [
    "# RNAInter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45041be",
   "metadata": {},
   "outputs": [],
   "source": [
    "piRNA = pd.concat([pd.read_csv('../resources/edge_data/piRNA-mRNA.txt',sep='\\t',header=None)[0],\n",
    "    pd.read_csv('../resources/edge_data/piRNA-lncRNA.txt',sep='\\t',header=None)[0]]).drop_duplicates()\n",
    "piRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "piRNA = pd.read_html('http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-39980')[0][[0,1]].T\n",
    "piRNA.columns = piRNA.iloc[0]\n",
    "piRNA = piRNA[1:]\n",
    "piRNA['piRBase Id'] = 'piR-39980'\n",
    "piRNA.reset_index(drop=True, inplace=True)\n",
    "piRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b73c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_html('http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-39980')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_html('http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-39980')[2][['No.', 'Location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb922c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "piRNA = pd.concat([piRNA,\n",
    "           pd.read_html('http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-39980')[1],\n",
    "           pd.read_html('http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-39980')[2][['No.', 'Location']]\n",
    "          ], axis=1)\n",
    "\n",
    "piRNA.reset_index(drop=True, inplace=True)\n",
    "piRNA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5468aa4",
   "metadata": {},
   "source": [
    "https://www.ncbi.nlm.nih.gov/nucleotide/DQ590027 --> http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-57139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "piRNA1 = piRNA.copy()\n",
    "\n",
    "piRNA = pd.read_html('http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-57139')[0][[0,1]].T\n",
    "piRNA.columns = piRNA.iloc[0]\n",
    "piRNA = piRNA[1:]\n",
    "piRNA['piRBase Id'] = 'piR-DQ590027'\n",
    "piRNA.reset_index(drop=True, inplace=True)\n",
    "\n",
    "piRNA = pd.concat([piRNA,\n",
    "           pd.read_html('http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-57139')[1],\n",
    "           pd.read_html('http://bigdata.ibp.ac.cn/piRBase/pirna.php?name=piR-hsa-57139')[2][['No.', 'Location']]\n",
    "          ], axis=1)\n",
    "\n",
    "piRNA.reset_index(drop=True, inplace=True)\n",
    "\n",
    "piRNA = pd.concat([piRNA1,piRNA]).loc[0].drop(columns=['Length', 'Organism'])\n",
    "piRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44742c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "piRNA.drop_duplicates().to_csv(properties_location + 'piRNA.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc53831",
   "metadata": {},
   "source": [
    "***\n",
    "# RNA drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed4c89",
   "metadata": {},
   "source": [
    "## DrugBank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645ddd2",
   "metadata": {},
   "source": [
    "https://go.drugbank.com/releases/latest#open-data --> DrugBank Vocabulary --> Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrugBank = pd.read_csv(unprocessed_data_location + 'drugbank vocabulary.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASOdnonO_data = pd.concat([pd.read_csv('../resources/edge_data/RASOd-mRNA.txt',sep='\\t',header=None)[1],\n",
    "    pd.read_csv('../resources/edge_data/RASOd-disease.txt',sep='\\t',header=None)[0],\n",
    "    pd.read_csv('../resources/edge_data/RASOd-protein11007.txt',sep='\\t',header=None)[0],\n",
    "    pd.read_csv('../resources/edge_data/RASOd-protein10002.txt',sep='\\t',header=None).loc[0]])\n",
    "\n",
    "aptamerdnonO_data = pd.concat([pd.read_csv('../resources/edge_data/Raptamerd-protein.txt',sep='\\t',header=None)[0],\n",
    "    pd.read_csv('../resources/edge_data/Raptamerd-disease.txt',sep='\\t',header=None)[0]])\n",
    "\n",
    "siRNAdnonO_data = pd.concat([pd.read_csv('../resources/edge_data/RsiRNAd-mRNA.txt',sep='\\t',header=None)[0],\n",
    "    pd.read_csv('../resources/edge_data/RsiRNAd-disease.txt',sep='\\t',header=None)[0]])\n",
    "\n",
    "mRNAvnonO_data = pd.read_csv('../resources/edge_data/RmRNAv-disease.txt',sep='\\t',header=None)[0]\n",
    "\n",
    "RNAdrugs = pd.concat([ASOdnonO_data, aptamerdnonO_data, siRNAdnonO_data, mRNAvnonO_data]).drop_duplicates().reset_index(drop=True)\n",
    "RNAdrugs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27210c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNAdrugs = pd.merge(pd.DataFrame(RNAdrugs), DrugBank, left_on=[0], right_on=['DrugBank ID']).drop(columns=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNAdrugs.drop_duplicates().to_csv(properties_location + 'RNAdrugs.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b11fbf",
   "metadata": {},
   "source": [
    "***\n",
    "# Gene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fbe2e",
   "metadata": {},
   "source": [
    "## PheKnowLator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ac322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://zenodo.org/records/10056198/files/genomic_typing_dict.pkl.zip?download=1'\n",
    "#data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "genomic_type_mapper = pickle.load(open(unprocessed_data_location + 'genomic_typing_dict.pkl', 'rb'))\n",
    "genomic_type_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76270de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'http://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt'\n",
    "#data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "hgnc = pd.read_csv(unprocessed_data_location + 'hgnc_complete_set.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "hgnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb07849",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc = hgnc.loc[hgnc['status'].apply(lambda x: x == 'Approved')]\n",
    "hgnc = hgnc[['hgnc_id', 'entrez_id', 'ensembl_gene_id', 'uniprot_ids', 'symbol', 'locus_type', 'alias_symbol', 'name', 'location', 'alias_name']]\n",
    "hgnc.rename(columns={'uniprot_ids': 'uniprot_id', 'location': 'map_location', 'locus_type': 'hgnc_gene_type'}, inplace=True)\n",
    "hgnc['hgnc_id'].replace('.*\\:', '', inplace=True, regex=True)  # strip 'HGNC' off of the identifiers\n",
    "hgnc.fillna('None', inplace=True)  # replace NaN with 'None'\n",
    "hgnc['entrez_id'] = hgnc['entrez_id'].apply(lambda x: str(int(x)) if x != 'None' else 'None')  # make col str\n",
    "\n",
    "# combine certain columns into single column\n",
    "hgnc['name'] = hgnc['name'] + '|' + hgnc['alias_name']\n",
    "hgnc['synonyms'] = hgnc['alias_symbol'] + '|' + hgnc['alias_name'] + '|' + hgnc['name']\n",
    "hgnc['symbol'] = hgnc['symbol'] + '|' + hgnc['alias_symbol']\n",
    "\n",
    "# explode nested data and reformat values in preparation for combining it with other gene identifiers\n",
    "explode_df_hgnc = explodes_data(hgnc.copy(), ['ensembl_gene_id', 'uniprot_id', 'symbol', 'name', 'synonyms'], '|')\n",
    "\n",
    "# reformat hgnc gene type\n",
    "for val in genomic_type_mapper['hgnc_gene_type'].keys():\n",
    "    explode_df_hgnc['hgnc_gene_type'].replace(val, genomic_type_mapper['hgnc_gene_type'][val], inplace=True)\n",
    "\n",
    "# reformat master hgnc gene type\n",
    "explode_df_hgnc['master_gene_type'] = explode_df_hgnc['hgnc_gene_type']\n",
    "master_dict = genomic_type_mapper['hgnc_master_gene_type']\n",
    "for val in master_dict.keys():\n",
    "    explode_df_hgnc['master_gene_type'].replace(val, master_dict[val], inplace=True)\n",
    "\n",
    "# post-process reformatted data\n",
    "explode_df_hgnc.drop(['alias_symbol', 'alias_name'], axis=1, inplace=True)  # remove original gene type column\n",
    "explode_df_hgnc.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_hgnc.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e92f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'ftp://ftp.ensembl.org/pub/release-102/gtf/homo_sapiens/Homo_sapiens.GRCh38.102.gtf.gz'\n",
    "#data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "ensembl_geneset = pd.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.gtf',\n",
    "                                  header = None, delimiter='\\t', skiprows=5, usecols=[8], low_memory=False)\n",
    "ensembl_geneset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ed4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_data = list(ensembl_geneset[8]); ensembl_df_data = []\n",
    "for i in tqdm(range(0, len(ensembl_data))):\n",
    "    if 'gene_id' in ensembl_data[i] and 'transcript_id' in ensembl_data[i]:\n",
    "        row_dict = {x.split(' \"')[0].lstrip(): x.split(' \"')[1].strip('\"') for x in ensembl_data[i].split(';')[0:-1]}\n",
    "        ensembl_df_data += [(row_dict['gene_id'], row_dict['transcript_id'], row_dict['gene_name'],\n",
    "                           row_dict['gene_biotype'], row_dict['transcript_name'], row_dict['transcript_biotype'])]\n",
    "# convert to data frame\n",
    "ensembl_geneset = pd.DataFrame(ensembl_df_data,\n",
    "                                   columns=['ensembl_gene_id', 'transcript_stable_id', 'symbol',\n",
    "                                            'ensembl_gene_type', 'transcript_name', 'ensembl_transcript_type'])\n",
    "\n",
    "# reformat ensembl gene type\n",
    "gene_dict = genomic_type_mapper['ensembl_gene_type']\n",
    "for val in gene_dict.keys(): ensembl_geneset['ensembl_gene_type'].replace(val, gene_dict[val], inplace=True)\n",
    "# reformat master gene type\n",
    "ensembl_geneset['master_gene_type'] = ensembl_geneset['ensembl_gene_type']\n",
    "gene_dict = genomic_type_mapper['ensembl_master_gene_type']\n",
    "for val in gene_dict.keys(): ensembl_geneset['master_gene_type'].replace(val, gene_dict[val], inplace=True)\n",
    "# reformat master transcript type\n",
    "ensembl_geneset['ensembl_transcript_type'].replace('vault_RNA', 'vaultRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'] = ensembl_geneset['ensembl_transcript_type']\n",
    "trans_dict = genomic_type_mapper['ensembl_master_transcript_type']\n",
    "for val in trans_dict.keys(): ensembl_geneset['master_transcript_type'].replace(val, trans_dict[val], inplace=True)\n",
    "\n",
    "# post-process reformatted data\n",
    "ensembl_geneset.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_geneset.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eec253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_uniprot = 'ftp://ftp.ensembl.org/pub/release-102/tsv/homo_sapiens/Homo_sapiens.GRCh38.102.uniprot.tsv.gz'\n",
    "#data_downloader(url_uniprot, unprocessed_data_location)\n",
    "\n",
    "ensembl_uniprot = pd.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.uniprot.tsv', header=0, delimiter='\\t', low_memory=False)\n",
    "ensembl_uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888150f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_uniprot.rename(columns={'xref': 'uniprot_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "ensembl_uniprot.replace('-', 'None', inplace=True)\n",
    "ensembl_uniprot.fillna('None', inplace=True)\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['xref_identity'].apply(lambda x: x != 'None')]\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['uniprot_id'].apply(lambda x: '-' not in x)]  # remove isoforms\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['info_type'].apply(lambda x: x == 'DIRECT')]\n",
    "# ensembl_uniprot['master_gene_type'] = ['protein-coding'] * len(ensembl_uniprot)\n",
    "# ensembl_uniprot['master_transcript_type'] = ['protein-coding'] * len(ensembl_uniprot)\n",
    "ensembl_uniprot.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "ensembl_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83813b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_entrez = 'ftp://ftp.ensembl.org/pub/release-102/tsv/homo_sapiens/Homo_sapiens.GRCh38.102.entrez.tsv.gz'\n",
    "#data_downloader(url_entrez, unprocessed_data_location)\n",
    "\n",
    "ensembl_entrez = pd.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.entrez.tsv', header=0, delimiter='\\t', low_memory=False)\n",
    "ensembl_entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a073d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_entrez.rename(columns={'xref': 'entrez_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "ensembl_entrez = ensembl_entrez.loc[ensembl_entrez['db_name'].apply(lambda x: x == 'EntrezGene')]\n",
    "ensembl_entrez = ensembl_entrez.loc[ensembl_entrez['info_type'].apply(lambda x: x == 'DEPENDENT')]\n",
    "ensembl_entrez.replace('-', 'None', inplace=True)\n",
    "ensembl_entrez.fillna('None', inplace=True)\n",
    "ensembl_entrez.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "ensembl_entrez.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cce0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_entrez).intersection(set(ensembl_uniprot)))\n",
    "ensembl_annot = pd.merge(ensembl_uniprot, ensembl_entrez, on=merge_cols, how='outer')\n",
    "ensembl_annot.fillna('None', inplace=True)\n",
    "\n",
    "ensembl_annot.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_annot).intersection(set(ensembl_geneset)))\n",
    "ensembl = pd.merge(ensembl_geneset, ensembl_annot, on=merge_cols, how='outer')\n",
    "ensembl.fillna('None', inplace=True)\n",
    "ensembl.replace('NA','None', inplace=True, regex=False)\n",
    "\n",
    "ensembl.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c55f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://rest.uniprot.org/uniprotkb/stream?compressed=true&download=true&fields=accession%2Cxref_geneid%2Cid%2Cxref_ensembl%2Cxref_hgnc%2Cgene_primary%2Cgene_synonym%2Corganism_id&format=tsv&query=%28%28organism_id%3A9606%29%29'\n",
    "#data_downloader(url, unprocessed_data_location, 'uniprot_identifier_mapping.tab')\n",
    "\n",
    "uniprot = pd.read_csv(unprocessed_data_location + 'uniprot_identifier_mapping.tab', header=0, delimiter='\\t', compression='gzip')\n",
    "uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot.fillna('None', inplace=True)  # replace NaN with 'None'\n",
    "uniprot.rename(columns={'Entry': 'uniprot_id',\n",
    "                        'GeneID': 'entrez_id',\n",
    "                        'Ensembl': 'transcript_stable_id',\n",
    "                        'HGNC': 'hgnc_id',\n",
    "                        'Gene Names (synonym)': 'synonyms',\n",
    "                        'Gene Names (primary)' :'symbol'}, inplace=True)\n",
    "\n",
    "# update space-delimited synonyms to a pipe (i.e. '|')\n",
    "uniprot['synonyms'] = uniprot['synonyms'].apply(lambda x: '|'.join(x.split()) if x.isupper() else x)\n",
    "\n",
    "# only keep reviewed entries\n",
    "#uniprot = uniprot.loc[uniprot['Status'].apply(lambda x: x != 'unreviewed')]\n",
    "\n",
    "# explode nested data\n",
    "explode_df_uniprot = explodes_data(uniprot.copy(), ['transcript_stable_id', 'entrez_id', 'hgnc_id'], ';')\n",
    "explode_df_uniprot = explodes_data(explode_df_uniprot.copy(), ['symbol', 'synonyms'], '|')\n",
    "\n",
    "# strip out uniprot names\n",
    "explode_df_uniprot['transcript_stable_id'].replace('\\s.*','', inplace=True, regex=True)\n",
    "\n",
    "# remove duplicates\n",
    "#explode_df_uniprot.drop(['Status'], axis=1, inplace=True)\n",
    "explode_df_uniprot.drop(['Entry Name', 'Organism (ID)'], axis=1, inplace=True)\n",
    "explode_df_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_uniprot.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d20482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz'\n",
    "#data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "ncbi_gene = pd.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info', header=0, delimiter='\\t', low_memory=False)\n",
    "ncbi_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "ncbi_gene = ncbi_gene.loc[ncbi_gene['#tax_id'].apply(lambda x: x == 9606)]  # remove non-human rows\n",
    "ncbi_gene.replace('-', 'None', inplace=True)\n",
    "ncbi_gene.rename(columns={'GeneID': 'entrez_id', 'Symbol': 'symbol', 'Synonyms': 'synonyms'}, inplace=True)\n",
    "ncbi_gene['synonyms'] = ncbi_gene['synonyms'] + '|' + ncbi_gene['description'] + '|' + ncbi_gene['Full_name_from_nomenclature_authority'] + '|' + ncbi_gene['Other_designations']\n",
    "ncbi_gene['symbol'] = ncbi_gene['Symbol_from_nomenclature_authority'] + '|' + ncbi_gene['symbol']\n",
    "ncbi_gene['name'] = ncbi_gene['Full_name_from_nomenclature_authority'] + '|' + ncbi_gene['description']\n",
    "\n",
    "# explode nested data\n",
    "explode_df_ncbi_gene = explodes_data(ncbi_gene.copy(), ['symbol', 'synonyms', 'name', 'dbXrefs'], '|')\n",
    "\n",
    "# clean up results\n",
    "explode_df_ncbi_gene['entrez_id'] = explode_df_ncbi_gene['entrez_id'].astype(str)\n",
    "explode_df_ncbi_gene = explode_df_ncbi_gene.loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.split(':')[0] in ['Ensembl', 'HGNC', 'IMGT/GENE-DB'])]\n",
    "explode_df_ncbi_gene['hgnc_id'] = explode_df_ncbi_gene['dbXrefs'].loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.startswith('HGNC'))]\n",
    "explode_df_ncbi_gene['ensembl_gene_id'] = explode_df_ncbi_gene['dbXrefs'].loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.startswith('Ensembl'))]\n",
    "explode_df_ncbi_gene.fillna('None', inplace=True)\n",
    "\n",
    "# reformat entrez gene type\n",
    "explode_df_ncbi_gene['entrez_gene_type'] = explode_df_ncbi_gene['type_of_gene']\n",
    "gene_dict = genomic_type_mapper['entrez_gene_type']\n",
    "for val in gene_dict.keys(): explode_df_ncbi_gene['entrez_gene_type'].replace(val, gene_dict[val], inplace=True)\n",
    "# reformat master gene type\n",
    "explode_df_ncbi_gene['master_gene_type'] = explode_df_ncbi_gene['entrez_gene_type']\n",
    "gene_dict = genomic_type_mapper['master_gene_type']\n",
    "for val in gene_dict.keys(): explode_df_ncbi_gene['master_gene_type'].replace(val, gene_dict[val], inplace=True)\n",
    "\n",
    "# post-process reformatted data\n",
    "explode_df_ncbi_gene.drop(['type_of_gene', 'dbXrefs', 'description', 'Nomenclature_status', 'Modification_date',\n",
    "                           'LocusTag', '#tax_id', 'Full_name_from_nomenclature_authority', 'Feature_type',\n",
    "                           'Symbol_from_nomenclature_authority'], axis=1, inplace=True)\n",
    "explode_df_ncbi_gene['hgnc_id'] = explode_df_ncbi_gene['hgnc_id'].replace('HGNC:', '', regex=True)\n",
    "explode_df_ncbi_gene['ensembl_gene_id'] = explode_df_ncbi_gene['ensembl_gene_id'].replace('Ensembl:', '', regex=True)\n",
    "explode_df_ncbi_gene.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_ncbi_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3149c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://proconsortium.org/download/current/promapping.txt'\n",
    "#data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "pro_map = pd.read_csv(unprocessed_data_location + 'promapping.txt', header=None, names=['pro_id', 'entry', 'pro_mapping'], delimiter='\\t')\n",
    "pro_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_map = pro_map.loc[pro_map['entry'].apply(lambda x: x.startswith('Uni') and '_VAR' not in x and ', ' not in x)]  # keep 'UniProtKB' rows\n",
    "pro_map = pro_map.loc[pro_map['pro_mapping'].apply(lambda x: x.startswith('exact'))] # keep exact mappings\n",
    "pro_map['pro_id'].replace('PR:','PR_', inplace=True, regex=True)  # replace PR: with PR_\n",
    "pro_map['entry'].replace('(^\\w*\\:)','', inplace=True, regex=True)  # remove id prefixes\n",
    "pro_map = pro_map.loc[pro_map['pro_id'].apply(lambda x: '-' not in x)] # remove isoforms\n",
    "pro_map.rename(columns={'entry': 'uniprot_id'}, inplace=True)  # rename columns before merging\n",
    "pro_map.drop(['pro_mapping'], axis=1, inplace=True)  # remove uneeded columns\n",
    "pro_map.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "pro_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(explode_df_hgnc.columns).intersection(set(ensembl.columns)))\n",
    "ensembl_hgnc_merged_data = pd.merge(ensembl, explode_df_hgnc, on=merge_cols, how='outer')\n",
    "\n",
    "ensembl_hgnc_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "ensembl_hgnc_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_hgnc_merged_data.columns).intersection(set(explode_df_uniprot.columns)))\n",
    "ensembl_hgnc_uniprot_merged_data = pd.merge(ensembl_hgnc_merged_data, explode_df_uniprot, on=merge_cols, how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "ensembl_hgnc_uniprot_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_uniprot_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7268a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = merge_cols = list(set(ensembl_hgnc_uniprot_merged_data).intersection(set(explode_df_ncbi_gene.columns)))\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data = pd.merge(ensembl_hgnc_uniprot_merged_data, explode_df_ncbi_gene, on=merge_cols, how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17457c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(ensembl_hgnc_uniprot_ncbi_merged_data, pro_map, on='uniprot_id', how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "merged_data.fillna('None', inplace=True)\n",
    "merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755345f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dates = []\n",
    "for x in tqdm(list(merged_data['symbol'])):\n",
    "    if '-' in x and len(x.split('-')[0]) < 3 and len(x.split('-')[1]) == 3:\n",
    "        clean_dates.append(x.split('-')[1].upper() + x.split('-')[0])\n",
    "    else: clean_dates.append(x)\n",
    "\n",
    "# add cleaned date var back to data set\n",
    "merged_data['symbol'] = clean_dates\n",
    "merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# make sure that all gene and transcript type colunmns have none recoded to unknown or not protein-coding\n",
    "merged_data['hgnc_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['ensembl_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['entrez_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['master_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['master_transcript_type'].replace('None', 'not protein-coding', inplace=True, regex=False)\n",
    "merged_data['ensembl_transcript_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "\n",
    "# remove duplicates\n",
    "merged_data_clean = merged_data.drop_duplicates(subset=None, keep='first')\n",
    "\n",
    "# write data\n",
    "merged_data_clean.to_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt', header=True, sep='\\t', index=False)\n",
    "    \n",
    "# preview data\n",
    "merged_data_clean.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9caf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data to convert all nones, empty values, and unknowns to NaN\n",
    "for col in merged_data_clean.columns:\n",
    "    merged_data_clean[col] = merged_data_clean[col].apply(lambda x: '|'.join([i for i in x.split('|') if i != 'None']))\n",
    "merged_data_clean.replace(to_replace=['None', '', 'unknown'], value=numpy.nan, inplace=True)\n",
    "identifiers = [x for x in merged_data_clean.columns if x.endswith('_id')] + ['symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to dictionary\n",
    "master_dict = {}\n",
    "for idx in tqdm(identifiers):\n",
    "    grouped_data = merged_data_clean.groupby(idx)\n",
    "    grp_ids = set([x for x in list(grouped_data.groups.keys()) if x != numpy.nan])\n",
    "    for grp in grp_ids:\n",
    "        df = grouped_data.get_group(grp).dropna(axis=1, how='all')\n",
    "        df_cols, key = df.columns, idx + '_' + grp\n",
    "        val_df = [[col + '_' + x for x in set(df[col]) if isinstance(x, str)] for col in df_cols if col != idx]\n",
    "        if len(val_df) > 0:\n",
    "            if key in master_dict.keys(): master_dict[key] += [i for j in val_df for i in j if len(i) > 0]\n",
    "            else: master_dict[key] = [i for j in val_df for i in j if len(i) > 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf73864",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_mapped_identifiers = dict()\n",
    "for key, values in tqdm(master_dict.items()):\n",
    "    identifier_info = set(values); gene_prefix = 'master_gene_type_'; trans_prefix = 'master_transcript_type_'\n",
    "    if key.split('_')[0] in ['protein', 'uniprot', 'pro']: pass\n",
    "    elif 'transcript' in key:\n",
    "        trans_match = [x.replace(trans_prefix, '') for x in values if trans_prefix in x]\n",
    "        if len(trans_match) > 0:\n",
    "            t_type_list = ['protein-coding' if ('protein-coding' in trans_match or 'protein_coding' in trans_match) else 'not protein-coding']\n",
    "            identifier_info |= {'transcript_type_update_' + max(set(t_type_list), key=t_type_list.count)}\n",
    "    else:\n",
    "        gene_match = [x.replace(gene_prefix, '') for x in values if x.startswith(gene_prefix) and 'type' in x]\n",
    "        if len(gene_match) > 0:\n",
    "            g_type_list = ['protein-coding' if ('protein-coding' in gene_match or 'protein_coding' in gene_match) else 'not protein-coding']\n",
    "            identifier_info |= {'gene_type_update_' + max(set(g_type_list), key=g_type_list.count)}\n",
    "    reformatted_mapped_identifiers[key] = identifier_info\n",
    "\n",
    "# save a copy of the dictionary\n",
    "# output > 4GB requires special approach: https://stackoverflow.com/questions/42653386/does-pickle-randomly-fail-with-oserror-on-large-files\n",
    "filepath = processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl'\n",
    "\n",
    "# defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "max_bytes, bytes_out = 2**31 - 1, pickle.dumps(reformatted_mapped_identifiers)\n",
    "n_bytes = sys.getsizeof(bytes_out)\n",
    "\n",
    "with open(filepath, 'wb') as f_out:\n",
    "    for idx in range(0, n_bytes, max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dfbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "filepath = processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl'\n",
    "\n",
    "# # defensive way to write pickle.load, allowing for very large files on all platforms\n",
    "max_bytes = 2**31 - 1\n",
    "input_size = os.path.getsize(filepath)\n",
    "bytes_in = bytearray(0)\n",
    "\n",
    "with open(filepath, 'rb') as f_in:\n",
    "     for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f_in.read(max_bytes)\n",
    "\n",
    "# # load ickled data\n",
    "reformatted_mapped_identifiers = pickle.loads(bytes_in)\n",
    "reformatted_mapped_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_data_clean[~merged_data_clean['entrez_id'].isna()].drop(columns=['transcript_stable_id','transcript_name','ensembl_gene_type','ensembl_transcript_type','master_transcript_type','protein_stable_id','hgnc_gene_type','entrez_gene_type']).drop_duplicates()\n",
    "df = df.drop_duplicates(subset=['entrez_id'], keep='first')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates().to_csv(properties_location + 'gene.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ce4c2",
   "metadata": {},
   "source": [
    "***\n",
    "# OBO terms\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1ef87",
   "metadata": {},
   "source": [
    "***\n",
    "# RO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce48fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "ro_graph = Graph()\n",
    "ro_graph.parse(ontology_data_location + 'ro_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(ro_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(ro_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(ro_graph)]\n",
    "master_synonyms = [x for x in ro_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in ro_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0]) if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in ro_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym\n",
    "    }\n",
    "    \n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'RO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef16dff",
   "metadata": {},
   "source": [
    "***\n",
    "# HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da342095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "hpo_graph = Graph()\n",
    "hpo_graph.parse(ontology_data_location + 'hp_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(hpo_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311136ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "dbxref_uri = URIRef(\"http://www.geneontology.org/formats/oboInOwl#hasDbXref\")\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(hpo_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(hpo_graph)]\n",
    "master_synonyms = [x for x in hpo_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in ro_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0])  if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in ro_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # DbXref\n",
    "    cls_ed = [x for x in hpo_graph.objects(x, dbxref_uri) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = '|'.join([str(c) for c in cls_ed]) if len(cls_ed) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym, 'DbXref': desc_ed\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'HPO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409365d8",
   "metadata": {},
   "source": [
    "***\n",
    "# GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "go_graph = Graph()\n",
    "go_graph.parse(ontology_data_location + 'go_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(go_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3c611",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "hasOBONamespace = URIRef(\"http://www.geneontology.org/formats/oboInOwl#hasOBONamespace\")\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(go_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(go_graph)]\n",
    "master_synonyms = [x for x in go_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in go_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0]) if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(cls_syn)]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in ro_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # vocabulary(MF/BP/CC)\n",
    "    cls_ed = [x for x in go_graph.objects(x, hasOBONamespace) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = str(cls_ed[0]) if len(cls_ed) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym': synonym, 'Vocabulary(MF/BP/CC)': desc_ed\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50905bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'GO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f449d91",
   "metadata": {},
   "source": [
    "***\n",
    "# Mondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "mondo_graph = Graph()\n",
    "mondo_graph.parse(ontology_data_location + 'mondo_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(mondo_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(mondo_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(mondo_graph)]\n",
    "master_synonyms = [x for x in mondo_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in mondo_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0])  if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in mondo_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # DbXref\n",
    "    cls_ed = [x for x in mondo_graph.objects(x, dbxref_uri) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = '|'.join([str(c) for c in cls_ed]) if len(cls_ed) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym, 'DbXref': desc_ed\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b955dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'Mondo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3bb097",
   "metadata": {},
   "source": [
    "***\n",
    "# VO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddceba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "vo_graph = Graph()\n",
    "vo_graph.parse(ontology_data_location + 'vo_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(vo_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fec954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(vo_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(vo_graph)]\n",
    "master_synonyms = [x for x in vo_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in vo_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0]) if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in vo_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # seeAlso\n",
    "    cls_seeAlso = [x for x in vo_graph.objects(x, RDFS.seeAlso) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    seeAlsos = str(cls_seeAlso[0]) if len(cls_seeAlso) > 0 else 'None'\n",
    "    # editor notes\n",
    "    cls_ed = [x for x in vo_graph.objects(x, obo.IAO_0000116) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = '|'.join([str(cls_ed[0])]) if len(cls_ed) > 0 else 'None'\n",
    "    # vaccine proper name\n",
    "    cls_pn = [x for x in vo_graph.objects(x, obo.VO_0003158) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_pn = str(cls_pn[0]) if len(cls_pn) > 0 else 'None'\n",
    "    # definition source\n",
    "    cls_ds = [x for x in vo_graph.objects(x, obo.IAO_0000119) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ds = str(cls_ds[0]) if len(cls_ds) > 0 else 'None'  \n",
    "    # alternative label\n",
    "    cls_al = [x for x in vo_graph.objects(x, obo.IAO_0000118) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_al = str(cls_al[0]) if len(cls_al) > 0 else 'None' \n",
    "    # FDA indications\n",
    "    cls_fi = [x for x in vo_graph.objects(x, obo.VO_0003160) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_fi = str(cls_fi[0]) if len(cls_fi) > 0 else 'None' \n",
    "    # trade name\n",
    "    cls_td = [x for x in vo_graph.objects(x, obo.VO_0003099) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_td = str(cls_td[0]) if len(cls_td) > 0 else 'None'\n",
    "    # example of usage\n",
    "    cls_eu = [x for x in vo_graph.objects(x, obo.IAO_0000112) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_eu = str(cls_eu[0]) if len(cls_eu) > 0 else 'None'\n",
    "    # vaccine STN\n",
    "    cls_stn = [x for x in vo_graph.objects(x, obo.VO_0003162) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_stn = str(cls_stn[0]) if len(cls_stn) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'AlternativeLabel':desc_al, 'seeAlso': seeAlsos, 'TradeName': desc_td,\n",
    "        'Description': desc, 'DefinitionSource': desc_ds, 'Synonym(s)': synonym, 'EditorNotes': desc_ed,\n",
    "        'VaccineProperName': desc_pn, 'FDAindications': desc_fi, 'ExampleOfUsage': desc_eu,\n",
    "        'vaccineSTN': desc_stn\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'VO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7444f",
   "metadata": {},
   "source": [
    "***\n",
    "# ChEBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ec63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "chebi_graph = Graph()\n",
    "chebi_graph.parse(ontology_data_location + 'chebi_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(chebi_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d042dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "hasOBONamespace = URIRef(\"http://www.geneontology.org/formats/oboInOwl#hasOBONamespace\")\n",
    "dbxref_uri = URIRef(\"http://www.geneontology.org/formats/oboInOwl#hasDbXref\")\n",
    "iupacName = URIRef(\"http://purl.obolibrary.org/obo/chebi#IUPAC_NAME\")\n",
    "charge = URIRef(\"http://purl.obolibrary.org/obo/chebi/charge\")\n",
    "mass = URIRef(\"http://purl.obolibrary.org/obo/chebi/mass\")\n",
    "smiles = URIRef(\"http://purl.obolibrary.org/obo/chebi/smiles\")\n",
    "formula = URIRef(\"http://purl.obolibrary.org/obo/chebi/formula\")\n",
    "monoisotopicmass = URIRef(\"http://purl.obolibrary.org/obo/chebi/monoisotopicmass\")\n",
    "inchi = URIRef(\"http://purl.obolibrary.org/obo/chebi/inchi\")\n",
    "inchikey = URIRef(\"http://purl.obolibrary.org/obo/chebi/inchikey\")\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(chebi_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(chebi_graph)]\n",
    "master_synonyms = [x for x in chebi_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in chebi_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0])  if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in chebi_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # DbXref\n",
    "    cls_ed = [x for x in chebi_graph.objects(x, dbxref_uri) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = '|'.join([str(c) for c in cls_ed]) if len(cls_ed) > 0 else 'None'\n",
    "    # vocabulary\n",
    "    cls_vo = [x for x in chebi_graph.objects(x, hasOBONamespace) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_vo = str(cls_vo[0]) if len(cls_vo) > 0 else 'None'\n",
    "    # IUPAC name\n",
    "    cls_iupac = [x for x in chebi_graph.objects(x, iupacName) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_iupac = str(cls_iupac[0]) if len(cls_iupac) > 0 else 'None'\n",
    "    # charge\n",
    "    cls_ch = [x for x in chebi_graph.objects(x, charge) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ch = str(cls_ch[0]) if len(cls_ch) > 0 else 'None'\n",
    "    # mass\n",
    "    cls_mass = [x for x in chebi_graph.objects(x, mass) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_mass = str(cls_mass[0]) if len(cls_mass) > 0 else 'None'\n",
    "    # smiles\n",
    "    cls_smiles = [x for x in chebi_graph.objects(x, smiles) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_smiles = str(cls_smiles[0]) if len(cls_smiles) > 0 else 'None'\n",
    "    # formula\n",
    "    cls_form = [x for x in chebi_graph.objects(x, formula) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_form = str(cls_form[0]) if len(cls_form) > 0 else 'None'\n",
    "    # monoisotopicmass\n",
    "    cls_mim = [x for x in chebi_graph.objects(x, monoisotopicmass) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_mim = str(cls_mim[0]) if len(cls_mim) > 0 else 'None'\n",
    "    # inchi\n",
    "    cls_in = [x for x in chebi_graph.objects(x, inchi) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_in = str(cls_in[0]) if len(cls_in) > 0 else 'None'\n",
    "    # inchikey\n",
    "    cls_ink = [x for x in chebi_graph.objects(x, inchikey) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ink = str(cls_ink[0]) if len(cls_ink) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym, 'DbXref': desc_ed,\n",
    "        'Namespace': desc_vo, 'IUPACname': desc_iupac, 'Charge': desc_ch, 'Mass': desc_mass,\n",
    "        'Smiles': desc_smiles, 'Monoisotopicmass': desc_mim, 'Inchi': desc_in, 'Inchikey': desc_ink\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c9a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'ChEBI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941db69b",
   "metadata": {},
   "source": [
    "***\n",
    "# Uberon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "uberon_graph = Graph()\n",
    "uberon_graph.parse(ontology_data_location + 'ext_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(uberon_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c3d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(uberon_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(uberon_graph)]\n",
    "master_synonyms = [x for x in uberon_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in uberon_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0])  if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in uberon_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # DbXref\n",
    "    cls_ed = [x for x in uberon_graph.objects(x, dbxref_uri) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = '|'.join([str(c) for c in cls_ed]) if len(cls_ed) > 0 else 'None'\n",
    "    # external definition\n",
    "    cls_extd = [x for x in uberon_graph.objects(x, obo.UBPROP_0000001) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_extd = str(cls_extd[0]) if len(cls_extd) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym,\n",
    "        'DbXref': desc_ed, 'ExternalDefinition': desc_extd\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'Uberon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49611ff2",
   "metadata": {},
   "source": [
    "***\n",
    "# CLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "clo_graph = Graph()\n",
    "clo_graph.parse(ontology_data_location + 'clo_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(clo_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "comment = URIRef(\"http://www.w3.org/2000/01/rdf-schema#comment\")\n",
    "seeAlso = URIRef(\"http://www.w3.org/2000/01/rdf-schema#seeAlso\")\n",
    "depictedBy = URIRef(\"http://xmlns.com/foaf/0.1/depicted_by\")\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(clo_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(clo_graph)]\n",
    "master_synonyms = [x for x in clo_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in clo_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0])  if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in clo_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # DbXref\n",
    "    cls_ed = [x for x in clo_graph.objects(x, dbxref_uri) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = '|'.join([str(c) for c in cls_ed]) if len(cls_ed) > 0 else 'None'\n",
    "    # comment\n",
    "    cls_com = [x for x in clo_graph.objects(x, comment) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_com = '|'.join([str(c) for c in cls_com]) if len(cls_com) > 0 else 'None'\n",
    "    # seeAlso\n",
    "    cls_sa = [x for x in clo_graph.objects(x, seeAlso) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_sa = '|'.join([str(c) for c in cls_sa]) if len(cls_sa) > 0 else 'None'\n",
    "    # depicted by\n",
    "    cls_db = [x for x in clo_graph.objects(x, depictedBy) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_db = '|'.join([str(c) for c in cls_db]) if len(cls_db) > 0 else 'None'\n",
    "    # example of usage\n",
    "    cls_eou = [x for x in clo_graph.objects(x, obo.IAO_0000112) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_eou = str(cls_eou[0]) if len(cls_eou) > 0 else 'None'\n",
    "    # definition source\n",
    "    cls_ds = [x for x in clo_graph.objects(x, obo.IAO_0000119) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ds = str(cls_ds[0]) if len(cls_ds) > 0 else 'None'\n",
    "    # alternative term\n",
    "    cls_at = [x for x in clo_graph.objects(x, obo.IAO_0000118) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_at = '|'.join([str(c) for c in cls_at]) if len(cls_at) > 0 else 'None'\n",
    "    # IEDB alternative term\n",
    "    cls_iedb = [x for x in clo_graph.objects(x, obo.OBI_9991118) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_iedb = str(cls_iedb[0]) if len(cls_iedb) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym, 'DbXref': desc_ed,\n",
    "        'Comment': desc_com, 'SeeAlso': desc_sa, 'DepictedBy': desc_db, 'ExampleOfUsage': desc_eou,\n",
    "        'DefinitionSource': desc_ds, 'AlternativeTerm': desc_at, 'IEDBalternativeTerm': desc_iedb\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c97de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'CLO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423788e3",
   "metadata": {},
   "source": [
    "***\n",
    "# PRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "pro_graph = Graph()\n",
    "pro_graph.parse(ontology_data_location + 'pr_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(pro_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ed521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "comment = URIRef(\"http://www.w3.org/2000/01/rdf-schema#comment\")\n",
    "seeAlso = URIRef(\"http://www.w3.org/2000/01/rdf-schema#seeAlso\")\n",
    "depictedBy = URIRef(\"http://xmlns.com/foaf/0.1/depicted_by\")\n",
    "orth = URIRef(\"http://purl.obolibrary.org/obo/pr#PRO-short-label\")\n",
    "syn2 = URIRef(\"http://purl.obolibrary.org/obo/pr#Gene-based\")\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(pro_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(pro_graph)]\n",
    "master_synonyms = [x for x in pro_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in pro_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0])  if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in pro_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # DbXref\n",
    "    cls_ed = [x for x in pro_graph.objects(x, dbxref_uri) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = '|'.join([str(c) for c in cls_ed]) if len(cls_ed) > 0 else 'None'\n",
    "    # comment\n",
    "    cls_com = [x for x in pro_graph.objects(x, comment) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_com = '|'.join([str(c) for c in cls_com]) if len(cls_com) > 0 else 'None'\n",
    "    # unique short label for PRO terms for display purposes; based on orthology\n",
    "    cls_orth = [x for x in pro_graph.objects(x, orth) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_orth = str(cls_orth[0]) if len(cls_orth) > 0 else 'None'\n",
    "    # synonyms based on current or previous gene name, ORF name, or ordered locus name\n",
    "    cls_syn2 = [x for x in pro_graph.objects(x, syn2) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_syn2 = '|'.join([str(c) for c in cls_syn2]) if len(cls_syn2) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym, 'DbXref': desc_ed,\n",
    "        'Comment': desc_com, 'UniqueShortLabel(orthology-based)': desc_orth,\n",
    "        'Synonym(s)(unusedName/ORF/locus-based)': desc_syn2,\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e20955",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'PRO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f1cfc",
   "metadata": {},
   "source": [
    "***\n",
    "# SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf343b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_graph = Graph()\n",
    "so_graph.parse(ontology_data_location + 'so_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(so_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae111fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(so_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(so_graph)]\n",
    "master_synonyms = [x for x in so_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in so_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0])  if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in so_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    # DbXref\n",
    "    cls_ed = [x for x in so_graph.objects(x, dbxref_uri) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc_ed = '|'.join([str(c) for c in cls_ed]) if len(cls_ed) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym, 'DbXref': desc_ed\n",
    "    }\n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'SO.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b4a48",
   "metadata": {},
   "source": [
    "***\n",
    "# PW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_graph = Graph()\n",
    "pw_graph.parse(ontology_data_location + 'pw_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology.'.format(len(pw_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(pw_graph)] #+\\\n",
    "      #[x for x in gets_object_properties(pw_graph)]\n",
    "master_synonyms = [x for x in pw_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in pw_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0]) if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = '|'.join([str(c) for c in cls_syn]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in pw_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = str(cls_desc[0]) if len(cls_desc) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x)] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym(s)': synonym\n",
    "    } \n",
    "\n",
    "pd.DataFrame(relation_metadata_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(relation_metadata_dict).T.drop_duplicates().to_csv(properties_location + 'PW.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
