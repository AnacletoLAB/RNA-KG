{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <p style=\"text-align: center;\">RNA Knowledge Graph Build Data Preparation</p>\n",
    "    \n",
    "***\n",
    "***\n",
    "\n",
    "**Authors:** [ECavalleri](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=emanuele.cavalleri@unimi.it) (RNA mapping rules), [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com) (PheKnowLator's KG mapping rules)\n",
    "\n",
    "**GitHub Repositories:** [RNA-KG](https://github.com/AnacletoLAB/RNA-KG/), [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki)\n",
    "  \n",
    "<br>  \n",
    "  \n",
    "**Purpose:** This notebook serves as a script to create mapping rules (i.e. look-up tables) in order to build edges for RNA-KG.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Dependencies:**   \n",
    "- **Scripts**: This notebook utilizes several helper functions, which are stored in the [`data_utils.py`](https://github.com/callahantiff/PheKnowLator/blob/master/pkt_kg/utils/data_utils.py) and [`kg_utils.py`](https://github.com/callahantiff/PheKnowLator/blob/master/pkt_kg/utils/kg_utils.py) scripts.  \n",
    "- **Data**: All downloaded and generated data sources are provided through [10.5281/zenodo.10078876](https://zenodo.org/doi/10.5281/zenodo.10078876) dedicated repository. <u>This notebook will download everything that is needed for you</u>.  \n",
    "_____\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-Up Environment\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import datetime\n",
    "import glob\n",
    "import itertools\n",
    "import networkx\n",
    "import numpy\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import gffpandas.gffpandas as gffpd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import re\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from reactome2py import content\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "from pkt_kg.utils import * \n",
    "from builds.ontology_cleaning import *\n",
    "\n",
    "from Bio import SeqIO, Entrez\n",
    "\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store resources\n",
    "resource_data_location = '../resources/'    \n",
    "\n",
    "# directory to use for unprocessed data\n",
    "unprocessed_data_location = '../resources/processed_data/unprocessed_data/'\n",
    "\n",
    "# directory to use for processed data\n",
    "processed_data_location = '../resources/processed_data/'\n",
    "\n",
    "# directory to write relations data to\n",
    "relations_data_location = '../resources/relations_data/'\n",
    "\n",
    "# directory to write ontology data to\n",
    "ontology_data_location = '../resources/ontologies/'\n",
    "\n",
    "# directory to write edges data to\n",
    "edge_data_location = '../resources/edge_data/'\n",
    "\n",
    "# processed data url \n",
    "processed_url = 'https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/'\n",
    "\n",
    "# owltools location\n",
    "owltools_location = '../pkt_kg/libs/owltools'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## CREATE MAPPING DATASETS  <a class=\"anchor\" id=\"create-identifier-maps\"></a>\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filepath = processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl'\n",
    "\n",
    "# defensive way to write pickle.load, allowing for very large files on all platforms\n",
    "max_bytes = 2**31 - 1\n",
    "input_size = os.path.getsize(filepath)\n",
    "bytes_in = bytearray(0)\n",
    "\n",
    "with open(filepath, 'rb') as f_in:\n",
    "    for _ in range(0, input_size, max_bytes):\n",
    "        bytes_in += f_in.read(max_bytes)\n",
    "\n",
    "# load pickled data\n",
    "reformatted_mapped_identifiers = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ensembl Gene-Entrez Gene <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map Ensembl gene identifiers to Entrez gene identifiers when creating `gene`-`gene` edges\n",
    "\n",
    "**Output:** `ENSEMBL_GENE_ENTREZ_GENE_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt',\n",
    "                  'ensembl_gene_id', 'entrez_id', 'ensembl_gene_type', 'entrez_gene_type',\n",
    "                  'gene_type_update', 'gene_type_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "egeg_data = pandas.read_csv(processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False,\n",
    "                            names=['Ensembl_Gene_IDs', 'Entrez_Gene_IDs',\n",
    "                                   'Ensembl_Gene_Type', 'Entrez_Gene_Type',\n",
    "                                   'Master_Gene_Type1', 'Master_Gene_Type2'])\n",
    "\n",
    "egeg_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ensembl Transcript-Protein Ontology <a class=\"anchor\" id=\"ensembltranscript-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Ensembl transcript identifiers to Protein Ontology identifiers when creating `rna`-`protein` edges\n",
    "\n",
    "**Output:** `ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt',\n",
    "                  'transcript_stable_id', 'pro_id', 'ensembl_transcript_type', None,\n",
    "                  'transcript_type_update', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "etpr_data = pandas.read_csv(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols=[0, 1, 2, 4],\n",
    "                            names=['Ensembl_Transcript_IDs', 'Protein_Ontology_IDs',\n",
    "                                   'Ensembl_Transcript_Type', 'Master_Transcript_Type'])\n",
    "\n",
    "etpr_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Entrez Gene-Ensembl Transcript <a class=\"anchor\" id=\"entrezgene-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map entrez gene identifiers to Ensembl transcript identifiers when creating `gene`-`rna` edges\n",
    "\n",
    "**Output:** `ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                  'entrez_id', 'transcript_stable_id', 'entrez_gene_type', 'ensembl_transcript_type',\n",
    "                  'gene_type_update', 'transcript_type_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "eet_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                           header=None, delimiter='\\t', low_memory=False,\n",
    "                           names=['Entrez_Gene_IDs', 'Ensembl_Transcript_IDs',\n",
    "                                  'Entrez_Gene_Type', 'Ensembl_Transcript_Type',\n",
    "                                  'Master_Gene_Type', 'Master_Transcript_Type'])\n",
    "\n",
    "eet_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Entrez Gene-Protein Ontology <a class=\"anchor\" id=\"entrezgene-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Protein Ontology identifiers to Ensembl transcript identifiers when creating the following edges:   \n",
    "- chemical-protein  \n",
    "- gene-protein\n",
    "\n",
    "**Output:** `ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'entrez_id', 'pro_id', 'entrez_gene_type', None,\n",
    "                  'gene_type_update', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "egpr_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols = [0, 1, 2, 4],\n",
    "                            names=['Gene_IDs', 'Protein_Ontology_IDs',\n",
    "                                   'Entrez_Gene_Type', 'Master_Gene_Type'])\n",
    "\n",
    "egpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Gene Symbol-Ensembl Transcript <a class=\"anchor\" id=\"genesymbol-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map gene symbols to Ensembl transcript identifiers when creating the following edges: \n",
    "- chemical-rna  \n",
    "- rna-anatomy  \n",
    "- rna-cell  \n",
    "\n",
    "**Output:** `GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                  'symbol', 'transcript_stable_id', 'master_gene_type', 'ensembl_transcript_type',\n",
    "                  'gene_type_update', 'transcript_type_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "set_data = pandas.read_csv(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                           header=None, delimiter='\\t', low_memory=False,\n",
    "                           names=['Gene_Symbols', 'Ensembl_Transcript_IDs',\n",
    "                                  'Gene_Type', 'Ensembl_Transcript_Type',\n",
    "                                  'Master_Gene_Type', 'Master_Transcript_Type'])\n",
    "\n",
    "set_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Gene Symbol-Entrez <a class=\"anchor\" id=\"genesymbol-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map gene symbols to Entrez identifiers\n",
    "\n",
    "**Output:** `GENE_SYMBOL_ENTREZ_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrez_enst_map = pd.read_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt', sep=\"\\t\", header=None)\n",
    "symbol_ensembl_map = pd.read_csv(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt', sep=\"\\t\", header=None)\n",
    "symbol_entrez_map = pd.merge(symbol_ensembl_map, entrez_enst_map, on=[1])\n",
    "symbol_entrez_map = symbol_entrez_map[['0_x','0_y']].drop_duplicates()\n",
    "symbol_entrez_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_entrez_map.to_csv(processed_data_location + 'GENE_SYMBOL_ENTREZ_MAP.txt', sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### STRING-Protein Ontology <a class=\"anchor\" id=\"string-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map STRING identifiers to Protein Ontology identifiers when creating `protein`-`protein` edges \n",
    "\n",
    "**Output:** `STRING_PRO_ONTOLOGY_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'protein_stable_id', 'pro_id', None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "stpr_data = pandas.read_csv(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols=[0, 1],\n",
    "                            names=['STRING_IDs', 'Protein_Ontology_IDs'])\n",
    "\n",
    "stpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Uniprot Accession-Protein Ontology <a class=\"anchor\" id=\"uniprotaccession-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Uniprot accession identifiers to Protein Ontology identifiers when creating the following edges:  \n",
    "- protein-gobp  \n",
    "- protein-gomf  \n",
    "- protein-gocc  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst \n",
    "- protein-pathway\n",
    "\n",
    "**Output:** `UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'uniprot_id', 'pro_id', None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "uapr_data = pandas.read_csv(processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols=[0, 1],\n",
    "                            names=['Uniprot_Accession_IDs', 'Protein_Ontology_IDs'])\n",
    "\n",
    "uapr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ChEBI-MeSH Identifiers <a class=\"anchor\" id=\"mesh-chebi\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [mapping-mesh-to-chebi](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#mapping-mesh-identifiers-to-chebi-identifiers)  \n",
    "\n",
    "**Purpose:** Map MeSH identifiers to ChEBI identifiers when creating the following edges:  \n",
    "- chemical-gene  \n",
    "- chemical-disease\n",
    "\n",
    "**Dependencies:** Recapitulates the [`LOOM`](https://www.bioontology.org/wiki/BioPortal_Mappings) algorithm implemented by BioPortal when creating mappings between resources. The procedure is relatively straightforward and consists of the following:\n",
    "- For all MeSH `SCR Chemicals`, obtain the following information:  \n",
    "  - <u>Identifiers</u>: MeSH identifiers     \n",
    "  - <u>Labels</u>: string labels using the `RDFS:label` object property  \n",
    "  - <u>Synonyms</u>: track down all synonyms using the `vocab:concept` and `vocab:preferredConcept` object properties   \n",
    "- For all ChEBI classes, obtain the following information:  \n",
    "  - <u>Labels</u>: string labels using the `RDFS:label` object property  \n",
    "  - <u>Synonyms</u>: track down all synonyms using all `synonym` object properties \n",
    "  \n",
    "*Alternatively:* You can use the [`ncbo_rest_api.py`](https://gist.github.com/callahantiff/a28fb3160782f42f104e9ec41553af0d) script to pull mappings from the BioPortal API, but note that it takes >2 days for it to finish.\n",
    "\n",
    "**Output:** `CHEBI_MESH_MAP.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***  \n",
    "**MeSH**  \n",
    "Downloads the `nt`-formatted version of the current MeSH vocabulary. Preprocesing is then performed in order to reformat the data so that it can be converted into a Pandas DataFrame in preparation of merging it with `ChEBI` in order to identify overlapping concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://nlmpubs.nlm.nih.gov/online/mesh/rdf/2025/mesh2025.nt'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data\n",
    "mesh = [x.split('> ') for x in tqdm(open(unprocessed_data_location + 'mesh2025.nt').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "mesh_dict, results = {}, []\n",
    "for row in tqdm(mesh):\n",
    "    dbx, lab, msh_type = None, None, None\n",
    "    s, p, o = row[0].split('/')[-1], row[1].split('#')[-1], row[2]  \n",
    "    if s[0] in ['C', 'D'] and ('.' not in s and 'Q' not in s) and len(s) >= 5:\n",
    "        s = 'MESH_' + s\n",
    "        if p == 'preferredConcept' or p == 'concept': dbx = 'MESH_' + o.split('/')[-1]\n",
    "        if 'label' in p.lower(): lab = o.split('\"')[1]\n",
    "        if 'type' in p.lower(): msh_type = o.split('#')[1]\n",
    "        if s in mesh_dict.keys():\n",
    "            if dbx is not None: mesh_dict[s]['dbxref'].add(dbx)\n",
    "            if lab is not None: mesh_dict[s]['label'].add(lab)\n",
    "            if msh_type is not None: mesh_dict[s]['type'].add(msh_type)\n",
    "        else:\n",
    "            mesh_dict[s] = {'dbxref': set() if dbx is None else {dbx},\n",
    "                            'label': set() if lab is None else {lab},\n",
    "                            'type': set() if msh_type is None else {msh_type},\n",
    "                            'synonym': set()}\n",
    "\n",
    "# fine tune dictionary - obtain labels for each entry's synonym identifiers\n",
    "for key in tqdm(mesh_dict.keys()):\n",
    "    for i in mesh_dict[key]['dbxref']:\n",
    "        if len(mesh_dict[key]['dbxref']) > 0 and i in mesh_dict.keys():\n",
    "            mesh_dict[key]['synonym'] |= mesh_dict[i]['label']\n",
    "\n",
    "# expand data and convert to pandas DataFrame\n",
    "for key, value in tqdm(mesh_dict.items()):\n",
    "    results += [[key, list(value['label'])[0], 'NAME']]\n",
    "    if len(value['synonym']) > 0:\n",
    "        for i in value['synonym']:\n",
    "            results += [[key, i, 'SYNONYM']]\n",
    "mesh_filtered = pandas.DataFrame({'CODE': [x[0] for x in results],\n",
    "                                  'TYPE': [x[2] for x in results],\n",
    "                                  'STRING': [x[1] for x in results]})\n",
    "\n",
    "# lowercase all strings and remove white space and punctuation\n",
    "mesh_filtered['STRING'] = mesh_filtered['STRING'].str.lower()\n",
    "mesh_filtered['STRING'] = mesh_filtered['STRING'].str.replace('[^\\w]','')\n",
    "\n",
    "# preview data\n",
    "mesh_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***  \n",
    "**ChEBI**  \n",
    "Downloads the flat-file containing labels and synonyms for all classes in the `ChEBI` ontology. Preprocessing is then performed in order to reformat the data so that it can be converted into a Pandas DataFrame in preparation of merging it with `MeSH` in order to identify overlapping concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://ftp.ebi.ac.uk/pub/databases/chebi/Flat_file_tab_delimited/names.tsv.gz'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data\n",
    "chebi = pandas.read_csv(unprocessed_data_location + 'names.tsv', header=0, delimiter='\\t')\n",
    "\n",
    "# preprocess data\n",
    "chebi_filtered = chebi[['COMPOUND_ID', 'TYPE', 'NAME']]\n",
    "chebi_filtered.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "chebi_filtered.columns = ['CODE', 'TYPE', 'STRING']\n",
    "\n",
    "# append CHEBI to the number in each code\n",
    "chebi_filtered['CODE'] = chebi_filtered['CODE'].apply(lambda x: \"{}{}\".format('CHEBI_', x))\n",
    "\n",
    "# lowercase all strings and remove white space and punctuation\n",
    "chebi_filtered['STRING'] = chebi_filtered['STRING'].str.lower()\n",
    "chebi_filtered['STRING'] = chebi_filtered['STRING'].str.replace('[^\\w]','')\n",
    "\n",
    "# preview data\n",
    "chebi_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***  \n",
    "**Merge Identifier Data**  \n",
    "Performs an inner merge of the `MeSH` and `ChEBI` Pandas DataFrames in order to find concepts that exist in both DataFrames. Results are then written out to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "chem_merge = pandas.merge(chebi_filtered[['STRING', 'CODE']], mesh_filtered[['STRING', 'CODE']], on='STRING', how='inner')\n",
    "\n",
    "# filter results\n",
    "mesh_edges = set()\n",
    "for idx, row in chem_merge.drop_duplicates().iterrows():\n",
    "    mesh, chebi = row['CODE_y'], row['CODE_x']\n",
    "    syns = [x for x in mesh_dict[mesh]['dbxref'] if 'C' in x or 'D' in x]\n",
    "    mesh_edges.add(tuple([mesh, chebi]))\n",
    "    if len(syns) > 0:\n",
    "        for x in syns:\n",
    "            mesh_edges.add(tuple([x, chebi]))\n",
    "\n",
    "# write resulting mappings\n",
    "with open(processed_data_location + 'MESH_CHEBI_MAP.txt', 'w') as out:\n",
    "    for pair in mesh_edges:\n",
    "        out.write(pair[0] + '\\t' + pair[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pandas.read_csv(processed_data_location + 'MESH_CHEBI_MAP.txt', header=None, names=['MESH_ID', 'CHEBI_ID'], delimiter='\\t')\n",
    "\n",
    "# preview mapping results\n",
    "data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Disease and Phenotype Identifiers <a class=\"anchor\" id=\"disease-identifiers\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [DisGeNET](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#disgenet)  \n",
    "\n",
    "**Purpose:** This script downloads the Human Phenotype Ontology (HPO), the MonDO Disease Ontology (MONDO), and [disease_mappings.tsv](https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz) in order to map UMLS identifiers to HPO and MONDO identifiers when creating the following edges:  \n",
    "- chemical-disease  \n",
    "- disease-phenotype  \n",
    "- chemical-phenotype  \n",
    "- gene-phenotype  \n",
    "- variant-phenotype  \n",
    "\n",
    "**Output:**   \n",
    "- Human Disease Ontology Mappings ➞ `DISEASE_MONDO_MAP.txt`\n",
    "- Human Phenotype Ontology Mappings ➞ `PHENOTYPE_HPO_MAP.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**MONDO Identifiers**  \n",
    "`MONDO` contains DbXRef mappings to other disease terminology identifiers. To make this useful, we will store the DbXRefs as a dictionary with `MONDO` identifiers as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mondo_graph = Graph().parse(ontology_data_location + 'mondo_with_imports.owl')\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(mondo_graph), datetime.datetime.now().strftime('%m/%d/%Y')))\n",
    "\n",
    "# get dbxrefs for all MONDO classes\n",
    "dbxref_res = gets_ontology_class_dbxrefs(mondo_graph)[0]\n",
    "mondo_dict = {str(k).lower().split('/')[-1]: {str(i).split('/')[-1].replace('_', ':') for i in v} for k, v in dbxref_res.items() if 'MONDO' in str(v)}\n",
    "\n",
    "# pickle dictionary\n",
    "pickle.dump(mondo_dict, open(processed_data_location + 'Mondo_Identifier_Map.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**HPO Identifiers**  \n",
    "`HPO` contains DbXRef mappings to other disease terminology identifiers. To make this useful, we will store the DbXRefs as a dictionary with `HPO` identifiers as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into RDFLib graph object\n",
    "hp_graph = Graph().parse(ontology_data_location + 'hp_with_imports.owl')\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(hp_graph), datetime.datetime.now().strftime('%m/%d/%Y')))\n",
    "\n",
    "# get dbxrefs for all HPO classes\n",
    "dbxref_res = gets_ontology_class_dbxrefs(hp_graph)[0]\n",
    "hp_dict = {str(k).lower().split('/')[-1]: {str(i).split('/')[-1].replace('_', ':') for i in v} for k, v in dbxref_res.items() if 'HP' in str(v)}\n",
    "\n",
    "# pickle dictionary\n",
    "pickle.dump(hp_dict, open(processed_data_location + 'HPO_Identifier_Map.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**DisGeNET Disease Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data\n",
    "disease_data = pandas.read_csv(unprocessed_data_location + 'disease_mappings.tsv', header=0, delimiter='\\t')\n",
    "\n",
    "# reformat data\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.lower()\n",
    "disease_data['diseaseId'] = disease_data['diseaseId'].str.lower()\n",
    "disease_data['vocabulary'] = ['doid' if x == 'do' else 'ordoid' if x == 'ordo' else x for x in disease_data['vocabulary']]\n",
    "\n",
    "# preview data\n",
    "disease_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Build Disease Identifier Dictionary_  \n",
    "In order to improve efficiency when mapping different disease terminology identifiers to the [MonDO Disease Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#mondo-disease-ontology) and [Human Phenotype Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-phenotype-ontology), we create a dictionary of disease identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all CUIs found with HPO and MONDO\n",
    "disease_data_keep = disease_data.query('vocabulary == \"hpo\" | vocabulary == \"mondo\"')\n",
    "\n",
    "# create mondo and hpo dictionary\n",
    "hp_mondo_dict = {}\n",
    "for idx, row in tqdm(disease_data_keep.iterrows(), total=disease_data_keep.shape[0]):\n",
    "    if row['vocabulary'] == 'mondo': key, value = 'umls:' + row['diseaseId'], 'MONDO:' + row['code']\n",
    "    else: key, value = 'umls:' + row['diseaseId'], row['code']\n",
    "    if key in hp_mondo_dict.keys(): hp_mondo_dict[key] |= {value}\n",
    "    else: hp_mondo_dict[key] = {value}\n",
    "# add ontology mappings from MONDO and HPO\n",
    "for key in tqdm(hp_mondo_dict.keys()):\n",
    "    if key in mondo_dict.keys():\n",
    "        hp_mondo_dict[key] = set(list(hp_mondo_dict[key]) + list(mondo_dict[key]))\n",
    "    if key in hp_dict.keys():\n",
    "        hp_mondo_dict[key] = set(list(hp_mondo_dict[key]) + list(hp_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all rows for HPO/MONDO CUIs to obtain mappings to other disease identifiers\n",
    "disease_data_other = disease_data[disease_data.diseaseId.isin(disease_data_keep['diseaseId'])]\n",
    "\n",
    "# get all other codes that map to MONDO or HPO by hopping through MONDO/HPO relevant CUIs\n",
    "disease_dict = {}\n",
    "for idx, row in tqdm(disease_data_other.iterrows(), total=disease_data_other.shape[0]):\n",
    "    if row['vocabulary'] == 'mondo' or row['vocabulary'] == 'hpo':\n",
    "        key, value = 'umls:' + row['diseaseId'].lower(), row['code']\n",
    "        if key in disease_dict.keys(): disease_dict[key] |= {value}\n",
    "        else: disease_dict[key] = {value}\n",
    "    else:\n",
    "        if 'mondo' not in row['code'] or 'hp' not in row['code']:\n",
    "            if ':' not in row['code']: key, value = row['vocabulary'] + ':' + row['code'], hp_mondo_dict['umls:' + row['diseaseId']]\n",
    "            else: key, value = row['code'], hp_mondo_dict['umls:' + row['diseaseId']]\n",
    "            if key in disease_dict.keys(): disease_dict[key] |= value\n",
    "            else: disease_dict[key] = value\n",
    "\n",
    "# add ontology dictionaries\n",
    "disease_dict = {**disease_dict, **mondo_dict, **hp_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Mapping Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'DISEASE_MONDO_MAP.txt', 'w') as outfile1, open(processed_data_location + 'PHENOTYPE_HPO_MAP.txt', 'w') as outfile2:\n",
    "    for k, v in tqdm(disease_dict.items()):\n",
    "        if any(x for x in v if x.startswith('MONDO')):\n",
    "            for idx in [x.replace(':', '_') for x in v if 'MONDO' in x]:\n",
    "                outfile1.write(k.upper().split(':')[-1] + '\\t' + idx + '\\n')\n",
    "        if any(x for x in v if x.startswith('HP')):\n",
    "            for idx in [x.replace(':', '_') for x in v if 'HP' in x]:\n",
    "                outfile2.write(k.upper().split(':')[-1]  + '\\t' + idx + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed MONDO Disease Ontology Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "dis_data = pandas.read_csv(processed_data_location + 'DISEASE_MONDO_MAP.txt', header=None, names=['Disease_IDs', 'MONDO_IDs'], delimiter='\\t')\n",
    "\n",
    "print('There are {} disease-MONDO edges'.format(len(dis_data)))\n",
    "dis_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Human Phenotype Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "hp_data = pandas.read_csv(processed_data_location + 'PHENOTYPE_HPO_MAP.txt', header=None, names=['Disease_IDs', 'HP_IDs'], delimiter='\\t')\n",
    "\n",
    "print('There are {} phenotype-HPO edges'.format(len(hp_data)))\n",
    "hp_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Human Protein Atlas/GTEx Tissue/Cells - UBERON + Cell Ontology + Cell Line Ontology <a class=\"anchor\" id=\"hpa-uberon\"></a>\n",
    "\n",
    "**Data Source Wiki Page:**  \n",
    "- [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#human-protein-atlas) \n",
    "- [genotype-tissue-expression-project](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#the-genotype-tissue-expression-gtex-project)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** Downloads a query for cell, tissue, and blood types with overexpressed protein-coding genes in the human proteome ([`proteinatlas_search.tsv`](https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv)) via [API](https://www.proteinatlas.org/about/help/dataaccess) and median gene-level TPM by tissue for all genes that are not protein-coding ([`GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct`](https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz)) in order to create mappings between cell and tissue type strings to the Uber-Anatomy, Cell Ontology, and Cell Line Ontology concepts (see [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-protein-atlas) for details on the mapping process). The mappings are then used to create the following edge types:  \n",
    "- rna-cell line  \n",
    "- rna-tissue type   \n",
    "- protein-cell line  \n",
    "- protein-tissue type  \n",
    "\n",
    "\n",
    "**Output:**  \n",
    "- Final HPA-ontology mappings ➞ `HPA_GTEx_TISSUE_CELL_MAP.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Human Protein Atlas**  \n",
    "To expedite the mapping process, all HPA tissues, cells, cell lines, and fluid types are extracted from the HPA data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv'\n",
    "data_downloader(url, unprocessed_data_location, 'proteinatlas_search.tsv.gz')\n",
    "\n",
    "# load data\n",
    "hpa = pandas.read_csv(unprocessed_data_location + 'proteinatlas_search.tsv', header=0, delimiter='\\t')\n",
    "hpa.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve terms to map and write results\n",
    "with open(unprocessed_data_location + 'HPA_tissues.txt', 'w') as outfile:\n",
    "    for x in tqdm(list(hpa.columns)):\n",
    "        if x.endswith('[NX]'):\n",
    "            outfile.write(x.split('RNA - ')[-1].split(' [NX]')[:-1][0] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Genotype-Tissue Expression Project**  \n",
    "Import the tissues, cells, cell lines, and fluids that we externally mapped from HPA and GTEx data to [UBERON](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#uber-anatomy-ontology), the [Cell Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#cell-ontology), and the [Cell Line Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#cell-line-ontology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "url='https://storage.googleapis.com/adult-gtex/bulk-gex/v10/rna-seq/GTEx_Analysis_v10_RNASeQCv2.4.2_gene_median_tpm.gct.gz'\n",
    "data_downloader(url, unprocessed_data_location, 'GTEx_Analysis_v10_RNASeQCv2.4.2_gene_median_tpm.gct.gz')\n",
    "\n",
    "# load data\n",
    "gtex = pandas.read_csv(unprocessed_data_location + 'GTEx_Analysis_v10_RNASeQCv2.4.2_gene_median_tpm.gct', header=0, skiprows=2, delimiter='\\t')\n",
    "gtex.fillna('None', inplace=True)  # replace NaN with 'None'\n",
    "gtex['Name'].replace('(\\..*)','', inplace=True, regex=True)  # remove identifier type, which appears after '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url='https://zenodo.org/records/10056198/files/zooma_tissue_cell_mapping_04JAN2020.xlsx.zip'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load ontology mapping data\n",
    "mapping_data = pandas.read_excel(open(unprocessed_data_location + 'zooma_tissue_cell_mapping_04JAN2020.xlsx', 'rb'),\n",
    "                                 sheet_name='Concept_Mapping - 04JAN2020', header=0, engine='openpyxl')\n",
    "mapping_data.fillna('None', inplace=True)  # convert NaN to None\n",
    "\n",
    "# preview data\n",
    "mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write HPA and GTEx Mapping Data_  \n",
    "The HPA and GTEx mapping data is written locally so that it can be used by the `PheKnowLator` algorithm when creating the knowledge graph edge lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt', 'w') as out:\n",
    "    for idx, row in tqdm(mapping_data.iterrows(), total=mapping_data.shape[0]):\n",
    "        if row['UBERON'] != 'None': out.write(str(row['TERM']).strip() + '\\t' + str(row['UBERON']).strip() + '\\n')\n",
    "        if row['CL'] != 'None': out.write(str(row['TERM']).strip() + '\\t' + str(row['CL']).strip() + '\\n')\n",
    "        if row['CLO'] != 'None': out.write(str(row['TERM']).strip() + '\\t' + str(row['CLO']).strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mapping data\n",
    "mapping_data = pandas.read_csv(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt', header=None, names=['TISSUE_CELL_TERM', 'ONTOLOGY_IDs'], delimiter='\\t')\n",
    "\n",
    "# preview data\n",
    "mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Mapping Reactome Pathways to the Pathway Ontology <a class=\"anchor\" id=\"reactome-pw\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Pathway Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#pathway-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [canonical pathways](http://compath.scai.fraunhofer.de/export_mappings) and [kegg-reactome pathway mappings](https://github.com/ComPath/resources/blob/master/mappings/kegg_reactome.csv) files from the [ComPath Ecosystem](https://github.com/ComPath) in order to create the following identifier mappings:  \n",
    "- `Reactome Pathway Identifiers`  ➞ `KEGG Pathway Identifiers` ➞ `Pathway Ontology Identifiers` \n",
    "\n",
    "**Output:**  \n",
    "- `REACTOME_PW_GO_MAPPINGS.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Pathway Ontology**   \n",
    "Use [OWL Tools](https://github.com/owlcollab/owltools/wiki) to download the [Pathway Ontology](http://www.obofoundry.org/ontology/pw.html). Once downloaded, we read the ontology in as a `RDFLib` graph object so that we can query it to obtain all `DbXRefs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_graph = Graph().parse(ontology_data_location + 'pw_with_imports.owl')\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(pw_graph), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reformat Mapping Results_  \n",
    "Create a dictionary of mapping results where pathway ontology identifiers are values and the keys are `DbXRef` identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dbxref results\n",
    "dbxref_res = gets_ontology_class_dbxrefs(pw_graph)[0]\n",
    "dbxref_dict = {str(k).lower().split('/')[-1]: {str(i).split('/')[-1].replace('_', ':') for i in v} for k, v in dbxref_res.items() if 'PW_' in str(v)}\n",
    "\n",
    "# get synonym results\n",
    "syn_res = gets_ontology_class_synonyms(pw_graph)[0]\n",
    "synonym_dict = {str(k).lower().split('/')[-1]: {str(i).split('/')[-1].replace('_', ':') for i in v} for k, v in syn_res.items() if 'PW_' in str(v)}\n",
    "\n",
    "# combine results into single dictionary\n",
    "id_mappings = {**dbxref_dict, **synonym_dict}\n",
    "\n",
    "print('There are {} results (date: {})'.format(len(id_mappings), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Reactome Pathways**  \n",
    "Download a file of all [Reactome Pathways](https://reactome.org/download/current/ReactomePathways.txt), [Reactome's GO Annotations]('https://reactome.org/download/current/gene_association.reactome.gz'), and [Reactome's mappings to CHEBI](https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt). This file will be filtered to only include human pathways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reactome Pathway Stable Identifiers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/ReactomePathways.txt'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "reactome_pathways = pandas.read_csv(unprocessed_data_location + 'ReactomePathways.txt', header=None, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-human pathways and save as list\n",
    "reactome_pathways = reactome_pathways.loc[reactome_pathways[2].apply(lambda x: x == 'Homo sapiens')] \n",
    "reactome_map = {x:set(['PW_0000001']) for x in set(list(reactome_pathways[0]))}     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reactome's Mappings to GO Annotations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/gene_association.reactome.gz'\n",
    "data_downloader(url, unprocessed_data_location, 'gene_association.reactome.gz')\n",
    "\n",
    "# load data\n",
    "reactome_pathways2 = pandas.read_csv(unprocessed_data_location + 'gene_association.reactome', header=None, delimiter='\\t', skiprows=4, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-human pathways and save as list\n",
    "reactome_pathways2 = reactome_pathways2.loc[reactome_pathways2[12].apply(lambda x: x == 'taxon:9606')] \n",
    "reactome_map.update({x.split(':')[-1]:set(['PW_0000001']) for x in set(list(reactome_pathways2[5]))})     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reactome's Mappings to ChEBI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt'\n",
    "data_downloader(url, unprocessed_data_location, 'ChEBI2Reactome_All_Levels.txt' )\n",
    "\n",
    "# load data\n",
    "reactome_pathways3 = pandas.read_csv(unprocessed_data_location + 'ChEBI2Reactome_All_Levels.txt', header=None, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-human pathways and save as list\n",
    "reactome_pathways3 = reactome_pathways3.loc[reactome_pathways3[5].apply(lambda x: x == 'Homo sapiens')] \n",
    "reactome_map.update({x:set(['PW_0000001']) for x in set(list(reactome_pathways3[1]))})     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**ComPath Reactome Pathway Mappings**  \n",
    "Use [ComPath Mappings](https://github.com/ComPath/resources/tree/master/mappings) to obtain the following mappings:  `Reactome Pathways`  ➞ `KEGG Pathways` ➞ `Pathway Ontology` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Canonical Pathways_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url1 = 'http://compath.scai.fraunhofer.de/export_mappings'\n",
    "data_downloader(url1, unprocessed_data_location, 'compath_canonical_pathway_mappings.txt')\n",
    "\n",
    "# load data\n",
    "compath_cannonical = pandas.read_csv(unprocessed_data_location + 'compath_canonical_pathway_mappings.txt', header=None, delimiter='\\t', low_memory=False)\n",
    "compath_cannonical.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(compath_cannonical.iterrows(), total=compath_cannonical.shape[0]):\n",
    "    if row[6] == 'kegg' and 'kegg:' + row[5].strip('path:hsa') in id_mappings.keys() and row[2] == 'reactome':\n",
    "        for x in id_mappings['kegg:' + row[5].strip('path:hsa')]:\n",
    "            if row[1] in reactome_map.keys(): reactome_map[row[1]] |= set([x.split('/')[-1]])\n",
    "            else: reactome_map[row[1]] = set([x.split('/')[-1]])\n",
    "    if (row[2] == 'kegg' and 'kegg:' + row[1].strip('path:hsa') in id_mappings.keys()) and row[6] == 'reactome':\n",
    "        for x in id_mappings['kegg:' + row[1].strip('path:hsa')]:\n",
    "            if row[5] in reactome_map.keys(): reactome_map[row[5]] |= set([x.split('/')[-1]])\n",
    "            else: reactome_map[row[5]] = set([x.split('/')[-1]])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_KEGG - Reactome Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url2 = 'https://raw.githubusercontent.com/ComPath/resources/master/mappings/kegg_reactome.csv'\n",
    "data_downloader(url2, unprocessed_data_location, 'kegg_reactome.csv')\n",
    "\n",
    "# load data\n",
    "kegg_reactome_map = pandas.read_csv(unprocessed_data_location + 'kegg_reactome.csv', header=0, delimiter=',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(kegg_reactome_map.iterrows(), total=kegg_reactome_map.shape[0]):\n",
    "    if row['Source Resource'] == 'reactome' and 'kegg:' + row['Target ID'].strip('path:hsa') in id_mappings.keys():\n",
    "        for x in id_mappings['kegg:' + row['Target ID'].strip('path:hsa')]:\n",
    "            if row['Source ID'] in reactome_map.keys(): reactome_map[row['Source ID']] |= set([x.split('/')[-1]])\n",
    "            else: reactome_map[row['Source ID']] = set([x.split('/')[-1]])\n",
    "    if row['Target Resource'] == 'reactome' and 'kegg:' + row['Source Resource'].strip('path:hsa') in id_mappings.keys():\n",
    "        for x in id_mappings['kegg:' + row['Source ID'].strip('path:hsa')]:\n",
    "            if row['Target ID'] in reactome_map.keys(): reactome_map[row['Target ID']] |= set([x.split('/')[-1]])\n",
    "            else: reactome_map[row['Target ID']] = set([x.split('/')[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Reactome Pathway GO Annotation Mappings**  \n",
    "Use Reactome's [API](https://reactome.org/dev/content-service) to obtain the following mappings: `Reactome Pathway Identifiers`  ➞ `Gene Ontology Identifiers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "for request_ids in tqdm(list(chunks(list(reactome_map.keys()), 20))):\n",
    "    result, key = content.query_ids(ids=','.join(request_ids)), 'goBiologicalProcess'\n",
    "    if result is not None and (isinstance(result, List) or result['code'] != 404):\n",
    "        for res in result:\n",
    "            if key in res.keys():\n",
    "                if res['stId'] in reactome_map.keys(): reactome_map[res['stId']] |= {'GO_' + res[key]['accession']}\n",
    "                else: reactome_map[res['stId']] = {'GO_' + res[key]['accession']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat identifiers -- replacing ontology concepts with ':' to '_'\n",
    "temp_dict = dict()\n",
    "for key, value in tqdm(reactome_map.items()):\n",
    "    temp_dict[key] = set(x.replace(':', '_') for x in value)\n",
    "\n",
    "# overwrite original reactome dict with cleaned mappings\n",
    "reactome_map = temp_dict\n",
    "\n",
    "# output data\n",
    "with open(processed_data_location + 'REACTOME_PW_GO_MAPPINGS.txt', 'w') as out:\n",
    "    for key in tqdm(reactome_map.keys()):\n",
    "        for x in reactome_map[key]:\n",
    "            if x.startswith('PW') or x.startswith('GO'): out.write(key + '\\t' + x + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "pw_data = pandas.read_csv(processed_data_location + 'REACTOME_PW_GO_MAPPINGS.txt', header=None, names=['Pathway_IDs', 'Mapping_IDs'], delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} pathway ontology mappings'.format(edge_count=len(pw_data)))\n",
    "pw_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "### Mapping Genomic Identifiers to the Sequence Ontology <a class=\"anchor\" id=\"genomic-soo\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Sequence Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/_edit#sequence-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the `genomic_sequence_ontology_mappings.xlsx` file in order to create the following identifier mappings:  \n",
    "- `Gene BioTypes`  ➞ `Sequence Ontology Identifiers`  \n",
    "- `RNA BioTypes`  ➞ `Sequence Ontology Identifiers`  \n",
    "- `variant Types`  ➞ `Sequence Ontology Identifiers`\n",
    "\n",
    "**Output:**  \n",
    "- `SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url='https://zenodo.org/records/10056198/files/genomic_sequence_ontology_mappings.xlsx.zip'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "mapping_data = pandas.read_excel(open(unprocessed_data_location + 'genomic_sequence_ontology_mappings.xlsx', 'rb'),\n",
    "                                 sheet_name='GenomicType_SO_Map_09Mar2020', header=0, engine='openpyxl')\n",
    "\n",
    "print(mapping_data.head(n=3))\n",
    "\n",
    "# convert data to dictionary\n",
    "genomic_type_so_map = {}\n",
    "for idx, row in tqdm(mapping_data.iterrows(), total=mapping_data.shape[0]):\n",
    "    if str(row['source_*_type']) != \"nan\":\n",
    "        genomic_type_so_map[row['source_*_type'] + '_' + row['Genomic']] = row['SO ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_type_so_map['artifact_Gene'] = 'SO_0002172'\n",
    "genomic_type_so_map['artifact_Transcript'] = 'SO_0002172'\n",
    "genomic_type_so_map['protein_coding_CDS_not_defined_Transcript'] = 'SO_0002249'\n",
    "genomic_type_so_map['protein_coding_LoF_Transcript'] = 'SO_0001841'\n",
    "genomic_type_so_map['variation_Variant'] = 'SO_0001060'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Genes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in genomic mapping data\n",
    "genomic_mapped_ids = pickle.load(open(processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl', 'rb'))\n",
    "\n",
    "sequence_map = {}\n",
    "for identifier in tqdm(genomic_mapped_ids.keys()):    \n",
    "    if identifier.startswith('entrez_id_') and identifier.replace('entrez_id_', '') != 'None':\n",
    "        id_clean = identifier.replace('entrez_id_', '')\n",
    "        \n",
    "        # get identifier types\n",
    "        ensembl = [x.replace('ensembl_gene_type_', '') for x in genomic_mapped_ids[identifier] if x.startswith('ensembl_gene_type') and x != 'ensembl_gene_type_unknown']\n",
    "        hgnc = [x.replace('hgnc_gene_type_', '')  for x in genomic_mapped_ids[identifier] if x.startswith('hgnc_gene_type') and x != 'hgnc_gene_type_unknown']\n",
    "        entrez = [x.replace('entrez_gene_type_', '')  for x in genomic_mapped_ids[identifier] if x.startswith('entrez_gene_type') and x != 'entrez_gene_type_unknown']\n",
    "        \n",
    "        # determine gene type\n",
    "        if len(ensembl) > 0: gene_type = genomic_type_so_map[ensembl[0].replace('ensembl_gene_type_', '') + '_Gene']\n",
    "        elif len(hgnc) > 0: gene_type = genomic_type_so_map[hgnc[0].replace('hgnc_gene_type_', '') + '_Gene']\n",
    "        elif len(entrez) > 0: gene_type = genomic_type_so_map[entrez[0].replace('entrez_gene_type_', '') + '_Gene']\n",
    "        else: gene_type = 'SO_0000704'  \n",
    "        \n",
    "        # update sequence map\n",
    "        if id_clean in sequence_map.keys(): sequence_map[id_clean] += [gene_type]\n",
    "        else: sequence_map[id_clean] = [gene_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Transcripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in processed Ensembl Transcript data \n",
    "transcript_data = pandas.read_csv(processed_data_location + 'ensembl_identifier_data_cleaned.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# convert to dictionary\n",
    "transcripts = {}\n",
    "for idx, row in tqdm(transcript_data.iterrows(), total=transcript_data.shape[0]):\n",
    "    if row['transcript_stable_id'] != 'None':\n",
    "        if row['transcript_stable_id'].replace('transcript_stable_id_', '') in transcripts.keys():\n",
    "            transcripts[row['transcript_stable_id'].replace('transcript_stable_id_', '')] += [row['ensembl_transcript_type']]\n",
    "        else: transcripts[row['transcript_stable_id'].replace('transcript_stable_id_', '')] = [row['ensembl_transcript_type']]\n",
    "            \n",
    "# update so map dictionary\n",
    "for identifier in tqdm(transcripts.keys()):\n",
    "    if transcripts[identifier][0] == 'protein_coding': trans_type = genomic_type_so_map['protein-coding_Transcript']\n",
    "    elif transcripts[identifier][0] == 'misc_RNA': trans_type = genomic_type_so_map['miscRNA_Transcript']\n",
    "    else: trans_type = genomic_type_so_map[str(list(set(transcripts[identifier]))[0]).replace('nan','miscRNA') + '_Transcript']\n",
    "    sequence_map[identifier] = [trans_type, 'SO_0000673']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Variants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in variant summary data \n",
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data    \n",
    "variant_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# convert to dictionary\n",
    "variants = {}\n",
    "for idx, row in tqdm(variant_data.iterrows(), total=variant_data.shape[0]):\n",
    "    if row['Assembly'] == 'GRCh38' and row['RS# (dbSNP)'] != -1:\n",
    "        if 'rs' + str(row['RS# (dbSNP)']) in variants.keys(): variants['rs' + str(row['RS# (dbSNP)'])] |= set([row['Type']])\n",
    "        else: variants['rs' + str(row['RS# (dbSNP)'])] = set([row['Type']])\n",
    "\n",
    "# update so map dictionary\n",
    "for identifier in tqdm(variants.keys()):\n",
    "    for typ in variants[identifier]:\n",
    "        var_type = genomic_type_so_map[typ.lower() + '_Variant']\n",
    "        if identifier in sequence_map.keys(): sequence_map[identifier] += [var_type]\n",
    "        else: sequence_map[identifier] = [var_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "**Write Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt', 'w') as outfile:\n",
    "    for key in tqdm(sequence_map.keys()):\n",
    "        for map_type in sequence_map[key]:\n",
    "            outfile.write(key + '\\t' + map_type + '\\n')\n",
    "\n",
    "# load data, print row count, and preview it\n",
    "so_data = pandas.read_csv(processed_data_location + 'SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt', header=None, delimiter='\\t', names=['Identifier', 'Sequence_Ontology_ID'])\n",
    "\n",
    "print('There are {edge_count} sequence ontology mappings'.format(edge_count=len(so_data)))\n",
    "so_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non dovrebbe servire ma controllare'''\n",
    "***\n",
    "\n",
    "**Combine Pathway and Sequence Ontology Mapping Data in Dictionary**  \n",
    "Combine the pathway and sequence mapping data into a dictionary and output it.\n",
    "\n",
    "# combine genomic and pathway maps\n",
    "subclass_mapping = {}  \n",
    "sequence_map.update(reactome_map)\n",
    "\n",
    "# iterate over pathway lists and combine them\n",
    "for key in tqdm(sequence_map.keys()):\n",
    "    subclass_mapping[key] = sequence_map[key]\n",
    "\n",
    "# save a copy of the dictionary\n",
    "pickle.dump(subclass_mapping, open(processed_data_location + 'subclass_construction_map.pkl', 'wb'), protocol=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Chemical labels+synonyms from ChEBI - ChEBI mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map Chemical labels+synonyms from ChEBI to ChEBI identifiers.\n",
    "\n",
    "**Output:** `DESC_CHEBI_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dbxrefs for all ontology classes' label\n",
    "def gets_ontology_class_label(graph: Graph) -> Tuple:\n",
    "    dbx_uris: Dict = dict()\n",
    "    dbx = [x for x in graph if 'label' in str(x[1]).lower() if isinstance(x[0], URIRef)]\n",
    "    for x in dbx:\n",
    "        if str(x[2]).lower() in dbx_uris.keys(): dbx_uris[str(x[2]).lower()].append(str(x[0]))\n",
    "        else: dbx_uris[str(x[2]).lower()] = [str(x[0])]\n",
    "    dbx_type = {str(x[2]).lower(): 'DbXref' for x in dbx}\n",
    "\n",
    "    ex_uris: Dict = dict()\n",
    "    ex = [x for x in graph if 'exactmatch' in str(x[1]).lower() if isinstance([0], URIRef)]\n",
    "    for x in ex:\n",
    "        if str(x[2]).lower() in ex_uris.keys(): ex_uris[str(x[2]).lower()].append(str(x[0]))\n",
    "        else: ex_uris[str(x[2]).lower()] = [str(x[0])]\n",
    "    ex_type = {str(x[2]).lower(): 'ExactMatch' for x in ex}\n",
    "\n",
    "    return {**dbx_uris, **ex_uris}, {**dbx_type, **ex_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dbxrefs for all ontology classes' label\n",
    "def gets_ontology_class_synonym(graph: Graph) -> Tuple:\n",
    "    dbx_uris: Dict = dict()\n",
    "    dbx = [x for x in graph if 'synonym' in str(x[1]).lower() if isinstance(x[0], URIRef)]\n",
    "    for x in dbx:\n",
    "        if str(x[2]).lower() in dbx_uris.keys(): dbx_uris[str(x[2]).lower()].append(str(x[0]))\n",
    "        else: dbx_uris[str(x[2]).lower()] = [str(x[0])]\n",
    "    dbx_type = {str(x[2]).lower(): 'DbXref' for x in dbx}\n",
    "\n",
    "    ex_uris: Dict = dict()\n",
    "    ex = [x for x in graph if 'exactmatch' in str(x[1]).lower() if isinstance([0], URIRef)]\n",
    "    for x in ex:\n",
    "        if str(x[2]).lower() in ex_uris.keys(): ex_uris[str(x[2]).lower()].append(str(x[0]))\n",
    "        else: ex_uris[str(x[2]).lower()] = [str(x[0])]\n",
    "    ex_type = {str(x[2]).lower(): 'ExactMatch' for x in ex}\n",
    "\n",
    "    return {**dbx_uris, **ex_uris}, {**dbx_type, **ex_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label+synonym look-up table for an ontology\n",
    "def gets_ontology_lookup(ontology_name, with_import=True) :\n",
    "    # with_import --> integrated ontologies; without_import --> ontologies used to standardize edge metadata\n",
    "    if with_import :\n",
    "        graph = Graph().parse(ontology_data_location + ontology_name + '_with_imports.owl')\n",
    "    else :\n",
    "        graph = Graph().parse(ontology_data_location + ontology_name + '.owl')\n",
    "\n",
    "    label = gets_ontology_class_label(graph)[0]\n",
    "    graph_dict = {str(k): {str(i).split('/')[-1] for i in v} for k, v in label.items()}\n",
    "\n",
    "    with open(unprocessed_data_location + 'DESC_' + ontology_name.upper() + '_MAP.txt', 'w') as outfile:\n",
    "        for k, v in {**graph_dict}.items():\n",
    "            outfile.write(str(k) + '\\t' + str(v).replace('{','').replace('\\'','').replace('}','') + '\\n')\n",
    "\n",
    "    desc_map = pd.read_csv(unprocessed_data_location+'DESC_' + ontology_name.upper() + '_MAP.txt',\n",
    "                           header=None, delimiter='\\t')\n",
    "    desc_map[1] = desc_map[1].str.split(', ')\n",
    "    desc_map = desc_map.explode(1)\n",
    "\n",
    "    syn = gets_ontology_class_synonym(graph)[0]\n",
    "    graph_dict = {str(k): {str(i).split('/')[-1] for i in v} for k, v in syn.items()}\n",
    "\n",
    "    with open(unprocessed_data_location + 'SYN_' + ontology_name.upper() + '_MAP.txt', 'w') as outfile:\n",
    "        for k, v in {**graph_dict}.items():\n",
    "            outfile.write(str(k) + '\\t' + str(v).replace('{','').replace('\\'','').replace('}','') + '\\n')\n",
    "\n",
    "    syn_map = pd.read_csv(unprocessed_data_location+'SYN_' + ontology_name.upper() + '_MAP.txt',\n",
    "                          header=None, delimiter='\\t')\n",
    "    syn_map[1] = syn_map[1].str.split(', ')\n",
    "    syn_map = syn_map.explode(1)\n",
    "    desc_map = pd.concat([desc_map, syn_map], ignore_index=True).drop_duplicates()\n",
    "    desc_map.to_csv(processed_data_location + 'DESC_' + ontology_name.upper() + '_MAP.txt',\n",
    "                    header=None, sep='\\t', index=None)\n",
    "    return desc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_chebi_map = gets_ontology_lookup('chebi')\n",
    "desc_chebi_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunks above have already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_chebi_map = pd.read_csv(processed_data_location + 'DESC_CHEBI_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Genomics label+synonym from SO - SO mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map genomics terms' label+synonym from SO to SO identifiers.\n",
    "\n",
    "**Output:** `DESC_SO_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_so_map = gets_ontology_lookup('so')\n",
    "desc_so_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunks above have already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_so_map = pd.read_csv(processed_data_location + 'DESC_SO_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### GO terms' label+synonym from GO - GO mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map GO terms' label+synonym from GO to GO identifiers.\n",
    "\n",
    "**Output:** `DESC_GO_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_go_map = gets_ontology_lookup('go')\n",
    "desc_go_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_go_map = pd.read_csv(processed_data_location + 'DESC_GO_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Pathways labels from Reactome - Reactome mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map Reactome pathways labels from Reactome to Reactome identifiers.\n",
    "\n",
    "**Output:** `DESC_REACTOME_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_downloader('https://raw.githubusercontent.com/ComPath/resources/master/mappings/kegg_reactome.csv',\n",
    "                unprocessed_data_location, 'kegg_reactome.csv')\n",
    "\n",
    "kegg_reactome_map = pd.read_csv(unprocessed_data_location + 'kegg_reactome.csv', header=0, delimiter=',')[['Source Name','Source ID']]\n",
    "kegg_reactome_map.columns=[0,1]\n",
    "kegg_reactome_map[0] = kegg_reactome_map[0].str.lower()\n",
    "kegg_reactome_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_downloader('https://reactome.org/download/current/ReactomePathways.txt', unprocessed_data_location)\n",
    "\n",
    "reactome_pathways = pd.read_csv(unprocessed_data_location + 'ReactomePathways.txt', header=None, delimiter='\\t')\n",
    "# remove all non-human pathways\n",
    "reactome_pathways = reactome_pathways[reactome_pathways[2] == 'Homo sapiens'][[0,1]]\n",
    "reactome_pathways.columns=[1,0]\n",
    "reactome_pathways[0] = reactome_pathways[0].str.lower()\n",
    "reactome_pathways.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_reactome_map = pd.concat([kegg_reactome_map, reactome_pathways])\n",
    "desc_reactome_map.to_csv(processed_data_location + \"DESC_REACTOME_MAP.txt\", header=False, sep=\"\\t\",index=False)\n",
    "desc_reactome_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_reactome_map = pd.read_csv(processed_data_location + 'DESC_REACTOME_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Pathways labels from WikiPathway - WikiPathway mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map WikiPathway pathways labels from WikiPathway to WikiPathway identifiers.\n",
    "\n",
    "**Output:** `DESC_WIKIPATHWAY_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.wikipathways.org/current/gmt/wikipathways-20250110-gmt-Homo_sapiens.gmt'\n",
    "data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "with open(unprocessed_data_location+'wikipathways-20250110-gmt-Homo_sapiens.gmt', 'r') as file:\n",
    "    data = file.read().rstrip()\n",
    "    \n",
    "desc_wpw_map = pd.DataFrame([ ln.rstrip().split('\\t') for ln in\n",
    "    io.StringIO(data).readlines() ]).fillna('')\n",
    "\n",
    "desc_wpw_map = desc_wpw_map[[0,1]]\n",
    "desc_wpw_map.columns=[0,1]\n",
    "desc_wpw_map[0] = desc_wpw_map[0].str.replace(r'%WikiPathways_.*$', '', regex=True).str.lower()\n",
    "desc_wpw_map[1] = desc_wpw_map[1].str.replace('https://www.wikipathways.org/instance/', '')\n",
    "\n",
    "desc_wpw_map.to_csv(processed_data_location + \"DESC_WIKIPATHWAYS_MAP.txt\", header=False, sep=\"\\t\",index=False)\n",
    "desc_wpw_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_wpw_map = pd.read_csv(processed_data_location + 'DESC_WIKIPATHWAYS_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Pathways labels from PW - PW mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map pathways labels from PW to PW identifiers.\n",
    "\n",
    "**Output:** `DESC_REACTOME_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_pw_map = gets_ontology_lookup('pw')\n",
    "desc_pw_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_pw_map = pd.read_csv(processed_data_location + 'DESC_PW_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Disease labels+synonyms from Mondo - Mondo mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map Diseases labels+synonyms from Mondo to Mondo identifiers.\n",
    "\n",
    "**Output:** `DESC_MONDO_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_mondo_map = gets_ontology_lookup('mondo')\n",
    "desc_mondo_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_mondo_map = pd.read_csv(processed_data_location + 'DESC_MONDO_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Phenotype labels+synonyms from HPO - HPO mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map Phenotype labels+synonyms from HPO to HPO identifiers.\n",
    "\n",
    "**Output:** `DESC_HP_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_hpo_map = gets_ontology_lookup('hp')\n",
    "desc_hpo_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_hpo_map = pd.read_csv(processed_data_location + 'DESC_HP_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge diseases and phenotypes since they are closely related. Moreover, \"x-disease\" and \"x-phenotype\" interactions share the same RO properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_disPhe_map = pd.concat([desc_mondo_map, desc_hpo_map]).drop_duplicates()\n",
    "desc_disPhe_map.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Disease Ontology (DO) - MONDO mapping <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map DO identifiers to MONDO identifiers.\n",
    "\n",
    "**Output:** `DISEASE_DOID_MONDO_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mondo_graph = Graph().parse(ontology_data_location + 'mondo_with_imports.owl')\n",
    "\n",
    "mondo_dbxref = gets_ontology_class_dbxrefs(mondo_graph)[0]\n",
    "\n",
    "# Fix DOIDs (substitute : with _)\n",
    "mondo_dict = {str(k).replace(':','_').upper(): {str(i).split('/')[-1].replace(':','_') for i in v}\n",
    "              for k, v in mondo_dbxref.items() if 'doid' in str(k)}\n",
    "list({**mondo_dict}.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'DOID_MONDO_MAP.txt', 'w') as outfile:\n",
    "    for k, v in mondo_dict.items():\n",
    "        outfile.write(str(k) + '\\t' + str(v).replace('{','').replace('\\'','').replace('}','') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doid_mondo_map = pd.read_csv(processed_data_location+'DOID_MONDO_MAP.txt', header=None, delimiter='\\t')\n",
    "doid_mondo_map[1] = doid_mondo_map[1].str.split(', ')\n",
    "doid_mondo_map = doid_mondo_map.explode(1)\n",
    "doid_mondo_map.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Disease description from DO - DO mapping <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map Disease descriptions from DO to DO identifiers.\n",
    "\n",
    "**Output:** None, this mapping will be used only internally.\n",
    "\n",
    "Note: Provided by [mir2Disease](http://watson.compbio.iupui.edu:8080/miR2Disease/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_downloader('http://watson.compbio.iupui.edu:8080/miR2Disease/download/diseaseList.txt', unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_do_map = pd.read_csv(unprocessed_data_location + 'diseaseList.txt', sep=\"\\t\")\n",
    "desc_do_map.columns = ['desc', 'doid']\n",
    "desc_do_map['desc'] = desc_do_map['desc'].str.lower()\n",
    "desc_do_map['doid'] = desc_do_map['doid'].str.upper().str.replace(':', '_')\n",
    "desc_do_map.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### TCGA - MONDO mapping <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To manually map the 32 TCGA cancer types to MONDO ontology.\n",
    "\n",
    "**Output:** `TCGA_MONDO_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_mondo_map = pd.DataFrame(data=[['ACC','MONDO_0004971'],\n",
    "                                 ['BLCA','MONDO_0004163'],\n",
    "                                 ['BRCA','MONDO_0006256'],\n",
    "                                 ['CESC','MONDO_0005131'],\n",
    "                                 ['CCRCC','MONDO_0005086'],\n",
    "                                 ['CHOL','MONDO_0019087'],\n",
    "                                 ['COAD','MONDO_0002271'],\n",
    "                                 ['DLBC','MONDO_0018905'],\n",
    "                                 ['ESCA','MONDO_0019086'],\n",
    "                                 ['GBM','MONDO_0018177'],\n",
    "                                 ['HNSC','MONDO_0010150'],\n",
    "                                 ['KICH','MONDO_0017885'],\n",
    "                                 ['KIRC','MONDO_0005005'],\n",
    "                                 ['KIRP','MONDO_0017884'],\n",
    "                                 ['LGG','MONDO_0005499'],\n",
    "                                 ['LIHC','MONDO_0007256'],\n",
    "                                 ['LUAD','MONDO_0005061'],\n",
    "                                 ['LUSC','MONDO_0005097'],\n",
    "                                 ['MESO','MONDO_0005065'],\n",
    "                                 ['OV','MONDO_0006046'],\n",
    "                                 ['PAAD','MONDO_0006047'],\n",
    "                                 ['PCPG','MONDO_0035540'],\n",
    "                                 ['PRAD','MONDO_0005082'],\n",
    "                                 ['READ','MONDO_0002169'],\n",
    "                                 ['SARC','MONDO_0005089'],\n",
    "                                 ['SKCM','MONDO_0005012'],\n",
    "                                 ['STAD','MONDO_0005036'],\n",
    "                                 ['TGCT','MONDO_0010108'],\n",
    "                                 ['THCA','MONDO_0015075'],\n",
    "                                 ['THYM','MONDO_0006456'],\n",
    "                                 ['UCEC','MONDO_0000553'],\n",
    "                                 ['UCS','MONDO_0006485'],\n",
    "                                 ['UVM','MONDO_0006486']\n",
    "                                 ])\n",
    "\n",
    "cancer_mondo_map.to_csv(processed_data_location + 'TCGA_MONDO_MAP.txt', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!controllare se serve!! term_mapping = {\n",
    "    'liver carcinoma': 'MONDO_0007256',\n",
    "    'oesophagus carcinoma': 'MONDO_0019086',\n",
    "    'breast carcinoma': 'MONDO_0004989',\n",
    "    'lung carcinoma': 'MONDO_0005138',\n",
    "    'haematopoietic and lymphoid tissue carcinoma': 'MONDO_0017348',\n",
    "    'prostate carcinoma': 'MONDO_0005159',\n",
    "    'large intestine carcinoma': 'MONDO_0024331',\n",
    "    'skin carcinoma': 'MONDO_0002656',\n",
    "    'pancreas carcinoma': 'MONDO_0006047',\n",
    "    'central nervous system carcinoma': 'MONDO_0006130',\n",
    "    'biliary tract carcinoma': 'MONDO_0003707',\n",
    "    'endometrium carcinoma': 'MONDO_0005461',\n",
    "    'ovary carcinoma': 'MONDO_0005140',\n",
    "    'kidney carcinoma': 'MONDO_0005206',\n",
    "    'urinary tract carcinoma': 'MONDO_0040679',\n",
    "    'cervix carcinoma': 'MONDO_0005131',\n",
    "    'soft tissue carcinoma': 'MONDO_0006424',\n",
    "    'stomach carcinoma': 'MONDO_0004950',\n",
    "    'bone carcinoma': 'MONDO_0002415',\n",
    "    'small intestine carcinoma': 'MONDO_0005522',\n",
    "    'thyroid carcinoma': 'MONDO_0015075',\n",
    "    'upper aerodigestive tract carcinoma': 'MONDO_0005398',\n",
    "    'placenta carcinoma': 'MONDO_0002178',\n",
    "    'salivary gland carcinoma': 'MONDO_0000521',\n",
    "    'adrenal gland carcinoma': 'MONDO_0002814',\n",
    "    'autonomic ganglia carcinoma': 'MONDO_0003996',\n",
    "    'meninges carcinoma': 'MONDO_0021322',\n",
    "    'eye carcinoma': 'MONDO_0002466',\n",
    "    'genital tract carcinoma': 'MONDO_0005140',\n",
    "    'pleura carcinoma': 'MONDO_0006294',\n",
    "    'parathyroid carcinoma': 'MONDO_0012004',\n",
    "    'thymus carcinoma': 'MONDO_0006451',\n",
    "    'pituitary carcinoma': 'MONDO_0017582',\n",
    "    'testis carcinoma': 'MONDO_0005447',\n",
    "    'peritoneum carcinoma': 'MONDO_0002113',\n",
    "    'uterine adnexa carcinoma': 'MONDO_0001351',\n",
    "    'gastrointestinal tract carcinoma': 'MONDO_0006181',\n",
    "    'fallopian tube carcinoma': 'MONDO_0006206',\n",
    "    'penis carcinoma': 'MONDO_0006360',\n",
    "    'vulva carcinoma': 'MONDO_0005215',\n",
    "    'ns': np.nan\n",
    "}\n",
    "\n",
    "lncRNA_disease2['desc'] = lncRNA_disease2['desc'].map(term_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Amino Acid - ChEBI mapping \n",
    "\n",
    "\n",
    "**Purpose:** To manually map amino acids ChEBI ontology (SO could've been used too).\n",
    "\n",
    "**Output:** `AminoAcid_ChEBI_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_chebi_map = pd.DataFrame(data=[['Leu','CHEBI_25017'],\n",
    "                                 ['Phe','CHEBI_28044'],\n",
    "                                 ['Ala','CHEBI_16449'],\n",
    "                                 ['Asn','CHEBI_22653'],\n",
    "                                 ['Glu','CHEBI_18237'],\n",
    "                                 ['His','CHEBI_27570'],\n",
    "                                 ['Asp','CHEBI_22660'],\n",
    "                                 ['Cys','CHEBI_22660'],\n",
    "                                 ['Gly','CHEBI_15428'],\n",
    "                                 ['Ile','CHEBI_24898'],\n",
    "                                 ['Lys','CHEBI_25094'],\n",
    "                                 ['Met','CHEBI_16811'],\n",
    "                                 ['Ser','CHEBI_17822'],\n",
    "                                 ['Val','CHEBI_27266'],\n",
    "                                 ['Gln','CHEBI_28300'],\n",
    "                                 ['Arg','CHEBI_29016'],\n",
    "                                 ['Pro','CHEBI_26271'],\n",
    "                                 ['Thr','CHEBI_26986'],\n",
    "                                 ['iMe','PR_000021937'],\n",
    "                                 ['Trp','CHEBI_27897'],\n",
    "                                 ['Tyr','CHEBI_18186']#,\n",
    "                                 #['Sup','tRNA-Suppressor NOT GROUNDED']\n",
    "                                 ])\n",
    "\n",
    "aa_chebi_map.to_csv(processed_data_location + 'AminoAcid_ChEBI_MAP.txt', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Gene symbol - PRO mapping <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map gene symbols to PRO identifiers.\n",
    "\n",
    "**Output:** `GENE_SYMBOL_PRO_ONTOLOGY_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_ensembl_map = pd.read_csv(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt', sep=\"\\t\", header=None)\n",
    "symbol_ensembl_map[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_pro_map = pd.read_csv(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt', sep=\"\\t\", header=None)\n",
    "ensembl_pro_map[[1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_pro = pd.merge(symbol_ensembl_map[[0,1]], ensembl_pro_map[[1,0]], left_on=[1], right_on=[0])\n",
    "symbol_to_pro = symbol_to_pro[['0_x', '1_y']].drop_duplicates()\n",
    "symbol_to_pro.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_pro.drop_duplicates().to_csv(processed_data_location+\n",
    "                                       'GENE_SYMBOL_PRO_ONTOLOGY_MAP.txt', header=None,\n",
    "                                       sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#symbol_to_pro = pd.read_csv(processed_data_location+'GENE_SYMBOL_PRO_ONTOLOGY_MAP.txt',names=['0_x','1_y'],sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Tissue labels+synonyms from Uberon - Uberon mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map Tissue labels+synonyms from Uberon to Uberon identifiers.\n",
    "\n",
    "**Output:** `DESC_EXT_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_uberon_map = gets_ontology_lookup('ext')\n",
    "desc_uberon_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_uberon_map = pd.read_csv(processed_data_location + 'DESC_EXT_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Cell line labels+synonyms from CLO - CLO mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map Cell line labels+synonyms from CLO to CLO identifiers.\n",
    "\n",
    "**Output:** `DESC_CLO_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_clo_map = gets_ontology_lookup('clo')\n",
    "desc_clo_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_clo_map = pd.read_csv(processed_data_location + 'DESC_CLO_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We map also strings not ending with ' cell' or ' cells' within cell lines\n",
    "desc_clo_map2 = desc_clo_map[(desc_clo_map[0].str.endswith(' cell')) | (desc_clo_map[0].str.endswith(' cells'))]\n",
    "desc_clo_map2[0] = desc_clo_map2[0].str.replace(' cell', '').str.replace(' cells', '')\n",
    "desc_clo_map = pd.concat([desc_clo_map, desc_clo_map2]).drop_duplicates()\n",
    "desc_clo_map[1] = desc_clo_map[1].str.split(', ')\n",
    "desc_clo_map = desc_clo_map.explode(1)\n",
    "desc_clo_map.to_csv(processed_data_location + 'DESC_CLO_MAP.txt', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Protein labels+synonyms from PRO - PRO mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map Protein labels+synonyms from PRO to PRO identifiers.\n",
    "\n",
    "**Output:** `DESC_PR_MAP.txt` and `DESC_PR_MAP_ALL.txt`\n",
    "\n",
    "Note: The employed PRO ontology is trimmed to contain only human and viral proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc_pro_map_all = gets_ontology_lookup('pr')\n",
    "# Remove genes\n",
    "desc_pro_map_all = desc_pro_map_all[~desc_pro_map_all[1].str.startswith('gene_symbol_report?hgnc_id=')]\n",
    "desc_pro_map_all.to_csv(processed_data_location + 'DESC_PR_ALL_MAP.txt', header=None, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_pro_map = desc_pro_map_all.copy()\n",
    "# We decide to preferentially keep proteins such that human ones have been defined\n",
    "desc_pro_map_human = desc_pro_map.dropna()[desc_pro_map.dropna()[0].str.contains('human', case=False)]\n",
    "desc_pro_map_human[0] = desc_pro_map_human[0].str.replace(\"human \", '')\n",
    "desc_pro_map_human[0] = desc_pro_map_human[0].str.replace(\"human\", '')\n",
    "desc_pro_map_human[0] = desc_pro_map_human[0].str.replace(\" \\(\", '', regex=True)\n",
    "desc_pro_map_human[0] = desc_pro_map_human[0].str.replace(\"\\)\", '', regex=True)\n",
    "desc_pro_map_human[0] = desc_pro_map_human[0].str.replace(\",(.*)\", '', regex=True)\n",
    "desc_pro_map_human[1] = desc_pro_map_human[1].str.split(', ')\n",
    "desc_pro_map_human = desc_pro_map_human.explode(1)\n",
    "desc_pro_map_human.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_pro_map[0] = desc_pro_map[0].str.replace(\"human \", '')\n",
    "desc_pro_map[0] = desc_pro_map[0].str.replace(\"human\", '')\n",
    "desc_pro_map[0] = desc_pro_map[0].str.replace(\" \\(\", '', regex=True)\n",
    "desc_pro_map[0] = desc_pro_map[0].str.replace(\"\\)\", '', regex=True)\n",
    "desc_pro_map[0] = desc_pro_map[0].str.replace(\",(.*)\", '', regex=True)\n",
    "desc_pro_map[1] = desc_pro_map[1].str.split(', ')\n",
    "desc_pro_map = desc_pro_map.explode(1)\n",
    "desc_pro_map = desc_pro_map[~desc_pro_map[0].isin(desc_pro_map_human[0])]\n",
    "desc_pro_map = pd.concat([desc_pro_map, desc_pro_map_human]).drop_duplicates()\n",
    "desc_pro_map.to_csv(processed_data_location + 'DESC_PR_MAP.txt', header=None, sep='\\t', index=False)\n",
    "desc_pro_map.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way (i.e., using this modified look-up table), an entity x will be linked to \"double-stranded RNA-activated factor 1 complex (human)\" (PR_000027111) instead of \"double-stranded RNA-activated factor 1 complex\" (PR_000027110)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunk above has already been run, uncomment and run the following lines to speed up construction:\n",
    "#desc_pro_map_all = pd.read_csv(processed_data_location + 'DESC_PR_ALL_MAP.txt', header=None, sep='\\t')\n",
    "#desc_pro_map = pd.read_csv(processed_data_location + 'DESC_PR_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### NCI Thesaurus labels+synonyms from NCIT - NCIT mapping\n",
    "\n",
    "\n",
    "**Purpose:** To map NCI Thesaurus labels+synonyms from NCIT to NCIT identifiers.\n",
    "\n",
    "**Output:** `DESC_NCIT_MAP.txt`\n",
    "\n",
    "Note: This is **not** an integrated ontology, but we use NCIT to standardize edge metadata as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '{} {} --merge-import-closure -o {}'\n",
    "os.system(command.format(owltools_location, 'http://purl.obolibrary.org/obo/ncit.owl',\n",
    "                         ontology_data_location + 'ncit.owl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_ncit_map = gets_ontology_lookup('ncit', with_import=False)\n",
    "os.remove(ontology_data_location + \"ncit.owl\")\n",
    "desc_ncit_map.to_csv(processed_data_location + 'DESC_NCIT_MAP.txt', header=None, sep='\\t', index=False)\n",
    "desc_ncit_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunks above have already been run, uncomment and run the following line to speed up construction:\n",
    "#desc_ncit_map = pd.read_csv(processed_data_location + 'DESC_NCIT_MAP.txt', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Gene symbol - ENTREZ mapping <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map gene symbols to ENTREZ identifiers.\n",
    "\n",
    "**Output:** `GENE_SYMBOL_ENTREZ_ID_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrez_enst_map = pd.read_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt', sep=\"\\t\", header=None)\n",
    "entrez_enst_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_entrez_map = pd.merge(symbol_ensembl_map, entrez_enst_map, on=[1])\n",
    "symbol_entrez_map = symbol_entrez_map[['0_x','0_y']].drop_duplicates()\n",
    "symbol_entrez_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_entrez_map.to_csv(processed_data_location+'GENE_SYMBOL_ENTREZ_ID_MAP.txt',header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunks above have already been run, uncomment and run the following line to speed up construction:\n",
    "#symbol_entrez_map = pd.read_csv(processed_data_location+'GENE_SYMBOL_ENTREZ_ID_MAP.txt',names=['0_x','0_y'],sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ribozyme - RFAM mapping \n",
    "\n",
    "**Purpose:** To map ribozyme to RFAM identifiers.\n",
    "\n",
    "**Output:** `ribozyme_RFAM_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribozyme_rfam_map = pd.DataFrame(data=[['LC ribozyme','RF00011'],\n",
    "                                 ['hammerhead ribozyme','RF00008,RF00163'],\n",
    "                                 ['glmS ribozyme','RF00234'],\n",
    "                                 ['HDV-F-prausnitzii','RF02682'],\n",
    "                                 ['HDV ribozyme','RF00094'],\n",
    "                                 ['HDV_ribozyme','RF00094'],\n",
    "                                 ['Hairpin','RF00173'],\n",
    "                                 ['Hammerhead_1','RF00163'],\n",
    "                                 ['Hammerhead_HH9','RF02275'],\n",
    "                                 ['Hammerhead_3','RF00008'],\n",
    "                                 ['Hammerhead_HH10','RF02277'],\n",
    "                                 ['Hammerhead_II','RF02276'],\n",
    "                                 ['Pistol','RF02679'],\n",
    "                                 ['Pistol ribozyme','RF02679'],\n",
    "                                 ['twister ribozyme','RF03160'],\n",
    "                                 ['Twister-P5','RF02684'],\n",
    "                                 ['Twister-P3','RF03154'],\n",
    "                                 ['RNAse P','RF00009'],\n",
    "                                 ['VS ribozyme',' RF00010'] \n",
    "                                 ])\n",
    "\n",
    "ribozyme_rfam_map[1] = ribozyme_rfam_map[1].str.split(',')\n",
    "ribozyme_rfam_map = ribozyme_rfam_map.explode(1)\n",
    "\n",
    "ribozyme_rfam_map.to_csv(processed_data_location + 'ribozyme_RFAM_MAP.txt', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### RefSeq transcript ID - circBase mapping \n",
    "\n",
    "**Purpose:** To map RefSeq transcript to circBase identifiers.\n",
    "\n",
    "**Output:** `CIRCBASE_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.circbase.org/download/hsa_hg19_circRNA.txt -O $unprocessed_data_location/hsa_hg19_circRNA.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circbase = pd.read_csv(unprocessed_data_location + 'hsa_hg19_circRNA.txt', sep='\\t')\n",
    "circbase = circbase[['circRNA ID','best transcript']].drop_duplicates()\n",
    "circbase.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circbase.to_csv(processed_data_location + 'CIRCBASE_MAP.txt', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Drug labels+synonyms from DrugBank - DrugBank+ChEBI mapping \n",
    "\n",
    "**Purpose:** To map drug labels to DrugBank+ChEBI identifiers.\n",
    "\n",
    "**Output:** `DESC_DRUGBANK_MAP.txt`\n",
    "\n",
    "Note: the download link is https://go.drugbank.com/releases/latest and requires registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $processed_data_location/DrugBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrugBank = pd.read_csv(processed_data_location + 'DrugBank/drugbank vocabulary.csv')\n",
    "links = pd.read_csv(processed_data_location + 'DrugBank/drug links.csv',dtype={'ChEBI ID':str})[['DrugBank ID', 'ChEBI ID']]\n",
    "links['ChEBI ID'] = 'CHEBI_' + links['ChEBI ID']\n",
    "DrugBank = pd.merge(DrugBank,links,on='DrugBank ID')\n",
    "DrugBank['ChEBI ID'] = DrugBank['ChEBI ID'].fillna(DrugBank['DrugBank ID'])\n",
    "DrugBank.rename(columns={'ChEBI ID':'ID'},inplace=True)\n",
    "\n",
    "DrugBank['Common name'] = DrugBank['Common name'].str.lower()\n",
    "DrugBank['Synonyms'] = DrugBank['Synonyms'].str.lower()\n",
    "DrugBank['Synonyms'] = DrugBank['Synonyms'].str.split(' \\| ')\n",
    "DrugBank = DrugBank.explode('Synonyms')\n",
    "DrugBank_syn = DrugBank[['Synonyms','ID']].rename(columns={'Synonyms':'Common name'})\n",
    "DrugBank = pd.concat([DrugBank[['Common name','ID']], DrugBank_syn]).drop_duplicates()\n",
    "DrugBank.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrugBank[['Common name', 'ID']].drop_duplicates().to_csv(\n",
    "    processed_data_location + 'DESC_DRUGBANK_MAP.txt', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### RNA proprietary identifiers+labels - RNAcentral mappings <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map RNA labels and proprietary identifiers to RNAcentral identifiers.\n",
    "\n",
    "**Output:** `RNACENTRAL_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $processed_data_location/RNAcentral_MAP\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/id_mapping.tsv.gz -O $processed_data_location/RNAcentral_MAP/id_mapping.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnacentral_map = pd.read_csv(processed_data_location + \"RNAcentral_MAP/id_mapping.tsv\", delimiter='\\t',\n",
    "                             names=['RNAcentral ID', 'DB', 'DB ID', 'Organism', 'RNA category',\"Label\"])\n",
    "rnacentral_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnacentral_map_human = rnacentral_map[rnacentral_map['Organism'] == 9606]\n",
    "rnacentral_map_human.to_csv(processed_data_location + \"RNAcentral_MAP/RNACENTRAL_MAP.txt\", sep=\"\\t\", index=False)\n",
    "rnacentral_map_human.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunks above have already been run, uncomment and run the following line to speed up construction:\n",
    "#rnacentral_map_human = pd.read_csv(processed_data_location + 'RNAcentral_MAP/RNACENTRAL_MAP.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also download single database mapping files for soruces proving relationships involving RNA molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/ensembl.tsv -O $processed_data_location/RNAcentral_MAP/ensembl.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/lncbook.tsv -O $processed_data_location/RNAcentral_MAP/lncbook.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/mirbase.tsv -O $processed_data_location/RNAcentral_MAP/mirbase.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/lncipedia.tsv -O $processed_data_location/RNAcentral_MAP/lncipedia.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/gtrnadb.tsv -O $processed_data_location/RNAcentral_MAP/gtrnadb.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/hgnc.tsv -O $processed_data_location/RNAcentral_MAP/hgnc.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/noncode.tsv -O $processed_data_location/RNAcentral_MAP/noncode.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/rfam.tsv -O $processed_data_location/RNAcentral_MAP/rfam.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/refseq.tsv -O $processed_data_location/RNAcentral_MAP/refseq.tsv\n",
    "!wget https://ftp.ebi.ac.uk/pub/databases/RNAcentral/current_release/id_mapping/database_mappings/pirbase.tsv -O $processed_data_location/RNAcentral_MAP/pirbase.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For piRBase, we keep only golden standard piRNA sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://bigdata.ibp.ac.cn/piRBase/download/v3.0/fasta/hsa.gold.fa.gz -O $unprocessed_data_location/hsa.gold.fa.gz\n",
    "import gzip\n",
    "with gzip.open(unprocessed_data_location + 'hsa.gold.fa.gz', 'rb') as f_in:\n",
    "    with open(unprocessed_data_location + 'hsa.gold.fa', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnacentral_map_pirbase = pd.read_csv(processed_data_location + \"RNAcentral_MAP/pirbase.tsv\",sep='\\t',\n",
    "                                     names=['RNAcentral ID', 'DB', 'piRBase ID', 'Organism', 'RNA category',\"Label\"])\n",
    "rnacentral_map_pirbase.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "records = list(SeqIO.parse(unprocessed_data_location + 'hsa.gold.fa', \"fasta\"))\n",
    "data = {\"Identifier\": [record.id for record in records], \"Sequence\": [str(record.seq) for record in records]}\n",
    "golden_pirnas = pd.DataFrame(data)\n",
    "golden_pirnas.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnacentral_map_pirbase = rnacentral_map_pirbase[rnacentral_map_pirbase['piRBase ID'].isin(golden_pirnas['Identifier'])]\n",
    "rnacentral_map_pirbase.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(rnacentral_map_pirbase['Organism']==9606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnacentral_map_pirbase[['RNAcentral ID','piRBase ID']].drop_duplicates().to_csv(\n",
    "    processed_data_location + 'RNAcentral_MAP/pirbase.tsv', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### tsRFun's tsRNA - RNAcentral mapping \n",
    "\n",
    "**Purpose:** To map tsRNA sequences from tsRFun to RNAcentral tRNA identifiers.\n",
    "\n",
    "**Output:** `tRNA_tsRNA_RNACENTRAL_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://rna.sysu.edu.cn/tsRFun/download/newID_20210202.txt  -O $unprocessed_data_location/newID_20210202.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsRNA_map = pd.read_csv(unprocessed_data_location + 'newID_20210202.txt', sep=\"\\t\")\n",
    "tsRNA_map = tsRNA_map[['tRNA','tsRNAid']]\n",
    "tsRNA_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtrnadb_rnacentral_map_human = pd.read_csv(processed_data_location + \"RNAcentral_MAP/gtrnadb.tsv\",sep='\\t',\n",
    "                                     names=['RNAcentral ID', 'DB', 'GtRNAdb ID', 'Organism', 'RNA category',\"Label\"])\n",
    "gtrnadb_rnacentral_map_human = gtrnadb_rnacentral_map_human[gtrnadb_rnacentral_map_human['Organism']==9606]\n",
    "tsRNA_RNAcentral_map = pd.merge(tsRNA_map, gtrnadb_rnacentral_map_human, left_on='tRNA', right_on='Label')[[\n",
    "    'tRNA','tsRNAid','RNAcentral ID']].drop_duplicates()\n",
    "tsRNA_RNAcentral_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsRNA_RNAcentral_map[['RNAcentral ID','tsRNAid']].drop_duplicates().to_csv(\n",
    "    processed_data_location + 'tRNA_tsRNA_RNACENTRAL_MAP.txt', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### GtRNAdb legacy - RNAcentral mapping \n",
    "\n",
    "**Purpose:** To map tRNA identifiers into RNAcentral identifiers.\n",
    "\n",
    "**Output:** `tRNA_GTRNADBLegacy_RNACENTRAL_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/hg38-tRNAs.fa -O $unprocessed_data_location/hg38-tRNAs.fa --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "\n",
    "identifiers = []\n",
    "seq = []\n",
    "\n",
    "# Replace the URL with the path to your local FASTA file\n",
    "fasta_file_path = unprocessed_data_location + 'hg38-tRNAs.fa'\n",
    "\n",
    "with open(fasta_file_path) as fasta_file:\n",
    "    for title, sequence in SimpleFastaParser(fasta_file):\n",
    "        identifiers.append(title.split(None, 1)[0])  # First word is ID\n",
    "        seq.append(sequence)\n",
    "        \n",
    "data = {\"Identifier\": identifiers, \"Sequence\": seq}\n",
    "df = pd.DataFrame(data)\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(df['Identifier'].str.startswith('Homo_sapiens_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Identifier'] = df['Identifier'].str[len('Homo_sapiens_'):]\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to show retrieval logic\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "tRNA = pd.read_html('http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/genes/tRNA-Ala-AGC-1-1.html')[0].T\n",
    "tRNA2 = pd.read_html('http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/genes/tRNA-Ala-AGC-1-1.html')[1].T\n",
    "tRNA = pd.concat([tRNA,tRNA2],axis=1)\n",
    "tRNA.columns = tRNA.iloc[0]\n",
    "tRNA = tRNA[1:]\n",
    "tRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for identifier in df['Identifier'] [1:] :\n",
    "\n",
    "    temp = pd.read_html('http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/genes/' + identifier + '.html')[0].T\n",
    "    temp2 = pd.read_html('http://gtrnadb.ucsc.edu/genomes/eukaryota/Hsapi38/genes/' + identifier + '.html')[1].T\n",
    "    temp = pd.concat([temp,temp2],axis=1)\n",
    "    temp.columns = temp.iloc[0]\n",
    "    temp = temp[1:]\n",
    "    tRNA = pd.concat([tRNA, temp])\n",
    "\n",
    "tRNA.Locus = tRNA.Locus.str.replace(' View in Genome Browser', '')\n",
    "tRNA = tRNA.drop(columns=['Organism', 'Known Modifications (Modomics)'])\n",
    "\n",
    "tRNA = tRNA[['GtRNAdb 2009 Legacy Name and Score','GtRNAdb Gene Symbol','RNAcentral ID']]\n",
    "tRNA_map = tRNA[~tRNA['GtRNAdb 2009 Legacy Name and Score'].isna()]\n",
    "tRNA_map['GtRNAdb 2009 Legacy Name and Score'] = tRNA_map['GtRNAdb 2009 Legacy Name and Score'].str.replace(r'\\s+\\(.*\\)', '', regex=True)\n",
    "tRNA_map['RNAcentral ID'] = tRNA_map['RNAcentral ID'].str.split('_').str[0]\n",
    "tRNA_map.rename(columns={'GtRNAdb 2009 Legacy Name and Score':0,'GtRNAdb Gene Symbol':1}, inplace=True) \n",
    "tRNA_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(tRNA_map['RNAcentral ID'].isin(rnacentral_map_human['RNAcentral ID']))\n",
    "# All legacy GtRNAdb IDs are mapped into RNAcentral identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRNA_map[[0,'RNAcentral ID']].drop_duplicates().to_csv(\n",
    "    processed_data_location + 'tRNA_GTRNADBLegacy_RNACENTRAL_MAP.txt', header=None, index=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If chunks above have already been run, uncomment and run the following line to speed up construction:\n",
    "#tRNA_map = pd.read_csv(processed_data_location+'tRNA_GTRNADBLegacy_RNACENTRAL_MAP.txt',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### MINTbase - GtRNAdb tRNA mapping \n",
    "\n",
    "**Purpose:** To map MINTbase to GtRNAdb identifiers.\n",
    "\n",
    "**Output:** `tRNA_MINTbase_GtRNAdb_MAP.txt`\n",
    "\n",
    "Note: `MINTbase-gtRNAdb_mapping.txt` is obtained from [MINTbase](https://cm.jefferson.edu/MINTbase/) (--> tRNA alignment --> Minimum RPM value: All tRFs --> Download --> remove first comment lines marked using # from the html and rename it accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRNA_MINTbase_GtRNAdb_map = pd.read_csv(unprocessed_data_location + 'MINTbase-gtRNAdb_mapping.txt',sep='\\t')\n",
    "tRNA_MINTbase_GtRNAdb_map = tRNA_MINTbase_GtRNAdb_map[['MINTbase tRNA name','gtRNAdb name']]\n",
    "tRNA_MINTbase_GtRNAdb_map = tRNA_MINTbase_GtRNAdb_map[tRNA_MINTbase_GtRNAdb_map['gtRNAdb name'] != '-']\n",
    "tRNA_MINTbase_GtRNAdb_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRNA_RNAcentral_map = pd.merge(tRNA_MINTbase_GtRNAdb_map, gtrnadb_rnacentral_map_human, left_on='gtRNAdb name', right_on='Label')[[\n",
    "    'MINTbase tRNA name','gtRNAdb name','RNAcentral ID']].drop_duplicates()\n",
    "tRNA_RNAcentral_map.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRNA_RNAcentral_map[['MINTbase tRNA name', 'RNAcentral ID']].drop_duplicates().to_csv(\n",
    "    processed_data_location + 'tRNA_MINTbase_RNACENTRAL_MAP.txt', header=None, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "```\n",
    "@misc{callahan_tj_2019_3401437,\n",
    "  author       = {Callahan, TJ},\n",
    "  title        = {PheKnowLator},\n",
    "  month        = mar,\n",
    "  year         = 2019,\n",
    "  doi          = {10.5281/zenodo.3401437},\n",
    "  url          = {https://doi.org/10.5281/zenodo.3401437}\n",
    "}\n",
    "```\n",
    "```\n",
    "@misc{cavalleri_e_2024_rna_kg,\n",
    "  author       = {Cavalleri, E},\n",
    "  title        = {RNA-KG},\n",
    "  year         = 2024,\n",
    "  doi          = {10.5281/zenodo.10078876},\n",
    "  url          = {https://doi.org/10.5281/zenodo.10078876}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
